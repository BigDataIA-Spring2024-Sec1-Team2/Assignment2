Time-Series Analysis
by Richard A. DeFusco, PhD, CFA, Dennis W . McLeavey, DBA, CFA, Jerald 
E. Pinto, PhD, CFA, and David E. Runkle, PhD, CFA.
Richard A. DeFusco, PhD, CFA, is at the University of Nebraska-Lincoln (USA). Dennis W. 
McLeavey, DBA, CFA, is at the University of Rhode Island (USA). Jerald E. Pinto, PhD, 
CFA, is at CFA Institute (USA). David E. Runkle, PhD, CFA, is at Jacobs Levy Equity 
Management (USA).
LEARNING OUTCOMES
Mastery The candidate should be able to:
calculate and evaluate the predicted trend value for a time series, 
modeled as either a linear trend or a log-linear trend, given the 
estimated trend coefficients
describe factors that determine whether a linear or a log-linear trend 
should be used with a particular time series and evaluate limitations 
of trend models
explain the requirement for a time series to be covariance stationary 
and describe the significance of a series that is not stationary
describe the structure of an autoregressive (AR) model of order 
p and calculate one- and two-period-ahead forecasts given the 
estimated coefficients
explain how autocorrelations of the residuals can be used to test 
whether the autoregressive model fits the time series
explain mean reversion and calculate a mean-reverting level
contrast in-sample and out-of-sample forecasts and compare the 
forecasting accuracy of different time-series models based on the 
root mean squared error criterion
explain the instability of coefficients of time-series models
describe characteristics of random walk processes and contrast them 
to covariance stationary processes
describe implications of unit roots for time-series analysis, explain 
when unit roots are likely to occur and how to test for them, and 
demonstrate how a time series with a unit root can be transformed 
so it can be analyzed with an AR model
describe the steps of the unit root test for nonstationarity and 
explain the relation of the test to autoregressive time-series modelsQuantitative MethodsREFRESHER READING
2024 CFA® PROGRAM • LEVEL 2
© 2023 CFA Institute. All rights reserved. Time-Series Analysis 2
LEARNING OUTCOMES
Mastery The candidate should be able to:
explain how to test and correct for seasonality in a time-series model 
and calculate and interpret a forecasted value using an AR model 
with a seasonal lag
explain autoregressive conditional heteroskedasticity (ARCH) and 
describe how ARCH models can be applied to predict the variance of 
a time series
explain how time-series variables should be analyzed for 
nonstationarity and/or cointegration before use in a linear regression
determine an appropriate time-series model to analyze a given 
investment problem and justify that choice
INTRODUCTION
As financial analysts, we often use time-series data to make investment decisions. A 
time series is a set of observations on a variable’s outcomes in different time periods: 
the quarterly sales for a particular company during the past five years, for example, or 
the daily returns on a traded security. In this reading, we explore the two chief uses 
of time-series models: to explain the past and to predict the future of a time series. 
We also discuss how to estimate time-series models, and we examine how a model 
describing a particular time series can change over time. The following two examples 
illustrate the kinds of questions we might want to ask about time series.
Suppose it is the beginning of 2020 and we are managing a US-based investment 
portfolio that includes Swiss stocks. Because the value of this portfolio would decrease 
if the Swiss franc depreciates with respect to the dollar, and vice versa, holding all else 
constant, we are considering whether to hedge the portfolio’s exposure to changes 
in the value of the franc. To help us in making this decision, we decide to model the 
time series of the franc/dollar exchange rate. Exhibit 1 shows monthly data on the 
franc/dollar exchange rate. The data are monthly averages of daily exchange rates. Has 
the exchange rate been more stable since 1987 than it was in previous years? Has the 
exchange rate shown a long-term trend? How can we best use past exchange rates to 
predict future exchange rates?1Introduction 3
Exhibit 1: Swiss Franc/US Dollar Exchange Rate, Monthly Average of Daily 
Data
3.0
2.5
2.0
1.5
1.0
0.5
0
77 80 84 88 91 95 99 02 06 10 13 17
Source: Board of Governors of the Federal Reserve System.
As another example, suppose it is the beginning of 2020. We cover retail stores for 
a sell-side firm and want to predict retail sales for the coming year. Exhibit 2 shows 
monthly data on US retail sales. The data are not seasonally adjusted, hence the spikes 
around the holiday season at the turn of each year. Because the reported sales in the 
stores’ financial statements are not seasonally adjusted, we model seasonally unadjusted 
retail sales. How can we model the trend in retail sales? How can we adjust for the 
extreme seasonality reflected in the peaks and troughs occurring at regular intervals? 
How can we best use past retail sales to predict future retail sales?
Exhibit 2: Monthly US Retail Sales
600,000
500,000
400,000
300,000
200,000
100,000
0
Jan/95 Oct/97 Jul/00 Apr/03 Jan/06 Oct/08 Jul/1 1Apr/14 Jan/17US Dollars (millions)
Source: US Department of Commerce, Census Bureau. Time-Series Analysis 4
Some fundamental questions arise in time-series analysis: How do we model trends? 
How do we predict the future value of a time series based on its past values? How do 
we model seasonality? How do we choose among time-series models? And how do 
we model changes in the variance of time series over time? We address each of these 
issues in this reading.
We first describe typical challenges in applying the linear regression model to 
time-series data. We present linear and log-linear trend models, which describe, respec -
tively, the value and the natural log of the value of a time series as a linear function of 
time. We then present autoregressive time-series models—which explain the current 
value of a time series in terms of one or more lagged values of the series. Such models 
are among the most commonly used in investments, and the section addresses many 
related concepts and issues. We then turn our attention to random walks. Because 
such time series are not covariance stationary, they cannot be modeled using autore-
gressive models unless they can be transformed into stationary series. We therefore 
explore appropriate transformations and tests of stationarity. The subsequent sections 
address moving-average time-series models and discuss the problem of seasonality 
in time series and how to address it. We also cover autoregressive moving-average 
models, a more complex alternative to autoregressive models. The last two topics are 
modeling changing variance of the error term in a time series and the consequences 
of regression of one time series on another when one or both time series may not be 
covariance stationary. 
Challenges of Working with Time Series
Throughout the reading, our objective will be to apply linear regression to a given 
time series. Unfortunately, in working with time series, we often find that the assump -
tions of the linear regression model are not satisfied. To apply time-series analysis, 
we need to assure ourselves that the linear regression model assumptions are met. 
When those assumptions are not satisfied, in many cases we can transform the time 
series or specify the regression model differently, so that the assumptions of the linear 
regression model are met.
We can illustrate assumption difficulties in the context of a common time-series 
model, an autoregressive model. Informally, an autoregressive model is one in which 
the independent variable is a lagged (that is, past) value of the dependent variable, 
such as the model xt = b0 + b1xt−1 + εt (we could also write the equation as yt = b0 
+ b1yt−1 + εt). Specific problems that we often encounter in dealing with time series 
include the following:
 ■The residual errors are correlated instead of being uncorrelated. In the 
calculated regression, the difference between xt and b0 + b1xt−1 is called 
the residual error (εt). The linear regression assumes that this error term 
is not correlated across observations. The violation of that assumption is 
frequently more critical in terms of its consequences in the case of time-se-
ries models involving past values of the time series as independent variables 
than for other models (such as cross-sectional models) in which the depen-
dent and independent variables are distinct. As we discussed in the reading 
on multiple regression, in a regression in which the dependent and indepen-
dent variables are distinct, serial correlation of the errors in this model does 
not affect the consistency of our estimates of intercept or slope coefficients. 
By contrast, in an autoregressive time-series regression, such as xt = b0 + 
b1xt−1 + εt, serial correlation in the error term causes estimates of the inter -
cept (b0) and slope coefficient (b1) to be inconsistent.Linear Trend Models 5
 ■The mean or variance of the time series changes over time. Regression 
results are invalid if we estimate an autoregressive model for a time series 
with mean or variance that changes over time.
Before we try to use time series for forecasting, we may need to transform the 
time-series model so that it is well specified for linear regression. With this objective in 
mind, you will observe that time-series analysis is relatively straightforward and logical.
LINEAR TREND MODELS
calculate and evaluate the predicted trend value for a time series, 
modeled as either a linear trend or a log-linear trend, given the 
estimated trend coefficients
Estimating a trend in a time series and using that trend to predict future values of the 
time series is the simplest method of forecasting. For example, we saw in Exhibit 2  
that monthly US retail sales show a long-term pattern of upward movement—that is, 
a trend. In this section, we examine two types of trends—linear trends and log-linear 
trends—and discuss how to choose between them.
Linear Trend Models
The simplest type of trend is a linear trend, one in which the dependent variable 
changes at a constant rate with time. If a time series, yt, has a linear trend, then we 
can model the series using the following regression equation:
 yt = b0 + b1t + εt, t = 1, 2, . . . , T ,   (1)
where
 yt = the value of the time series at time t  (value of the dependent variable)
 b0 = the y -intercept term
 b1 = the slope coefficient
 t  = time, the independent or explanatory variable
 εt = a random error term
In Equation 1, the trend line, b0 + b1t, predicts the value of the time series at time 
t (where t takes on a value of 1 in the first period of the sample and increases by 1 in 
each subsequent period). Because the coefficient b1 is the slope of the trend line, we 
refer to b1 as the trend coefficient. We can estimate the two coefficients, b0 and b1, 
using ordinary least squares, denoting the estimated coefficients as     ˆ b    0    and     ˆ b    1   . Recall 
that ordinary least squares is an estimation method based on the criterion of mini-
mizing the sum of a regression’s squared residuals.
Now we demonstrate how to use these estimates to predict the value of the time 
series in a particular period. Recall that t  takes on a value of 1 in Period 1. Therefore, 
the predicted or fitted value of yt in Period 1 is     ˆ y    1   =    ˆ b    0   +    ˆ b    1     (  1 )     . Similarly, in a sub-
sequent period—say, the sixth period—the fitted value is     ˆ y    6   =    ˆ b    0   +    ˆ b    1     (  6 )     . Now 
suppose that we want to predict the value of the time series for a period outside the 
sample—say, period T  + 1. The predicted value of yt for period T  + 1 is 
    ˆ y    T+1   =    ˆ b    0   +    ˆ b    1     (  T + 1 )     . For example, if     ˆ b    0    is 5.1 and     ˆ b    1    is 2, then at t  = 5 the pre-2 Time-Series Analysis 6
dicted value of y5 is 15.1 and at t  = 6 the predicted value of y6 is 17.1. Note that each 
consecutive observation in this time series increases by     ˆ b    1    = 2, irrespective of the 
level of the series in the previous period.
EXAMPLE 1
The Trend in the US Consumer Price Index
It is January 2020. As a fixed-income analyst in the trust department of a bank, 
Lisette Miller is concerned about the future level of inflation and how it might 
affect portfolio value. Therefore, she wants to predict future inflation rates. For 
this purpose, she first needs to estimate the linear trend in inflation. To do so, 
she uses the monthly US Consumer Price Index (CPI) inflation data, expressed 
as an annual percentage rate, (1% is represented as 1.0) shown in Exhibit 3. The 
data include 228 months from January 1995 through June 2019, and the model 
to be estimated is yt = b0 + b1t + εt, t = 1, 2, . . . , 294. The table in Exhibit 4 
shows the results of estimating this equation. With 294 observations and two 
parameters, this model has 292 degrees of freedom. At the 0.05 significance 
level, the critical value for a t -statistic is 1.97. The intercept     (     ˆ b    0   = 2.7845 )      is 
statistically significant because the value of the t -statistic for the coefficient is 
well above the critical value. The trend coefficient is negative     (     ˆ b    1   = − 0.0037 )     , 
suggesting a slightly declining trend in inflation during the sample time period. 
However, the trend is not statistically significant because the absolute value of 
the t-statistic for the coefficient is below the critical value. The estimated regres -
sion equation can be written as
 yt = 2.7845 − 0.0037t .
 
Exhibit 3: Monthly CPI Inflation, Not Seasonally Adjusted
 
025
20
15
10
5
–5
–10
–15
–20
–25
95 97 99 01 03 05 07 09 11 13 17 19 15
Source: Bureau of Labor Statistics.
 
Exhibit 4: Estimating a Linear Trend in Inflation: Monthly 
Observations, January 1995–June 2019
 
 
Regression Statistics
R20.0099
Standard error 3.1912Linear Trend Models 7
Regression Statistics
Observations 294
Durbin–Watson 1.2145
 
 
  Coefficient Standard Error t-Statistic
Intercept 2.7845 0.3732 7.4611
t (Trend) −0.0037 0.0022 −1.68
 
Source: US Bureau of Labor Statistics.
Because the trend line slope is estimated to be −0.0037, Miller concludes that 
the linear trend model’s best estimate is that the annualized rate of inflation 
declined at a rate of about 37 bps per month during the sample time period. 
The decline is not statistically significantly different from zero.
In January 1995, the first month of the sample, the predicted value of inflation 
is     ˆ y    1    = 2.7845 − 0.0037(1) = 2.7808%. In June 2019, the 294th, or last, month 
of the sample, the predicted value of inflation is     ˆ y    228    = 2.7845 − 0.0037(294) = 
1.697%. Note, though, that these predicted values are for in-sample periods. A 
comparison of these values with the actual values indicates how well Miller’s 
model fits the data; however, a main purpose of the estimated model is to predict 
the level of inflation for out-of-sample periods. For example, for June 2020 (12 
months after the end of the sample), t  = 294 + 12 = 306, and the predicted level 
of inflation is     ˆ y    306    = 2.7845 − 0.0037(306) = 1.6523%.
Exhibit 5 shows the inflation data along with the fitted trend. Consistent with 
the negative but small and statistically insignificant trend coefficient, the fitted 
trend line is slightly downward sloping. Note that inflation does not appear to 
be above or below the trend line for a long period of time. No persistent differ -
ences exist between the trend and actual inflation. The residuals (actual minus 
trend values) appear to be unpredictable and uncorrelated in time. Therefore, 
using a linear trend line to model inflation rates from 1995 through 2019 does 
not appear to violate the assumptions of the linear regression model. Note also 
that the R2 in this model is quite low, indicating great uncertainty in the infla-
tion forecasts from this model. In fact, the estimated model explains only 0.99% 
of the variation in monthly inflation. Although linear trend models have their 
uses, they are often inappropriate for economic data. Most economic time series 
reflect trends with changing slopes and/or intercepts over time. The linear trend 
model identifies the slope and intercept that provides the best linear fit for all 
past data. The model’s deviation from the actual data can be greatest near the 
end of a data series, which can compromise forecasting accuracy. Later in this 
reading, we will examine whether we can build a better model of inflation than 
a model that uses only a trend line. Time-Series Analysis 8
 
Exhibit 5: Monthly CPI Inflation with Trend
 
025
20
15
10
5
–5
–10
–15
–20
–25
Jan/95 May/99 Sep/03 Jan/08 May/12 Sep/16
Source: US Bureau of Labor Statistics.
 
LOG-LINEAR TREND MODELS
calculate and evaluate the predicted trend value for a time series, 
modeled as either a linear trend or a log-linear trend, given the 
estimated trend coefficients
describe factors that determine whether a linear or a log-linear trend 
should be used with a particular time series and evaluate limitations 
of trend models
Sometimes a linear trend does not correctly model the growth of a time series. In those 
cases, we often find that fitting a linear trend to a time series leads to persistent rather 
than uncorrelated errors. If the residuals from a linear trend model are persistent, then 
we need to employ an alternative model satisfying the conditions of linear regression. 
For financial time series, an important alternative to a linear trend is a log-linear 
trend. Log-linear trends work well in fitting time series that have exponential growth.
Exponential growth means constant growth at a particular rate. For example, 
annual growth at a constant rate of 5% is exponential growth. How does exponential 
growth work? Suppose we describe a time series by the following equation:
   y  t   =  e    b  0  + b  1  t ,  t = 1,  2,  …,  T.  (2)
Exponential growth is growth at a constant rate     (   e    b  1    − 1 )      with continuous com-
pounding. For instance, consider values of the time series in two consecutive periods. 
In Period 1, the time series has the value y1 =   e    b  0  + b  1     (  1 )      , and in Period 2, it has the 
value y2 =   e    b  0  + b  1     (  2 )      . The resulting ratio of the values of the time series in the first two 
periods is y2/y1 =     (   e    b  0  + b  1     (  2 )      )     /    (   e    b  0  + b  1     (  1 )      )      =   e    b  1     (  1 )      . Generally, in any period t , the 
time series has the value yt =   e    b  0  + b  1     (  t )      . In period t  + 1, the time series has the value 
yt+1 =   e    b  0  + b  1     (  t+1 )      . The ratio of the values in the periods (t  + 1) and t is yt+1/yt =   3Log-Linear Trend Models 9
e    b  0  + b  1     (  t+1 )      /  e    b  0  + b  1     (  t )       =   e    b  1     (  1 )      . Thus, the proportional rate of growth in the time series 
over two consecutive periods is always the same: (yt+1 − yt)/yt = yt+1/yt − 1 =   e    b  1     − 1. 
For example, if we use annual periods and   e    b  1     = 1.04 for a particular series, then that 
series grows by 1.04 − 1 = 0.04, or 4% a year. Therefore, exponential growth is growth 
at a constant rate. Continuous compounding is a mathematical convenience that allows 
us to restate the equation in a form that is easy to estimate.
If we take the natural log of both sides of Equation 2, the result is the following 
equation:
 ln yt = b0 + b1t, t = 1, 2, . . . , T .
Therefore, if a time series grows at an exponential rate, we can model the natural 
log of that series using a linear trend (an exponential growth rate is a compound 
growth rate with continuous compounding). Of course, no time series grows exactly 
at a constant rate. Consequently, if we want to use a log-linear model, we must esti-
mate the following equation:
 ln yt = b0 + b1t + εt, t = 1, 2, . . . , T .   (3)
Note that this equation is linear in the coefficients b0 and b1. In contrast to a linear 
trend model, in which the predicted trend value of yt is     ˆ b    0   +    ˆ b    1   t , the predicted trend 
value of yt in a log-linear trend model is   e      ˆ b    0  +   ˆ b    1  t   because   e   ln y  t     = yt.
Examining Equation 3, we see that a log-linear model predicts that ln yt will increase 
by b1 from one time period to the next. The model predicts a constant growth rate 
in yt of   e    b  1    − 1 . For example, if b1 = 0.05, then the predicted growth rate of yt in each 
period is e0.05 − 1 = 0.051271, or 5.13%. In contrast, the linear trend model (Equation 
1) predicts that yt grows by a constant amount from one period to the next.
Example 2 illustrates the problem of nonrandom residuals in a linear trend model, 
and Example 3 shows a log-linear regression fit to the same data.
EXAMPLE 2
A Linear Trend Regression for Quarterly Sales at Starbucks
In September 2019, technology analyst Ray Benedict wants to use Equation 1  
to fit the data on quarterly sales for Starbucks Corporation shown in Exhibit 6. 
Starbucks’ fiscal year ends in June. Benedict uses 74 observations on Starbucks’ 
sales from the second quarter of fiscal year 2001 (starting in April 2001) to the 
third quarter of fiscal year 2019 (ending in June 2019) to estimate the linear 
trend regression model yt = b0 + b1t + εt, t = 1, 2, . . . , 74. Exhibit 7 shows the 
results of estimating this equation. Time-Series Analysis 10
 
Exhibit 6: Starbucks Quarterly Sales by Fiscal Y ear
 
8,000
7,000
6,000
5,000
4,000
3,000
2,000
1,000
0
1/Apr/01 3/Apr/05 2/Apr/17 31/Mar/13 29/Mar/09
Source: Bloomberg.
 
Exhibit 7: Estimating a Linear Trend in Starbucks Sales
 
 
Regression Statistics
R20.9603
Standard error 353.36
Observations 74
Durbin–Watson 0.40
 
 
  Coefficient Standard Error t-Statistic
Intercept 137.4213 82.99 1.6559
t (Trend) 80.2060 1.9231 41.7066
 
Source: Bloomberg.
At first glance, the results shown in Exhibit 7 seem quite reasonable: The trend 
coefficient is highly statistically significant. When Benedict plots the data on 
Starbucks’ sales and the trend line, however, he sees a different picture. As Exhibit 
8 shows, before 2008 the trend line is persistently below sales. Subsequently, 
until 2015, the trend line is persistently above sales and then varies somewhat 
thereafter.Log-Linear Trend Models 11
 
Exhibit 8: Starbucks Quarterly Sales with Trend
 
8,000
7,000
6,000
5,000
4,000
3,000
2,000
1,000
0
1/Apr/01 2/Jan/05 27/Mar/16 1/Jul/12 28/Sep/08
Source: Bloomberg.
Recall a key assumption underlying the regression model: that the regression 
errors are not correlated across observations. If a trend is persistently above or 
below the value of the time series, however, the residuals (the difference between 
the time series and the trend) are serially correlated. Exhibit 9 shows the resid-
uals (the difference between sales and the trend) from estimating a linear trend 
model with the raw sales data. The figure shows that the residuals are persistent: 
They are consistently negative from 2008 to 2015 and consistently positive from 
2001 to 2008 and from 2017 to 2019.
Because of this persistent serial correlation in the errors of the trend model, 
using a linear trend to fit sales at Starbucks would be inappropriate, even though 
the R2 of the equation is high (0.96). The assumption of uncorrelated residual 
errors has been violated. Because the dependent and independent variables 
are not distinct, as in cross-sectional regressions, this assumption violation is 
serious and causes us to search for a better model.
 
Exhibit 9: Residual from Predicting Starbucks Sales with a Trend
 
01,000
800
600
400
200
–200
–400
–600
–800
1/Apr/01 2/Jan/05 27/Mar/16 1/Jul/12 28/Sep/08
Source: Bloomberg.
  Time-Series Analysis 12
EXAMPLE 3
A Log-Linear Regression for Quarterly Sales at Starbucks
1. Having rejected a linear trend model in Example 2, technology analyst 
Benedict now tries a different model for the quarterly sales for Starbucks 
Corporation from the second quarter of 2001 to the third quarter of 2019. 
The curvature in the data plot shown in Exhibit 6 provides a hint that an 
exponential curve may fit the data. Consequently, he estimates the following 
linear equation:
 ln yt = b0 + b1t + εt, t = 1, 2, . . . , 74.  
This equation seems to fit the sales data well. As Exhibit 10 shows, the R2 for 
this equation is 0.95. An R2 of 0.95 means that 95% of the variation in the 
natural log of Starbucks’ sales is explained solely by a linear trend.
 
Exhibit 10: Estimating a Linear Trend in Lognormal 
Starbucks Sales
 
 
Regression Statistics
R20.9771
Standard error 0.1393
Observations 74
Durbin–Watson 0.26
 
 
  Coefficient Standard Error t-Statistic
Intercept 6.7617 0.0327 206.80
t (Trend) 0.0295 0.0008 36.875
 
Source: Compustat.
Although both Equations 1 and Equation 3 have a high R2, Exhibit 11 shows 
how well a linear trend fits the natural log of Starbucks’ sales (Equation 3). 
The natural logs of the sales data lie very close to the linear trend during the 
sample period, and log sales are not substantially above or below the trend 
for long periods of time. Thus, a log-linear trend model seems better suited 
for modeling Starbucks’ sales than a linear trend model is.
1. Benedict wants to use the results of estimating Equation 3 to predict 
Starbucks’ sales in the future. What is the predicted value of Starbucks’ 
sales for the fourth quarter of 2019?
Solution:
The estimated value     ˆ b    0    is 6.7617, and the estimated value     ˆ b    1    is 0.0295. 
Therefore, for fourth quarter of 2019 (t = 75), the estimated model predicts 
that ln     ˆ y    75    = 6.7617 + 0.0295(75) = 8.9742 and that sales will be    ˆ y   =  e   ln   ˆ y    75     = 
e8.9742 = $7,896.7 million. Note that a     ˆ b    1    of 0.0295 implies that the exponen-
tial growth rate per quarter in Starbucks’ sales will be 2.99394% (e0.0295 − 1 
= 0.0299394).Trend Models and Testing for Correlated Errors 13
 
Exhibit 11: Natural Log of Starbucks Quarterly Sales
 
10
789
6
5
4
3
2
1
0
1/Apr/01 2/Jan/05 27/Mar/16 1/Jul/12 28/Sep/08
Source: Compustat.
2. How much different is the previous forecast from the prediction of the 
linear trend model?
Solution:
Exhibit 7 showed that for the linear trend model, the estimated value of     ˆ b    0    is 
137.4213 and the estimated value of     ˆ b    1    is 80.2060. Thus, if we predict 
Starbucks’ sales for the fourth quarter of 2019 (t = 75) using the linear trend 
model, the forecast is     ˆ y    75    = 137.4213 + 80.2060(75) = $6,152.87 million. This 
forecast is far below the prediction made by the log-linear regression model. 
Later we will examine whether we can build a better model of Starbucks’ 
quarterly sales than a model that uses only a log-linear trend.
TREND MODELS AND TESTING FOR CORRELATED 
ERRORS
describe factors that determine whether a linear or a log-linear trend 
should be used with a particular time series and evaluate limitations 
of trend models
Both the linear trend model and the log-linear trend model are single-variable regres -
sion models. If they are to be correctly specified, the regression model assumptions 
must be satisfied. In particular, the regression error for one period must be uncorrelated 
with the regression error for all other periods. In Example 2 in the previous section, we 
could infer an obvious violation of that assumption from a visual inspection of a plot 
of residuals (Exhibit 9). The log-linear trend model of Example 3 appeared to fit the 
data much better, but we still need to confirm that the uncorrelated errors assumption 
is satisfied. To address that question formally, we must carry out a Durbin–Watson 
test on the residuals.4 Time-Series Analysis 14
LOGICAL ORDERING OF TIME-SERIES OBSERVATIONS
In contrast to cross-sectional observations, time-series observations have a logical 
ordering. They must be processed in chronological order of the time periods 
involved. For example, we should not make a prediction of the inflation rate using 
a CPI series in which the order of the observations had been scrambled, because 
time patterns such as growth in the independent variables can negatively affect 
the statistical properties of the estimated regression coefficients.
In the reading on regression analysis, we showed how to test whether regression 
errors are serially correlated using the Durbin–Watson statistic. For example, if the 
trend models shown in Examples 1 and 3  really capture the time-series behavior of 
inflation and the log of Starbucks’ sales, then the Durbin–Watson statistic for both 
of those models should not differ significantly from 2.0. Otherwise, the errors in the 
model are either positively or negatively serially correlated, and that correlation can 
be used to build a better forecasting model for those time series.
In Example 1, estimating a linear trend in the monthly CPI inflation yielded a 
Durbin–Watson statistic of 1.2145. Is this result significantly different from 2.0? To 
find out, we need to test the null hypothesis of no positive serial correlation. For a 
sample with 228 observations and one independent variable, the critical value, dl, for 
the Durbin–Watson test statistic at the 0.05 significance level is above 1.77. Because 
the value of the Durbin–Watson statistic (1.09) is below this critical value, we can 
reject the hypothesis of no positive serial correlation in the errors. (Remember that 
significantly small values of the Durbin–Watson statistic indicate positive serial 
correlation; significantly large values point to negative serial correlation; here the 
Durbin–Watson statistic of 1.09 indicates positive serial correlation.) We can conclude 
that a regression equation that uses a linear trend to model inflation has positive serial 
correlation in the errors. We will need a different kind of regression model because 
this one violates the least squares assumption of no serial correlation in the errors.
In Example 3, estimating a linear trend with the natural logarithm of sales for the 
Starbucks example yielded a Durbin–Watson statistic of 0.26. Suppose we wish to test 
the null hypothesis of no positive serial correlation. The critical value, dl, is above 1.60 
at the 0.05 significance level. The value of the Durbin–Watson statistic (0.12) is below 
this critical value, so we can reject the null hypothesis of no positive serial correlation 
in the errors. We can conclude that a regression equation that uses a trend to model 
the log of Starbucks’ quarterly sales has positive serial correlation in the errors. So, 
for this series as well, we need to build a different kind of model.
Overall, we conclude that the trend models sometimes have the limitation that 
errors are serially correlated. Existence of serial correlation suggests that we can build 
better forecasting models for such time series than trend models.
AR TIME-SERIES MODELS AND 
COVARIANCE-STATIONARY SERIES
explain the requirement for a time series to be covariance stationary 
and describe the significance of a series that is not stationary
A key feature of the log-linear model’s depiction of time series, and a key feature of 
time series in general, is that current-period values are related to previous-period 
values. For example, Starbucks’ sales for the current period are related to its sales in 5AR Time-Series Models and Covariance-Stationary Series 15
the previous period. An autoregressive model (AR), a time series regressed on its 
own past values, represents this relationship effectively. When we use this model, we 
can drop the normal notation of y as the dependent variable and x as the independent 
variable because we no longer have that distinction to make. Here we simply use xt. 
For example, Equation 4 shows a first-order autoregression, AR(1), for the variable xt:
 xt = b0 + b1xt−1 + εt.   (4)
Thus, in an AR(1) model, we use only the most recent past value of xt to predict the 
current value of xt. In general, a p th-order autoregression, AR(p ), for the variable xt 
is shown by
 xt = b0 + b1xt−1 + b2xt−2 + . . . + bpxt–p + εt.   (5)
In this equation, p  past values of xt are used to predict the current value of xt. In the 
next section, we discuss a key assumption of time-series models that include lagged 
values of the dependent variable as independent variables.
Covariance-Stationary Series
Note that the independent variable (xt−1) in Equation 4 is a random variable. This fact 
may seem like a mathematical subtlety, but it is not. If we use ordinary least squares to 
estimate Equation 4 when we have a randomly distributed independent variable that 
is a lagged value of the dependent variable, our statistical inference may be invalid. To 
make a valid statistical inference, we must make a key assumption in time-series anal -
ysis: We must assume that the time series we are modeling is covariance stationary.1
What does it mean for a time series to be covariance stationary? The basic idea is 
that a time series is covariance stationary if its properties, such as mean and variance, 
do not change over time. A covariance stationary series must satisfy three principal 
requirements. First, the expected value of the time series must be constant and finite 
in all periods: E (yt) = μ and |μ | < ∞, t  = 1, 2, . . . , T  (for this first requirement, we use 
the absolute value to rule out the case in which the mean is negative without limit—i.e., 
minus infinity). Second, the variance of the time series must be constant and finite in 
all periods. Third, the covariance of the time series with itself for a fixed number of 
periods in the past or future must be constant and finite in all periods. The second 
and third requirements can be summarized as follows:
 cov(yt, yt–s) = λs; |λs| < ∞; t = 1, 2, . . . , T ; s = 0, ±1, ±2, . . . , ±T ,
where λ  signifies a constant. (Note that when s  in this equation equals 0, this equa -
tion imposes the condition that the variance of the time series is finite, because the 
covariance of a random variable with itself is its variance: cov(yt, yt) = var(yt).) What 
happens if a time series is not covariance stationary but we model it using Equation 4? 
The estimation results will have no economic meaning. For a non-covariance-stationary 
time series, estimating the regression in Equation 4 will yield spurious results. In 
particular, the estimate of b1 will be biased, and any hypothesis tests will be invalid.
How can we tell if a time series is covariance stationary? We can often answer this 
question by looking at a plot of the time series. If the plot shows roughly the same 
mean and variance over time without any significant seasonality, then we may want 
to assume that the time series is covariance stationary.
Some of the time series we looked at in the exhibits appear to be covariance sta-
tionary. For example, the inflation data shown in Exhibit 3 appear to have roughly 
the same mean and variance over the sample period. Many of the time series one 
1 “Weakly stationary” is a synonym for covariance stationary. Note that the terms “stationary” and “sta-
tionarity” are often used to mean “covariance stationary” or “covariance stationarity, ” respectively. You may 
also encounter the more restrictive concept of “strictly” stationary, which has little practical application. 
For details, see Diebold (2008). Time-Series Analysis 16
encounters in business and investments, however, are not covariance stationary. For 
example, many time series appear to grow (or decline) steadily over time and thus 
have a mean that is nonconstant, which implies that they are nonstationary. As an 
example, the time series of quarterly sales in Exhibit 8 clearly shows the mean increas -
ing as time passes. Thus, Starbucks’ quarterly sales are not covariance stationary (in 
general, any time series accurately described with a linear or log-linear trend model 
is not covariance stationary, although a transformation of the original series might be 
covariance stationary). Macroeconomic time series such as those relating to income 
and consumption are often strongly trending as well. A time series with seasonality 
(regular patterns of movement with the year) also has a nonconstant mean, as do 
other types of time series that we discuss later (in particular, random walks are not 
covariance stationary).
Exhibit 2 showed that monthly retail sales (not seasonally adjusted) are also not 
covariance stationary. Sales in December are always much higher than sales in other 
months (these are the regular large peaks), and sales in January are always much lower 
(these are the regular large drops after the December peaks). On average, sales also 
increase over time, so the mean of sales is not constant.
Later we will show that we can often transform a nonstationary time series into a 
stationary time series. But whether a stationary time series is original or transformed, 
a warning is necessary: Stationarity in the past does not guarantee stationarity in the 
future. There is always the possibility that a well-specified model will fail when the 
state of the world changes and yields a different underlying model that generates the 
time series.
DETECTING SERIALLY CORRELATED ERRORS IN AN AR 
MODEL
describe the structure of an autoregressive (AR) model of order 
p and calculate one- and two-period-ahead forecasts given the 
estimated coefficients
explain how autocorrelations of the residuals can be used to test 
whether the autoregressive model fits the time series
We can estimate an autoregressive model using ordinary least squares if the time 
series is covariance stationary and the errors are uncorrelated. Unfortunately, our 
previous test for serial correlation, the Durbin–Watson statistic, is invalid when the 
independent variables include past values of the dependent variable. Therefore, for 
most time-series models, we cannot use the Durbin–Watson statistic. Fortunately, we 
can use other tests to determine whether the errors in a time-series model are serially 
correlated. One such test reveals whether the autocorrelations of the error term are 
significantly different from 0. This test is a t -test involving a residual autocorrelation 
and the standard error of the residual autocorrelation. As background for the test, 
we next discuss autocorrelation in general before moving to residual autocorrelation.
The autocorrelations of a time series are the correlations of that series with its 
own past values. The order of the correlation is given by k , where k  represents the 
number of periods lagged. When k  = 1, the autocorrelation shows the correlation of 
the variable in one period with its occurrence in the previous period. For example, 
the kth-order autocorrelation (ρk) is6Detecting Serially Correlated Errors in an AR Model 17
   ρ  k   =   cov   (   x  t  ,  x  t−k   )     _   σ  x  2    =   E   [     (   x  t   − μ )       (   x  t−k   − μ )     ]      _________________   σ  x  2   , 
where E stands for the expected value. Note that we have the relationship cov(xt, 
xt–k) ≤   σ  x  2  , with equality holding when k  = 0. This means that the absolute value of ρk 
is less than or equal to 1.
Of course, we can never directly observe the autocorrelations, ρk. Instead, we must 
estimate them. Thus, we replace the expected value of xt, μ, with its estimated value,    _ x   , 
to compute the estimated autocorrelations. The k th-order estimated autocorrelation 
of the time series xt, which we denote     ˆ ρ    k   , is
     ˆ ρ    k   =     ∑ 
t=k+1  T
   [     (   x  t   −   _ x   )       (   x  t−k   −   _ x   )     ]     ___________________   
  ∑ 
t=1  T
    (   x  t   −   _ x   )     2    . 
Analogous to the definition of autocorrelations for a time series, we can define the 
autocorrelations of the error term for a time-series model as2
    ρ  ε,k   =   cov   (   ε  t  ,  ε  t−k   )     _   σ  ε  2   
   =   E   [     (   ε  t   − 0 )       (   ε  t−k   − 0 )     ]      _________________   σ  ε  2      
=   E   (   ε  t    ε  t−k   )     _   σ  ε  2   .   
We assume that the expected value of the error term in a time-series model is 0.3
We can determine whether we are using the correct time-series model by test -
ing whether the autocorrelations of the error term (error autocorrelations) differ 
significantly from 0. If they do, the model is not specified correctly. We estimate the 
error autocorrelation using the sample autocorrelations of the residuals (residual 
autocorrelations) and their sample variance.
A test of the null hypothesis that an error autocorrelation at a specified lag equals 
0 is based on the residual autocorrelation for that lag and the standard error of the 
residual correlation, which is equal to  1 /  √ _
 T   , where T is the number of observations 
in the time series (Diebold 2008). Thus, if we have 100 observations in a time series, 
the standard error for each of the estimated autocorrelations is 0.1. We can compute 
the t-test of the null hypothesis that the error correlation at a particular lag equals 
0 by dividing the residual autocorrelation at that lag by its standard error     (  1 /  √ _
 T   )     .
How can we use information about the error autocorrelations to determine 
whether an autoregressive time-series model is correctly specified? We can use a 
simple three-step method. First, estimate a particular autoregressive model—say, an 
AR(1) model. Second, compute the autocorrelations of the residuals from the model.4 
Third, test to see whether the residual autocorrelations differ significantly from 0. If 
significance tests show that the residual autocorrelations differ significantly from 0, 
2 Whenever we refer to autocorrelation without qualification, we mean autocorrelation of the time series 
itself rather than autocorrelation of the error term or residuals.
3 This assumption is similar to the one made in earlier coverage of regression analysis about the expected 
value of the error term.
4 We can compute these residual autocorrelations easily with most statistical software packages. In Microsoft 
Excel, for example, to compute the first-order residual autocorrelation, we compute the correlation of the 
residuals from Observations 1 through T  − 1 with the residuals from Observations 2 through T. Time-Series Analysis 18
the model is not correctly specified; we may need to modify it in ways that we will 
discuss shortly.5 We now present an example to demonstrate how this three-step 
method works.
EXAMPLE 4
Predicting Gross Margins for Intel Corporation
1. Analyst Melissa Jones decides to use a time-series model to predict In-
tel Corporation’s gross margin [(Sales − Cost of goods sold)/Sales] using 
quarterly data from the first quarter of 2003 through the second quarter of 
2019. She does not know the best model for gross margin but believes that 
the current-period value will be related to the previous-period value. She 
decides to start out with a first-order autoregressive model, AR(1): Gross 
margint = b0 + b1(Gross margint−1) + εt. Her observations on the depen-
dent variable are 1Q 2003 through 2Q 2019. Exhibit 12 shows the results of 
estimating this AR(1) model, along with the autocorrelations of the residuals 
from that model.
 
Exhibit 12: Autoregression: AR(1) Model Gross Margin of Intel 
Quarterly Observations, January 2003–June 2019
 
 
Regression Statistics
R20.5746
Standard error 0.03002
Observations 65
Durbin–Watson 1.743
 
 
  Coefficient Standard Error t-Statistic
Intercept 0.1513 0.0480 3.15
Gross 
margint−10.7462 0.0809 9.2236
 
 
Autocorrelations of the Residual
Lag Autocorrelation Standard Error t-Statistic
1 0.1308 0.1240 1.0545
2 −0.2086 0.1240 −1.6818
3 0.0382 0.1240 0.3080
4 0.0608 0.1240 0.4903
 
Source: Bloomberg.
The first thing to note about Exhibit 12 is that both the intercept (    ˆ b    0    = 
0.1513) and the coefficient on the first lag (    ˆ b    1    = 0.7462) of the gross margin 
are highly significant in the regression equation. The first lag of a time series 
5 Often, econometricians use additional tests for the significance of residual autocorrelations. For example, 
the Box–Pierce Q -statistic is frequently used to test the joint hypothesis that all autocorrelations of the 
residuals are equal to 0. For further discussion, see Diebold (2008).Mean Reversion and Multiperiod Forecasts 19
is the value of the time series in the previous period. The t-statistic for the 
intercept is about 3.2, whereas the t-statistic for the first lag of the gross 
margin is more than 9. With 65 observations and two parameters, this 
model has 63 degrees of freedom. At the 0.05 significance level, the critical 
value for a t-statistic is about 2.0. Therefore, Jones must reject the null 
hypotheses that the intercept is equal to 0 (b0 = 0) and the coefficient on the 
first lag is equal to 0 (b1 = 0) in favor of the alternative hypothesis that the 
coefficients, individually, are not equal to 0. But are these statistics valid? 
Although the Durbin–Watson statistic is presented in Exhibit 12, it cannot 
be used to test serial correlation when the independent variables include 
past values of the dependent variable. The correct approach is to test 
whether the residuals from this model are serially correlated.
At the bottom of Exhibit 12, the first four autocorrelations of the residual 
are displayed along with the standard error and the t-statistic for each of 
those autocorrelations.6 The sample has 65 observations, so the standard 
error for each of the autocorrelations is  1 /  √ _ 65    = 0.1240. Exhibit 12 shows 
that none of the first four autocorrelations has a t-statistic larger than 1.6818 
in absolute value. Therefore, Jones can conclude that none of these autocor -
relations differs significantly from 0. Consequently, she can assume that the 
residuals are not serially correlated and that the model is correctly specified, 
and she can validly use ordinary least squares to estimate the parameters 
and the parameters’ standard errors in the autoregressive model (for other 
tests for serial correlation of residuals, see Diebold 2008).
Now that Jones has concluded that this model is correctly specified, how 
can she use it to predict Intel’s gross margin in the next period? The estimat -
ed equation is Gross margint = 0.1513 + 0.7462(Gross margint−1) + εt. The 
expected value of the error term is 0 in any period. Therefore, this model 
predicts that gross margin in period t + 1 will be Gross margint+1 = 0.1513 
+ 0.7462(Gross margint). For example, if gross margin is 55% in this quarter 
(0.55), the model predicts that in the next quarter gross margin will increase 
to 0.1513 + 0.7462(0.55) = 0.5617, or 56.17%. However, if gross margin is 
currently 65% (0.65), the model predicts that in the next quarter, gross mar -
gin will fall to 0.1513 + 0.7462(0.65) = 0.6363, or 63.63%. As we show in the 
following section, the model predicts that gross margin will increase if it is 
below a certain level (59.61%) and decrease if it is above that level.
MEAN REVERSION AND MULTIPERIOD FORECASTS
explain mean reversion and calculate a mean-reverting level
describe the structure of an autoregressive (AR) model of order 
p and calculate one- and two-period-ahead forecasts given the 
estimated coefficients
6 For seasonally unadjusted data, analysts often compute the same number of autocorrelations as there 
are observations in a year (for example, four for quarterly data). The number of autocorrelations computed 
also often depends on sample size, as discussed in Diebold (2008).7 Time-Series Analysis 20
We say that a time series shows mean reversion if it tends to fall when its level is 
above its mean and rise when its level is below its mean. Much like the temperature in 
a room controlled by a thermostat, a mean-reverting time series tends to return to its 
long-term mean. How can we determine the value that the time series tends toward? If 
a time series is currently at its mean-reverting level, then the model predicts that the 
value of the time series will be the same in the next period. At its mean-reverting level, 
we have the relationship xt+1 = xt. For an AR(1) model (xt+1 = b0 + b1xt), the equality 
xt+1 = xt implies the level xt = b0 + b1xt or that the mean-reverting level, xt, is given by
   x  t   =    b  0   _ 1 −  b  1    . 
So the AR(1) model predicts that the time series will stay the same if its current value 
is b0/(1 − b1), increase if its current value is below b0/(1 − b1), and decrease if its 
current value is above b0/(1 − b1).
In the case of gross margins for Intel, the mean-reverting level for the model 
shown in Exhibit 12 is 0.1513/(1 − 0.7462) = 0.5961. If the current gross margin is 
above 0.5961, the model predicts that the gross margin will fall in the next period. If 
the current gross margin is below 0.5961, the model predicts that the gross margin 
will rise in the next period. As we will discuss later, all covariance-stationary time 
series have a finite mean-reverting level.
Multiperiod Forecasts and the Chain Rule of Forecasting
Often, financial analysts want to make forecasts for more than one period. For exam-
ple, we might want to use a quarterly sales model to predict sales for a company for 
each of the next four quarters. To use a time-series model to make forecasts for more 
than one period, we must examine how to make multiperiod forecasts using an AR(1) 
model. The one-period-ahead forecast of xt from an AR(1) model is as follows:
     ˆ x    t+1   =    ˆ b    0   +    ˆ b    1    x  t    (6)
If we want to forecast xt+2 using an AR(1) model, our forecast will be based on
     ˆ x    t+2   =    ˆ b    0   +    ˆ b    1    x  t+1    (7)
Unfortunately, we do not know xt+1 in period t,  so we cannot use Equation 7 directly 
to make a two-period-ahead forecast. We can, however, use our forecast of xt+1 and 
the AR(1) model to make a prediction of xt+2. The chain rule of forecasting is a 
process in which the next period’s value, predicted by the forecasting equation, is 
substituted into the equation to give a predicted value two periods ahead. Using the 
chain rule of forecasting, we can substitute the predicted value of xt+1 into Equation 
7 to get     ˆ x    t+2    =     ˆ b    0    +     ˆ b    1      ˆ x    t+1   . We already know     ˆ x    t+1    from our one-period-ahead forecast 
in Equation 6. Now we have a simple way of predicting xt+2.
Multiperiod forecasts are more uncertain than single-period forecasts because 
each forecast period has uncertainty. For example, in forecasting xt+2, we first have 
the uncertainty associated with forecasting xt+1 using xt, and then we have the uncer -
tainty associated with forecasting xt+2 using the forecast of xt+1. In general, the more 
periods a forecast has, the more uncertain it is. Note that if a forecasting model is 
well specified, the prediction errors from the model will not be serially correlated. 
If the prediction errors for each period are not serially correlated, then the variance 
of a multiperiod forecast will be higher than the variance of a single-period forecast.Mean Reversion and Multiperiod Forecasts 21
EXAMPLE 5
Multiperiod Prediction of Intel’s Gross Margin
Suppose that at the beginning of 2020, we want to predict Intel’s gross margin 
in two periods using the model shown in Exhibit 12. Assume that Intel’s gross 
margin in the current period is 63%. The one-period-ahead forecast of Intel’s 
gross margin from this model is 0.6214 = 0.1513 + 0.7462(0.63). By substituting 
the one-period-ahead forecast, 0.6214, back into the regression equation, we can 
derive the following two-period-ahead forecast: 0.6150 = 0.1513 + 0.7462(0.6214). 
Therefore, if the current gross margin for Intel is 63%, the model predicts that 
Intel’s gross margin in two quarters will be 61.50%.
EXAMPLE 6
Modeling US CPI Inflation
Analyst Lisette Miller has been directed to build a time-series model for monthly 
US inflation. Inflation and expectations about inflation, of course, have a signif -
icant effect on bond returns. For a 24-year period beginning January 1995 and 
ending December 2018, she selects as data the annualized monthly percentage 
change in the CPI. Which model should Miller use?
The process of model selection parallels that of Example 4 relating to Intel’s 
gross margins. The first model Miller estimates is an AR(1) model, using the 
previous month’s inflation rate as the independent variable: Inflationt = b0 + 
b1(Inflationt−1) + εt, t = 1, 2, . . . , 287. To estimate this model, she uses monthly 
CPI inflation data from January 1995 to December 2018 (t  = 1 denotes February 
1995). Exhibit 13 shows the results of estimating this model.
 
Exhibit 13: Monthly CPI Inflation at an Annual Rate: AR(1) Model—
Monthly Observations, February 1995–December 2018
 
 
Regression Statistics
R20.1586
Standard error 2.9687
Observations 287
Durbin–Watson 1.8442
 
 
  Coefficient Standard Error t-Statistic
Intercept 1.3346 0.2134 6.2540
Inflationt−1 0.3984 0.0544 7.3235
 
 
Autocorrelations of the Residual
Lag Autocorrelation Standard Error t-Statistic
1 0.0777 0.0590 1.3175
2 −0.1653 0.0590 −2.8013
3 −0.1024 0.0590 −1.7362
4 −0.0845 0.0590 1.4324
 
Source: US Bureau of Labor Statistics. Time-Series Analysis 22
As Exhibit 13 shows, both the intercept (    ˆ b    0    = 1.3346) and the coefficient on 
the first lagged value of inflation (    ˆ b    1    = 0.3984) are highly statistically significant, 
with large t -statistics. With 287 observations and two parameters, this model 
has 285 degrees of freedom. The critical value for a t -statistic at the 0.05 signif -
icance level is about 1.97. Therefore, Miller can reject the individual null hypoth -
eses that the intercept is equal to 0 (b0 = 0) and the coefficient on the first lag 
is equal to 0 (b1 = 0) in favor of the alternative hypothesis that the coefficients, 
individually, are not equal to 0.
Are these statistics valid? Miller will know when she tests whether the 
residuals from this model are serially correlated. With 287 observations in this 
sample, the standard error for each of the estimated autocorrelations is  1 /  √ _ 287    
= 0.0590. The critical value for the t -statistic is 1.97. Because the second esti-
mated autocorrelation has t -statistic larger than 1.97 in absolute value, Miller 
concludes that the autocorrelations are significantly different from 0. This model 
is thus misspecified because the residuals are serially correlated.
If the residuals in an autoregressive model are serially correlated, Miller 
can eliminate the correlation by estimating an autoregressive model with more 
lags of the dependent variable as explanatory variables. Exhibit 14 shows the 
result of estimating a second time-series model, an AR(2) model using the 
same data as in the analysis shown in Exhibit 13. With 286 observations and 
three parameters, this model has 283 degrees of freedom. Because the degrees 
of freedom are almost the same as those for the estimates shown in Exhibit 13, 
the critical value of the t -statistic at the 0.05 significance level also is almost 
the same (1.97). If she estimates the equation with two lags—Inflationt = b0 + 
b1(Inflationt−1) + b2(Inflationt−2) + εt—Miller finds that all three of the coef -
ficients in the regression model (an intercept and the coefficients on two lags 
of the dependent variable) differ significantly from 0. The bottom portion of 
Exhibit 14 shows that none of the first four autocorrelations of the residual has 
a t-statistic greater in absolute value than the critical value of 1.97. Therefore, 
Miller fails to reject the hypothesis that the individual autocorrelations of the 
residual equal 0. She concludes that this model is correctly specified because 
she finds no evidence of serial correlation in the residuals.
 
Exhibit 14: Monthly CPI Inflation at an Annual Rate: AR(2) Model—
Monthly Observations, March 1995–December 2018
 
 
Regression Statistics
R20.1907
Standard error 2.9208Comparing Forecast Model Performance 23
Regression Statistics
Observations 286
Durbin–Watson 1.9934
 
 
  Coefficient Standard Error t-Statistic
Intercept 1.5996 0.2245 7.1252
Inflationt−1 0.4759 0.0583 8.1636
Inflationt−2 −0.1964 0.0583 −3.368
 
 
Autocorrelations of the Residual
Lag Autocorrelation Standard Error t-Statistic
1 0.0032 0.0591 0.0536
2 0.0042 0.0591 0.0707
3 −0.0338 0.0591 −0.5696
4 0.0155 0.0591 0.2623
 
Source: US Bureau of Labor Statistics.
1. The analyst selected an AR(2) model because the residuals from the AR(1) 
model were serially correlated. Suppose that in a given month, inflation 
had been 4% at an annual rate in the previous month and 3% in the month 
before that. What would be the difference in the analyst forecast of inflation 
for that month if she had used an AR(1) model instead of the AR(2) model?
Solution to 1:
The AR(1) model shown in Exhibit 13 predicted that inflation in the next 
month would be 1.3346 + 0.3984(4) = 2.93%, approximately, whereas the 
AR(2) model shown in Exhibit 14 predicts that inflation in the next month 
will be 1.5996 + 0.4759(4) − 0.1964(3) = 2.91% approximately. If the analyst 
had used the incorrect AR(1) model, she would have predicted inflation 
to be 2 bps higher (2.93% versus 2.91%) than when using the AR(2) model. 
Although in this case the difference in the predicted inflation is actually very 
small, this kind of scenario illustrates that using an incorrect forecast could 
adversely affect the quality of her company’s investment choices.
COMPARING FORECAST MODEL PERFORMANCE
contrast in-sample and out-of-sample forecasts and compare the 
forecasting accuracy of different time-series models based on the 
root mean squared error criterion8 Time-Series Analysis 24
One way to compare the forecast performance of two models is to compare the vari-
ance of the forecast errors that the two models make. The model with the smaller 
forecast error variance will be the more accurate model, and it will also have the 
smaller standard error of the time-series regression. (This standard error usually is 
reported directly in the output for the time-series regression.)
In comparing forecast accuracy among models, we must distinguish between 
in-sample forecast errors and out-of-sample forecast errors. In-sample forecast 
errors are the residuals from a fitted time-series model. For example, when we esti-
mated a linear trend with raw inflation data from January 1995 to December 2018, the 
in-sample forecast errors were the residuals from January 1995 to December 2018. 
If we use this model to predict inflation outside this period, the differences between 
actual and predicted inflation are out-of-sample forecast errors.
EXAMPLE 7
In-Sample Forecast Comparisons of US CPI Inflation
In Example 6, the analyst compared an AR(1) forecasting model of monthly US 
inflation with an AR(2) model of monthly US inflation and decided that the AR(2) 
model was preferable. Exhibit 13 showed that the standard error from the AR(1) 
model of inflation is 2.9687, and Exhibit 14 showed that the standard error from 
the AR(2) model is 2.9208. Therefore, the AR(2) model had a lower in-sample 
forecast error variance than the AR(1) model had, which is consistent with our 
belief that the AR(2) model was preferable. Its standard error is 2.9208/2.9687 
= 98.39% of the forecast error of the AR(1) model.
Often, we want to compare the forecasting accuracy of different models after the 
sample period for which they were estimated. We wish to compare the out-of-sample 
forecast accuracy of the models. Out-of-sample forecast accuracy is important because 
the future is always out of sample. Although professional forecasters distinguish 
between out-of-sample and in-sample forecasting performance, many articles that 
analysts read contain only in-sample forecast evaluations. Analysts should be aware that 
out-of-sample performance is critical for evaluating a forecasting model’s real-world 
contribution.
Typically, we compare the out-of-sample forecasting performance of forecasting 
models by comparing their root mean squared error (RMSE), which is the square 
root of the average squared error. The model with the smallest RMSE is judged the 
most accurate. The following example illustrates the computation and use of RMSE 
in comparing forecasting models.Comparing Forecast Model Performance 25
EXAMPLE 8
Out-of-Sample Forecast Comparisons of US CPI Inflation
1. Suppose we want to compare the forecasting accuracy of the AR(1) and 
AR(2) models of US inflation estimated over 1995 to 2018, using CPI data 
from January 2019 to September 2019.
 
Exhibit 15: Out-of-Sample Forecast Error Comparisons: January 2019–September 2019 US CPI Inflation 
(Annualized)
 
 
Date Infl(t ) Infl(t −1) Infl(t −2) AR(1) Error Squared Error AR(2) Error Squared Error
2019              
January 0.0000 0.0000 0.0000 0.1335 0.0178 −1.6000 2.5599
February 2.4266 0.0000 0.0000 −2.2931 5.2585 0.8266 0.6833
March 4.9070 2.4266 0.0000 −3.8068 14.4916 2.1522 4.6320
April 3.6600 4.9070 2.4266 −1.5716 2.4699 0.2014 0.0406
May 1.2066 3.6600 4.9070 0.3850 0.1482 −1.1714 1.3722
June 1.2066 1.2066 3.6600 −0.5924 0.3510 −0.2488 0.0619
July 3.6600 1.2066 1.2066 −3.0458 9.2770 1.7228 2.9680
August 1.2066 3.6600 1.2066 0.3850 0.1482 −1.8982 3.6030
September 0.0000 1.2066 3.6600 0.6142 0.3772 −1.4554 2.1181
        Average 3.6155 Average 2.0043
        RMSE 1.9014 RMSE 1.4157
 
Note: Any apparent discrepancies between error and squared error results are due to rounding.
Source: US Bureau of Labor Statistics. 
Solution:
For each month from January 2019 to September 2019, the first column of 
numbers in Exhibit 15 shows the actual annualized inflation rate during 
the month. The second and third columns show the rate of inflation in the 
previous two months. The fourth column shows the out-of-sample errors 
(Actual − Forecast) from the AR(1) model shown in Exhibit 13. The fifth 
column shows the squared errors from the AR(1) model. The sixth column 
shows the out-of-sample errors from the AR(2) model shown in Exhibit 14. 
The final column shows the squared errors from the AR(2) model. The bot -
tom of the table displays the average squared error and the RMSE. Accord-
ing to these measures, the AR(2) model was slightly more accurate than the 
AR(1) model in its out-of-sample forecasts of inflation from January 2019 to 
September 2019. The RMSE from the AR(2) model was only 1.4157/1.9014 
= 74.46% as large as the RMSE from the AR(1) model. Therefore, the AR(2) 
model was more accurate both in sample and out of sample. Of course, this 
was a small sample to use in evaluating out-of-sample forecasting perfor -
mance. Sometimes, an analyst may have conflicting information about 
whether to choose an AR(1) or an AR(2) model. We must also consider 
regression coefficient stability. We will continue the comparison between 
these two models in the following section. Time-Series Analysis 26
INSTABILITY OF REGRESSION COEFFICIENTS
explain the instability of coefficients of time-series models
One of the important issues an analyst faces in modeling a time series is the sample 
period to use. The estimates of regression coefficients of the time-series model can 
change substantially across different sample periods used for estimating the model. 
Often, the regression coefficient estimates of a time-series model estimated using an 
earlier sample period can be quite different from those of a model estimated using a 
later sample period. Similarly, the estimates can be different between models estimated 
using relatively shorter and longer sample periods. Further, the choice of model for a 
particular time series can also depend on the sample period. For example, an AR(1) 
model may be appropriate for the sales of a company in one particular sample period, 
but an AR(2) model may be necessary for an earlier or later sample period (or for a 
longer or shorter sample period). Thus, the choice of a sample period is an important 
decision in modeling a financial time series.
Unfortunately, there is usually no clear-cut basis in economic or financial the-
ory for determining whether to use data from a longer or shorter sample period to 
estimate a time-series model. We can get some guidance, however, if we remember 
that our models are valid only for covariance-stationary time series. For example, we 
should not combine data from a period when exchange rates were fixed with data 
from a period when exchange rates were floating. The exchange rates in these two 
periods would not likely have the same variance because exchange rates are usually 
much more volatile under a floating-rate regime than when rates are fixed. Similarly, 
many US analysts consider it inappropriate to model US inflation or interest-rate 
behavior since the 1960s as a part of one sample period, because the Federal Reserve 
had distinct policy regimes during this period. A simple way to determine appropriate 
samples for time-series estimation is to look at graphs of the data to see whether the 
time series looks stationary before estimation begins. If we know that a government 
policy changed on a specific date, we might also test whether the time-series relation 
was the same before and after that date.
In the following example, we illustrate how the choice of a longer versus a shorter 
period can affect the decision of whether to use, for example, a first- or second-order 
time-series model. We then show how the choice of the time-series model (and the 
associated regression coefficients) affects our forecast. Finally, we discuss which sample 
period, and accordingly which model and corresponding forecast, is appropriate for 
the time series analyzed in the example.
EXAMPLE 9
Instability in Time-Series Models of US Inflation
In Example 6, the analyst Lisette Miller concluded that US CPI inflation should 
be modeled as an AR(2) time series. A colleague examined her results and 
questioned estimating one time-series model for inflation in the United States 
since 1995, given that the Federal Reserve responded aggressively to the financial 
crisis that emerged in 2007. He argues that the inflation time series from 1995 
to 2018 has two regimes or underlying models generating the time series: one 
running from 1995 through 2007 and another starting in 2008. Therefore, the 
colleague suggests that Miller estimate a new time-series model for US inflation 9Instability of Regression Coefficients 27
starting in 2008. Because of his suggestion, Miller first estimates an AR(1) model 
for inflation using data for a sample period from 2008 to 2018. Exhibit 16 shows 
her AR(1) estimates.
 
Exhibit 16: Autoregression: AR(1) Model Monthly CPI Inflation at an 
Annual Rate, January 2008–December 2018
 
 
Regression Statistics
R20.2536
Standard error 3.0742
Observations 132
Durbin–Watson 1.8164
 
 
  Coefficient Standard Error t-Statistic
Intercept 0.8431 0.2969 2.8397
Inflationt−1 0.5036 0.0758 6.6438
 
 
Autocorrelations of the Residual
Lag Autocorrelation Standard Error t-Statistic
1 0.0999 0.087 1.1479
2 −0.1045 0.087 −1.2015
3 −0.1568 0.087 −1.8051
4 0.0500 0.087 0.5750
 
Source: US Bureau of Labor Statistics.
The bottom part of Exhibit 16 shows that the first four autocorrelations of the 
residuals from the AR(1) model are quite small. None of these autocorrelations 
has a t -statistic larger than 1.99, the critical value for significance. Consequently, 
Miller cannot reject the null hypothesis that the residuals are serially uncorrelated. 
The AR(1) model is correctly specified for the sample period from 2008 to 2018, 
so there is no need to estimate the AR(2) model. This conclusion is very different 
from that reached in Example 6 using data from 1995 to 2018. In that example, 
Miller initially rejected the AR(1) model because its residuals exhibited serial 
correlation. When she used a larger sample, an AR(2) model initially appeared 
to fit the data much better than did an AR(1) model.
How deeply does our choice of sample period affect our forecast of future 
inflation? Suppose that in a given month, inflation was 4% at an annual rate, and 
the month before that it was 3%. The AR(1) model shown in Exhibit 16 predicts 
that inflation in the next month will be 0.8431 + 0.5036(4) ≈ 2.86%. Therefore, 
the forecast of the next month’s inflation using the 2008 to 2018 sample is 2.86%. 
Remember from the analysis following Example 6 that the AR(2) model for the 
1995 to 2018 sample predicts inflation of 2.91% in the next month. Thus, using 
the correctly specified model for the shorter sample produces an inflation fore-
cast 0.05 pps below the forecast made from the correctly specified model for the 
longer sample period. Such a difference might substantially affect a particular 
investment decision.
Which model is correct? Exhibit 17 suggests an answer. Monthly US inflation 
was so much more volatile during the middle part of the study period than in 
the earlier or later years that inflation is probably not a covariance-stationary 
time series from 1995 to 2018. Therefore, we can reasonably believe that the  Time-Series Analysis 28
data have more than one regime and Miller should estimate a separate model for 
inflation from 2009 to 2018, as shown previously. In fact, the standard deviation 
of annualized monthly inflation rates is just 2.86% for 1995–2007 but 3.54% for 
2008–2018, largely because of volatility during the 2008 crisis. As the example 
shows, experience (such as knowledge of government policy changes) and judg-
ment play a vital role in determining how to model a time series. Simply relying 
on autocorrelations of the residuals from a time-series model cannot tell us the 
correct sample period for our analysis.
 
Exhibit 17: Monthly CPI Inflation
 
025
20
15
10
5
–5
–10
–15
–20
–25
Jan/95 May/99 Sep/03 Jan/08 May/12 Sep/16
Source: US Bureau of Labor Statistics.
RANDOM WALKS
describe characteristics of random walk processes and contrast them 
to covariance stationary processes
explain mean reversion and calculate a mean-reverting level
So far, we have examined those time series in which the time series has a tendency 
to revert to its mean level as the change in a variable from one period to the next 
follows a mean-reverting pattern. In contrast, there are many financial time series in 
which the changes follow a random pattern. We discuss these “random walks” in the 
following section.
Random Walks
A random walk is one of the most widely studied time-series models for financial 
data. A random walk is a time series in which the value of the series in one period 
is the value of the series in the previous period plus an unpredictable random error. 
A random walk can be described by the following equation:10Random Walks 29
   x  t   =  x  t−1   +  ε  t  ,  E   (   ε  t   )     = 0,  E   (   ε  t  2  )     =  σ   2 ,  cov   (   ε  t  ,  ε  s   )     = E   (   ε  t    ε  s   )     
 = 0 if t ≠ s.  (8)
Equation 8 means that the time series xt is in every period equal to its value in the 
previous period plus an error term, εt, that has constant variance and is uncorrelated 
with the error term in previous periods. Note two important points. First, this equa-
tion is a special case of an AR(1) model with b0 = 0 and b1 = 1.7 Second, the expected 
value of εt is zero. Therefore, the best forecast of xt that can be made in period t  − 1 
is xt−1. In fact, in this model, xt−1 is the best forecast of x  in every period after t  − 1.
Random walks are quite common in financial time series. For example, many 
studies have tested whether and found that currency exchange rates follow a random 
walk. Consistent with the second point made in the previous paragraph, some studies 
have found that sophisticated exchange rate forecasting models cannot outperform 
forecasts made using the random walk model and that the best forecast of the future 
exchange rate is the current exchange rate.
Unfortunately, we cannot use the regression methods we have discussed so far to 
estimate an AR(1) model on a time series that is actually a random walk. To see why 
this is so, we must determine why a random walk has no finite mean-reverting level 
or finite variance. Recall that if xt is at its mean-reverting level, then xt = b0 + b1xt, or 
xt = b0/(1 − b1). In a random walk, however, b0 = 0 and b1 = 1, so b0/(1 − b1) = 0/0. 
Therefore, a random walk has an undefined mean-reverting level.
What is the variance of a random walk? Suppose that in Period 1, the value of x1 is 
0. Then we know that x2 = 0 + ε2. Therefore, the variance of x2 = var(ε2) = σ2. Now x3 = 
x2 + ε3 = ε2 + ε3. Because the error term in each period is assumed to be uncorrelated 
with the error terms in all other periods, the variance of x3 = var(ε2) + var(ε3) = 2σ2. 
By a similar argument, we can show that for any period t,  the variance of xt = (t  − 1)
σ2. But this means that as t  grows large, the variance of xt grows without an upper 
bound: It approaches infinity. This lack of upper bound, in turn, means that a random 
walk is not a covariance-stationary time series, because a covariance-stationary time 
series must have a finite variance.
What is the practical implication of these issues? We cannot use standard regression 
analysis on a time series that is a random walk. We can, however, attempt to convert 
the data to a covariance-stationary time series if we suspect that the time series is a 
random walk. In statistical terms, we can difference it.
We difference a time series by creating a new time series—say, yt—that in each 
period is equal to the difference between xt and xt−1. This transformation is called 
first-differencing because it subtracts the value of the time series in the first prior 
period from the current value of the time series. Sometimes the first difference of 
xt is written as Δxt = xt − xt−1. Note that the first difference of the random walk in 
Equation 8 yields
   y  t   =  x  t   −  x  t−1   =  ε  t  ,  E   (   ε  t   )     = 0,  E   (   ε  t  2  )     =  σ   2 ,  cov   (   ε  t  ,  ε  s   )     = E   (   ε  t    ε  s   )     
 = 0 for t  ≠ s. 
The expected value of εt is 0. Therefore, the best forecast of yt that can be made 
in period t  − 1 is 0. This implies that the best forecast is that there will be no change 
in the value of the current time series, xt−1.
The first-differenced variable, yt, is covariance stationary. How is this so? First, note 
that this model (yt = εt) is an AR(1) model with b0 = 0 and b1 = 0. We can compute the 
mean-reverting level of the first-differenced model as b0/(1 − b1) = 0/1 = 0. Therefore, 
a first-differenced random walk has a mean-reverting level of 0. Note also that the 
variance of yt in each period is var(εt) = σ2. Because the variance and the mean of yt 
7 Equation 8 with a nonzero intercept added (as in Equation 9, given later) is sometimes referred to as a 
random walk with drift. Time-Series Analysis 30
are constant and finite in each period, yt is a covariance-stationary time series and we 
can model it using linear regression. Of course, modeling the first-differenced series 
with an AR(1) model does not help us predict the future, because b0 = 0 and b1 = 0. 
We simply conclude that the original time series is, in fact, a random walk.
Had we tried to estimate an AR(1) model for a time series that was a random walk, 
our statistical conclusions would have been incorrect because AR models cannot be 
used to estimate random walks or any time series that is not covariance stationary. 
The following example illustrates this issue with exchange rates.
EXAMPLE 10
The Yen/US Dollar Exchange Rate
1. Financial analysts often assume that exchange rates are random walks. Con-
sider an AR(1) model for the Japanese yen/US dollar exchange rate (JPY/
USD). Exhibit 18 shows the results of estimating the model using month-
end observations from October 1980 through August 2019.
 
Exhibit 18: Y en/US Dollar Exchange Rate: AR(1) Model Month-End 
Observations, October 1980–August 2019
 
 
Regression Statistics
R20.9897
Standard error 4.5999
Observations 467
Durbin–Watson 1.9391
 
 
  Coefficient Standard Error t-Statistic
Intercept 0.8409 0.6503 1.2931
JPY/USDt−1 0.9919 0.0047 211.0426
 
 
Autocorrelations of the Residual
Lag Autocorrelation Standard Error t-Statistic
1 0.0302 0.0465 0.6495
2 0.0741 0.0465 1.5935
3 0.0427 0.0465 0.9183
4 −0.0034 0.0465 0.0731
 
Source: US Federal Reserve Board of Governors.
The results in Exhibit 18 suggest that the yen/US dollar exchange rate is a 
random walk because the estimated intercept does not appear to be signifi-
cantly different from 0 and the estimated coefficient on the first lag of the 
exchange rate is very close to 1. Can we use the t-statistics in Exhibit 18 to 
test whether the exchange rate is a random walk? Unfortunately, no, because 
the standard errors in an AR model are invalid if the model is estimated 
using a data series that is a random walk (remember, a random walk is not 
covariance stationary). If the exchange rate is, in fact, a random walk, we 
might come to an incorrect conclusion based on faulty statistical tests and Random Walks 31
then invest incorrectly. We can use a test presented in the next section to 
test whether the time series is a random walk.
Suppose the exchange rate is a random walk, as we now suspect. If so, the 
first-differenced series, yt = xt − xt−1, will be covariance stationary. We 
present the results from estimating yt = b0 + b1yt−1 + εt in Exhibit 19. If the 
exchange rate is a random walk, then b0 = 0, b1 = 0, and the error term will 
not be serially correlated.
 
Exhibit 19: First-Differenced Y en/US Dollar Exchange Rate: AR(1) 
Model Month-End Observations, November 1980–August 2019
 
 
Regression Statistics
R20.0008
Standard error 4.6177
Observations 466
Durbin–Watson 2.0075
 
 
  Coefficient Standard Error t-Statistic
Intercept −0.2185 0.2142 −1.0200
JPY/USDt−1 
− JPY/USDt−20.0287 0.0464 0.6185
 
 
Autocorrelations of the Residual
Lag Autocorrelation Standard Error t-Statistic
1 −0.0023 0.0463 −0.0501
2 0.0724 0.0463 1.5643
3 0.0387 0.0463 0.8361
4 −0.0062 0.0463 −0.1329
 
Source: US Federal Reserve Board of Governors.
In Exhibit 19, neither the intercept nor the coefficient on the first lag of the 
first-differenced exchange rate differs significantly from 0, and no residual 
autocorrelations differ significantly from 0. These findings are consistent 
with the yen/US dollar exchange rate being a random walk.
We have concluded that the differenced regression is the model to choose. 
Now we can see that we would have been seriously misled if we had based 
our model choice on an R2 comparison. In Exhibit 18, the R2 is 0.9897, 
whereas in Exhibit 19, the R2 is 0.0008. How can this be, if we just concluded 
that the model in Exhibit 19 is the one that we should use? In Exhibit 18, the 
R2 measures how well the exchange rate in one period predicts the exchange 
rate in the next period. If the exchange rate is a random walk, its current 
value will be an extremely good predictor of its value in the next period, and 
thus the R2 will be extremely high. At the same time, if the exchange rate 
is a random walk, then changes in the exchange rate should be completely 
unpredictable. Exhibit 19 estimates whether changes in the exchange rate 
from one month to the next can be predicted by changes in the exchange 
rate over the previous month. If they cannot be predicted, the R2 in Exhibit  Time-Series Analysis 32
19 should be very low. In fact, it is low (0.0008). This comparison provides a 
good example of the general rule that we cannot necessarily choose which 
model is correct solely by comparing the R2 from the two models.
The exchange rate is a random walk, and changes in a random walk are by 
definition unpredictable. Therefore, we cannot profit from an investment 
strategy that predicts changes in the exchange rate.
To this point, we have discussed only simple random walks—that is, random walks 
without drift. In a random walk without drift, the best predictor of the time series 
in the next period is its current value. A random walk with drift, however, should 
increase or decrease by a constant amount in each period. The equation describing a 
random walk with drift is a special case of the AR(1) model:
    x  t   =  b  0   +  b  1    x  t−1   +  ε  t  ,
    b  1   = 1,    b  0   ≠ 0, or   
 x  t   =  b  0   +  x  t−1   +  ε  t  ,  E   (   ε  t   )     = 0.   (9)
A random walk with drift has b0 ≠ 0, compared to a simple random walk, which 
has b0 = 0.
We have already seen that b1 = 1 implies an undefined mean-reversion level and 
thus nonstationarity. Consequently, we cannot use an AR model to analyze a time 
series that is a random walk with drift until we transform the time series by taking 
first differences. If we first-difference Equation 9, the result is yt = xt − xt−1, yt = b0 
+ εt, b0 ≠ 0.
THE UNIT ROOT TEST OF NONSTATIONARITY
describe implications of unit roots for time-series analysis, explain 
when unit roots are likely to occur and how to test for them, and 
demonstrate how a time series with a unit root can be transformed 
so it can be analyzed with an AR model
describe the steps of the unit root test for nonstationarity and 
explain the relation of the test to autoregressive time-series models
In this section, we discuss how to use random walk concepts to determine whether a 
time series is covariance stationary. This approach focuses on the slope coefficient in 
the random-walk-with-drift case of an AR(1) model in contrast with the traditional 
autocorrelation approach, which we discuss first.
The examination of the autocorrelations of a time series at various lags is a 
well-known prescription for inferring whether or not a time series is stationary. 
Typically, for a stationary time series, either autocorrelations at all lags are statistically 
indistinguishable from zero or the autocorrelations drop off rapidly to zero as the 
number of lags becomes large. Conversely, the autocorrelations of a nonstationary 
time series do not exhibit those characteristics. However, this approach is less definite 
than a currently more popular test for nonstationarity known as the Dickey–Fuller 
test for a unit root.
We can explain what is known as the unit root problem in the context of an AR(1) 
model. If a time series comes from an AR(1) model, then to be covariance stationary, 
the absolute value of the lag coefficient, b1, must be less than 1.0. We could not rely 
on the statistical results of an AR(1) model if the absolute value of the lag coefficient 11The Unit Root Test of Nonstationarity 33
were greater than or equal to 1.0 because the time series would not be covariance 
stationary. If the lag coefficient is equal to 1.0, the time series has a unit root: It is 
a random walk and is not covariance stationary (note that when b1 is greater than 1 
in absolute value, we say that there is an “explosive root”). By definition, all random 
walks, with or without a drift term, have unit roots.
How do we test for unit roots in a time series? If we believed that a time series, xt, 
was a random walk with drift, it would be tempting to estimate the parameters of the 
AR(1) model xt = b0 + b1xt−1 + εt using linear regression and conduct a t -test of the 
hypothesis that b1 = 1. Unfortunately, if b1 = 1, then xt is not covariance stationary and 
the t-value of the estimated coefficient,     ˆ b    1   , does not actually follow the t -distribution; 
consequently, a t -test would be invalid.
Dickey and Fuller (1979) developed a regression-based unit root test based on a 
transformed version of the AR(1) model xt = b0 + b1xt−1 + εt. Subtracting xt−1 from 
both sides of the AR(1) model produces
 xt − xt−1 = b0 + (b1 − 1)xt−1 + εt,
or
 xt − xt−1 = b0 + g1xt−1 + εt, E(εt) = 0,   (10)
where g1 = (b1 − 1). If b1 = 1, then g1 = 0 and thus a test of g1 = 0 is a test of b1 = 
1. If there is a unit root in the AR(1) model, then g1 will be 0 in a regression where 
the dependent variable is the first difference of the time series and the independent 
variable is the first lag of the time series. The null hypothesis of the Dickey–Fuller test 
is H0: g1 = 0—that is, that the time series has a unit root and is nonstationary—and 
the alternative hypothesis is Ha: g1 < 0, that the time series does not have a unit root 
and is stationary.
To conduct the test, one calculates a t -statistic in the conventional manner for     ˆ g    1    
but instead of using conventional critical values for a t -test, one uses a revised set 
of values computed by Dickey and Fuller; the revised critical values are larger in 
absolute value than the conventional critical values. A number of software packages 
incorporate Dickey–Fuller tests. 
EXAMPLE 11
(Historical Example)
AstraZeneca’s Quarterly Sales (1)
In January 2012, equity analyst Aron Berglin is building a time-series model 
for the quarterly sales of AstraZeneca, a British/Swedish biopharmaceutical 
company headquartered in London. He is using AstraZeneca’s quarterly sales 
in US dollars for January 2000 to December 2011 and any lagged sales data that 
he may need prior to 2000 to build this model. He finds that a log-linear trend 
model seems better suited for modeling AstraZeneca’s sales than does a linear 
trend model. However, the Durbin–Watson statistic from the log-linear regres -
sion is just 0.7064, which causes him to reject the hypothesis that the errors in 
the regression are serially uncorrelated. He concludes that he cannot model the 
log of AstraZeneca’s quarterly sales using only a time trend line. He decides to 
model the log of AstraZeneca’s quarterly sales using an AR(1) model. He uses 
ln Salest = b0 + b1(ln Salest−1) + εt.
Before he estimates this regression, the analyst should use the Dickey–Fuller 
test to determine whether there is a unit root in the log of AstraZeneca’s quarterly 
sales. If he uses the sample of quarterly data on AstraZeneca’s sales from the 
first quarter of 2000 through the fourth quarter of 2011, takes the natural log  Time-Series Analysis 34
of each observation, and computes the Dickey–Fuller t -test statistic, the value 
of that statistic might cause him to fail to reject the null hypothesis that there 
is a unit root in the log of AstraZeneca’s quarterly sales.
If a time series appears to have a unit root, how should we model it? One method 
that is often successful is to model the first-differenced series as an autoregressive 
time series. The following example demonstrates this method.
EXAMPLE 12
AstraZeneca’s Quarterly Sales (2)
1. The plot of the log of AstraZeneca’s quarterly sales is shown in Exhibit 20. 
By looking at the plot, Berglin is convinced that the log of quarterly sales is 
not covariance stationary (that it has a unit root). 
 
Exhibit 20: Log of AstraZeneca’s Quarterly Sales
 
Ln ($ millions)
10,000
8,000
6,000
4,000
2,000
0
00 12 04 02 06 08 10
Year
Source: Compustat.
So he creates a new series, yt, that is the first difference of the log of Astra-
Zeneca’s quarterly sales. Exhibit 21 shows that series.
Berglin compares Exhibit 21 to Exhibit 20 and notices that first-differencing 
the log of AstraZeneca’s quarterly sales eliminates the strong upward trend 
that was present in the log of AstraZeneca’s sales. Because the first-differ -
enced series has no strong trend, Berglin is better off assuming that the 
differenced series is covariance stationary rather than assuming that Astra-
Zeneca’s sales or the log of AstraZeneca’s sales is a covariance-stationary 
time series.The Unit Root Test of Nonstationarity 35
 
Exhibit 21: Log Difference, AstraZeneca’s Quarterly Sales
 
Ln Dif ference
0.2
0.1
0
–0.1
–0.2
00 12 04 02 06 08 10
Year
Source: Compustat.
Now suppose Berglin decides to model the new series using an AR(1) mod-
el. Berglin uses ln(Salest) − ln(Salest−1) = b0 + b1[ln(Salest−1) − ln(Salest−2)] 
+ εt. Exhibit 22 shows the results of that regression.
 
Exhibit 22: Log Differenced Sales: AR(1) Model of AstraZeneca 
Quarterly Observations, January 2000–December 2011
 
 
Regression Statistics  
R20.3005
Standard error 0.0475
Observations 48
Durbin–Watson 1.6874
 
 
  Coefficient Standard Error t-Statistic
Intercept 0.0222 0.0071 3.1268
ln Salest−1 − 
ln Salest−2−0.5493 0.1236 −4.4442
 
 
Autocorrelations of the Residual
Lag Autocorrelation Standard Error t-Statistic
1 0.2809 0.1443 1.9466
2 −0.0466 0.1443 −0.3229
3 0.0081 0.1443 0.0561
4 0.2647 0.1443 1.8344
 
Source: Compustat.
The lower part of Exhibit 22 suggests that the first four autocorrelations of 
residuals in this model are not statistically significant. With 48 observations 
and two parameters, this model has 46 degrees of freedom. The critical  Time-Series Analysis 36
value for a t-statistic in this model is above 2.0 at the 0.05 significance level. 
None of the t-statistics for these autocorrelations has an absolute value 
larger than 2.0. Therefore, we fail to reject the null hypotheses that each of 
these autocorrelations is equal to 0 and conclude instead that no significant 
autocorrelation is present in the residuals.
This result suggests that the model is well specified and that we could use 
the estimates. Both the intercept (    ˆ b    0    = 0.0222) and the coefficient (    ˆ b    1    = 
−0.5493) on the first lag of the new first-differenced series are statistically 
significant. 
1. Explain how to interpret the estimated coefficients in the model.
Solution:
The value of the intercept (0.0222) implies that if sales have not changed in 
the current quarter (yt = ln Salest − ln Salest−1 = 0), sales will grow by 2.22% 
next quarter.8 If sales have changed during this quarter, however, the model 
predicts that sales will grow by 2.22% minus 0.5493 times the sales growth in 
this quarter.
2. AstraZeneca’s sales in the third and fourth quarters of 2011 were 
$8,405 million and $8,872 million, respectively. If we use the previous 
model soon after the end of the fourth quarter of 2011, what will be 
the predicted value of AstraZeneca’s sales for the first quarter of 2012?
Solution:
Let us say that t is the fourth quarter of 2011, so t − 1 is the third quarter 
of 2011 and t + 1 is the first quarter of 2012. Then we would have to com-
pute     ˆ y    t+1    = 0.0222 − 0.5493yt. To compute     ˆ y    t+1   , we need to know yt = ln 
Salest − ln Salest−1. In the third quarter of 2011, AstraZeneca’s sales were 
$8,405 million, so ln Salest−1 = ln 8,405 = 9.0366. In the fourth quarter 
of 2011, AstraZeneca’s sales were $8,872 million, so ln Salest = ln 8,872 
= 9.0907. Thus yt = 9.0907 − 9.0366 = 0.0541. Therefore,     ˆ y    t+1    = 0.0222 − 
0.5493(0.0541) = −0.0075. If     ˆ y    t+1    = −0.0075, then −0.0075 = ln Salest+1 − ln 
Salest = ln(Salest+1/Salest). If we exponentiate both sides of this equation, 
the result is
    e   −0.0075  =    (     Sales  t+1   _  Sales  t     )    .
    Sales  t+1   =  Sales  t    e   −0.0075     
= $8, 872 million × 0.9925    
= $8, 805 million.   
Thus, based on fourth quarter sales for 2011, this model would have pre-
dicted that AstraZeneca’s sales in the first quarter of 2012 would be $8,805 
million. This sales forecast might have affected our decision to buy Astra-
Zeneca’s stock at the time.
8 Note that 2.22 percent is the exponential growth rate, not [(Current quarter sales/Previous quarter sales) 
− 1]. The difference between these two methods of computing growth is usually small.Moving-Average Time-Series Models 37
MOVING-AVERAGE TIME-SERIES MODELS
So far, many of the forecasting models we have used have been autoregressive models. 
Because most financial time series have the qualities of an autoregressive process, 
autoregressive time-series models are probably the most frequently used time-series 
models in financial forecasting. Some financial time series, however, seem to more 
closely follow another kind of time-series model, called a moving-average model. For 
example, as we will show, returns on the S&P BSE 100 Index can be better modeled 
as a moving-average process than as an autoregressive process.
In this section, we present the fundamentals of moving-average models so that 
you can ask the right questions when considering their use. We first discuss how to 
smooth past values with a moving average and then how to forecast a time series 
using a moving-average model. Even though both methods include the words “moving 
average” in the name, they are very different.
Smoothing Past Values with an n-Period Moving Average
Suppose you are analyzing the long-term trend in the past sales of a company. In 
order to focus on the trend, you may find it useful to remove short-term fluctuations 
or noise by smoothing out the time series of sales. One technique to smooth out 
period-to-period fluctuations in the value of a time series is an n -period moving 
average. An n -period moving average of the current and past n  − 1 values of a time 
series, xt, is calculated as
     x  t   +  x  t−1   + ⋯ +  x  t−   (  n−1 )        _________________  n  .  (11)
The following example demonstrates how to compute a moving average of AstraZeneca’s 
quarterly sales.
EXAMPLE 13
AstraZeneca’s Quarterly Sales (3)
Suppose we want to compute the four-quarter moving average of AstraZeneca’s 
sales as of the beginning of the first quarter of 2012. AstraZeneca’s sales in the 
previous four quarters were as follows: 1Q 2011, $8,490 million; 2Q 2011, $8,601 
million; 3Q 2011, $8,405 million; and 4Q 2011, $8,872 million. The four-quarter 
moving average of sales as of the beginning of the first quarter of 2012 is thus 
(8,490 + 8,601 + 8,405 + 8,872)/4 = $8,592 million.
We often plot the moving average of a series with large fluctuations to help discern 
any patterns in the data. Exhibit 23 shows monthly retail sales for the United States 
from December 1995 to June 2019, along with a 12-month moving average of the 
data (data from January 1995 are used to compute the 12-month moving average).12 Time-Series Analysis 38
Exhibit 23: Monthly US Real Retail Sales and 12-Month Moving Average of 
Retail Sales
600,000
500,000
400,000
300,000
200,000
100,000
0
Dec/95 Dec/98 Dec/01 Dec/04 Dec/07 Dec/10 Dec/13 Dec/17
Monthly Sales Moving Average
Source: Bloomberg.
As Exhibit 23 shows, each year has a very strong peak in retail sales (December) 
followed by a sharp drop in sales (January). Because of the extreme seasonality in the 
data, a 12-month moving average can help us focus on the long-term movements 
in retail sales instead of seasonal fluctuations. Note that the moving average does 
not have the sharp seasonal fluctuations of the original retail sales data. Rather, the 
moving average of retail sales grows steadily—for example, from 1995 through the 
second half of 2008—and then declines for about a year and grows steadily thereafter. 
We can see that trend more easily by looking at a 12-month moving average than by 
looking at the time series itself.
Exhibit 24 shows monthly Europe Brent Crude Oil spot prices along with a 
12-month moving average of oil prices. Although these data do not have the same 
sharp regular seasonality displayed in the retail sales data in Exhibit 23, the moving 
average smooths out the monthly fluctuations in oil prices to show the longer-term 
movements.Moving-Average Time-Series Models 39
Exhibit 24: Monthly Europe Brent Crude Oil Price and 12-Month Moving 
Average of Prices
140
120
100
80
60
40
20
0
Jan/97 Oct/99 Jul/02 Apr/05 Jan/08 Oct/10 Jul/13 Apr/16 Jan/19
Spot Price Moving Average
Source: US Energy Information Administration.
Exhibit 24 also shows one weakness with a moving average: It always lags large 
movements in the actual data. For example, when oil prices rose quickly in late 2007 
and the first half of 2008, the moving average rose only gradually. When oil prices 
fell sharply toward the end of 2008, the moving average also lagged. Consequently, 
a simple moving average of the recent past, though often useful in smoothing out a 
time series, may not be the best predictor of the future. A main reason for this is that 
a simple moving average gives equal weight to all the periods in the moving average. 
In order to forecast the future values of a time series, it is often better to use a more 
sophisticated moving-average time-series model. We discuss such models below.
Moving-Average Time-Series Models for Forecasting
Suppose that a time series, xt, is consistent with the following model:
    x  t   =  ε  t   + θ  ε  t−1  ,  E   (   ε  t   )     = 0,  E   (   ε  t  2  )     =  σ   2 ,  
      
cov   (   ε  t  ,  ε  s   )     = E   (   ε  t    ε  s   )     = 0 for t  ≠ s.    (12)
This equation is called a moving-average model of order 1, or simply an MA(1) model. 
Theta (θ) is the parameter of the MA(1) model.9
Equation 12 is a moving-average model because in each period, xt is a moving aver -
age of εt and εt−1, two uncorrelated random variables that each have an expected value 
of zero. Unlike the simple moving-average model of Equation 11, this moving-average 
model places different weights on the two terms in the moving average (1 on εt, and 
θ on εt−1).
9 Note that a moving-average time-series model is very different from a simple moving average, as 
discussed in Section 6.1. The simple moving average is based on observed values of a time series. In a 
moving-average time-series model, we never directly observe εt or any other εt–j, but we can infer how a 
particular moving-average model will imply a particular pattern of serial correlation for a time series, as 
we will discuss. Time-Series Analysis 40
We can see if a time series fits an MA(1) model by looking at its autocorrelations 
to determine whether xt is correlated only with its preceding and following values. 
First, we examine the variance of xt in Equation 12 and its first two autocorrelations. 
Because the expected value of xt is 0 in all periods and εt is uncorrelated with its own 
past values, the first autocorrelation is not equal to 0, but the second and higher auto -
correlations are equal to 0. Further analysis shows that all autocorrelations except for 
the first will be equal to 0 in an MA(1) model. Thus for an MA(1) process, any value 
xt is correlated with xt−1 and xt+1 but with no other time-series values; we could say 
that an MA(1) model has a memory of one period.
Of course, an MA(1) model is not the most complex moving-average model. A 
qth-order moving-average model, denoted MA(q ) and with varying weights on lagged 
terms, can be written as
    x  t   =  ε  t   +  θ  1    ε  t−1   + ⋯ +  θ  q    ε  t−q  ,  E   (   ε  t   )     = 0,  E   (   ε  t  2  )     =  σ   2 ,
       
cov   (   ε  t  ,  ε  s   )     = E   (   ε  t    ε  s   )     = 0 for t  ≠ s.    (13)
How can we tell whether an MA(q ) model fits a time series? We examine the auto-
correlations. For an MA(q ) model, the first q  autocorrelations will be significantly 
different from 0, and all autocorrelations beyond that will be equal to 0; an MA(q ) 
model has a memory of q  periods. This result is critical for choosing the right value 
of q for an MA model. We discussed this result previously for the specific case of q  = 
1 that all autocorrelations except for the first will be equal to 0 in an MA(1) model.
How can we distinguish an autoregressive time series from a moving-average time 
series? Once again, we do so by examining the autocorrelations of the time series 
itself. The autocorrelations of most autoregressive time series start large and decline 
gradually, whereas the autocorrelations of an MA(q ) time series suddenly drop to 0 
after the first q  autocorrelations. We are unlikely to know in advance whether a time 
series is autoregressive or moving average. Therefore, the autocorrelations give us our 
best clue about how to model the time series. Most time series, however, are best 
modeled with an autoregressive model.
EXAMPLE 14
(Historical Example)
A Time-Series Model for Monthly Returns on the S&P BSE 
100 Index 
The S&P BSE 100 Index is designed to reflect the performance of India’s top 100 
large-cap companies listed on the BSE Ltd. (formerly Bombay Stock Exchange). 
Are monthly returns on the S&P BSE 100 Index autocorrelated? If so, we may 
be able to devise an investment strategy to exploit the autocorrelation. What is 
an appropriate time-series model for S&P BSE 100 monthly returns?
Exhibit 25 shows the first six autocorrelations of returns to the S&P BSE 100 
using monthly data from January 2000 through December 2013. Note that all 
of the autocorrelations are quite small. Do they reach significance? With 168 
observations, the critical value for a t-statistic in this model is about 1.98 at 
the 0.05 significance level. None of the autocorrelations has a t -statistic larger 
in absolute value than the critical value of 1.98. Consequently, we fail to reject 
the null hypothesis that those autocorrelations, individually, do not differ sig-
nificantly from 0.Seasonality in Time-Series Models 41
 
Exhibit 25: Annualized Monthly Returns to the S&P BSE 100, 
January 2000–December 2013
 
 
Autocorrelations
Lag Autocorrelation Standard Error t-Statistic
1 0.1103 0.0772 1.4288
2 −0.0045 0.0772 −0.0583
3 0.0327 0.0772 0.4236
4 0.0370 0.0772 0.4793
5 −0.0218 0.0772 −0.2824
6 0.0191 0.0772 0.2474
Observations 168    
 
Source: BSE Ltd.
If returns on the S&P BSE 100 were an MA(q ) time series, then the first q  auto -
correlations would differ significantly from 0. None of the autocorrelations is 
statistically significant, however, so returns to the S&P BSE 100 appear to come 
from an MA(0) time series. An MA(0) time series in which we allow the mean 
to be nonzero takes the following form:10
    x  t   = μ +  ε  t  ,  E   (   ε  t   )     = 0,  E   (   ε  t  2  )     =  σ   2 ,  
     
cov   (   ε  t  ,  ε  s   )     = E   (   ε  t    ε  s   )     = 0 for t  ≠ s,    (14)
which means that the time series is not predictable. This result should not be 
surprising, because most research suggests that short-term returns to stock 
indexes are difficult to predict.
We can see from this example how examining the autocorrelations allowed 
us to choose between the AR and MA models. If returns to the S&P BSE 100 had 
come from an AR(1) time series, the first autocorrelation would have differed 
significantly from 0 and the autocorrelations would have declined gradually. 
Not even the first autocorrelation is significantly different from 0, however. 
Therefore, we can be sure that returns to the S&P BSE 100 do not come from 
an AR(1) model—or from any higher-order AR model, for that matter. This 
finding is consistent with our conclusion that the S&P BSE 100 series is MA(0).
SEASONALITY IN TIME-SERIES MODELS
explain how to test and correct for seasonality in a time-series model 
and calculate and interpret a forecasted value using an AR model 
with a seasonal lag
10 On the basis of investment theory and evidence, we expect that the mean monthly return on the S&P 
BSE 100 is positive (μ > 0). We can also generalize Equation 13 for an MA(q ) time series by adding a 
constant term, μ. Including a constant term in a moving-average model does not change the expressions 
for the variance and autocovariances of the time series. A number of early studies of weak-form market 
efficiency used Equation 14 as the model for stock returns. See Garbade (1982).13 Time-Series Analysis 42
As we analyze the results of the time-series models in this reading, we encounter 
complications. One common complication is significant seasonality, a case in which 
the series shows regular patterns of movement within the year. At first glance, sea-
sonality might appear to rule out using autoregressive time-series models. After all, 
autocorrelations will differ by season. This problem can often be solved, however, by 
using seasonal lags in an autoregressive model.
A seasonal lag is usually the value of the time series one year before the current 
period, included as an extra term in an autoregressive model. Suppose, for example, 
that we model a particular quarterly time series using an AR(1) model, xt = b0 + b1xt−1 
+ εt. If the time series had significant seasonality, this model would not be correctly 
specified. The seasonality would be easy to detect because the seasonal autocorrela-
tion (in the case of quarterly data, the fourth autocorrelation) of the error term would 
differ significantly from 0. Suppose this quarterly model has significant seasonality. In 
this case, we might include a seasonal lag in the autoregressive model and estimate
 xt = b0 + b1xt−1 + b2xt−4 + εt    (15)
to test whether including the seasonal lag would eliminate statistically significant 
autocorrelation in the error term.
In Example 15 and Example 16, we illustrate how to test and adjust for seasonality 
in a time-series model. We also illustrate how to compute a forecast using an autore-
gressive model with a seasonal lag.
EXAMPLE 15
Seasonality in Sales at Starbucks
1. Earlier, we concluded that we could not model the log of Starbucks’ quar -
terly sales using only a time-trend line (as shown in Example 3) because the 
Durbin–Watson statistic from the regression provided evidence of positive 
serial correlation in the error term. Based on methods presented in this 
reading, we might next investigate using the first difference of log sales to 
remove an exponential trend from the data to obtain a covariance-stationary 
time series.
Using quarterly data from the last quarter of 2001 to the second quarter of 
2019, we estimate the following AR(1) model using ordinary least squares: 
(ln Salest − ln Salest−1) = b0 + b1(ln Salest−1 − ln Salest−2) + εt. Exhibit 26 
shows the results of the regression.
 
Exhibit 26: Log Differenced Sales: AR(1) Model—Starbucks, 
Quarterly Observations, 2001–2019
 
 
Regression Statistics
R20.2044
Standard error 0.0611Seasonality in Time-Series Models 43
Regression Statistics
Observations 72
Durbin–Watson 1.9904
 
 
  Coefficient Standard Error t-Statistic
Intercept 0.0469 0.0080 5.8625
ln Salest−1 − 
ln Salest−2−0.4533 0.1069 −4.2404
 
 
Autocorrelations of the Residual
Lag Autocorrelation Standard Error t-Statistic
1 0.0051 0.1179 −0.0433
2 −0.1676 0.1179 −1.4218
3 −0.0130 0.1179 −0.1099
4 0.7630 0.1179 6.4720
 
Source: Bloomberg.
The first thing to note in Exhibit 26 is the strong seasonal autocorrelation of 
the residuals. The bottom portion of the table shows that the fourth auto-
correlation has a value of 0.7630 and a t-statistic of 6. With 72 observations 
and two parameters, this model has 70 degrees of freedom.11 The critical 
value for a t-statistic is about 1.99 at the 0.05 significance level. Given this 
value of the t-statistic, we must reject the null hypothesis that the fourth 
autocorrelation is equal to 0 because the t-statistic is larger than the critical 
value of 1.99.
In this model, the fourth autocorrelation is the seasonal autocorrelation 
because this AR(1) model is estimated with quarterly data. Exhibit 26 shows 
the strong and statistically significant seasonal autocorrelation that occurs 
when a time series with strong seasonality is modeled without taking the 
seasonality into account. Therefore, the AR(1) model is misspecified, and we 
should not use it for forecasting.
Suppose we decide to use an autoregressive model with a seasonal lag 
because of the seasonal autocorrelation. We are modeling quarterly data, so 
we estimate Equation 15: (ln Salest − ln Salest−1) = b0 + b1(ln Salest−1 − ln 
Salest−2) + b2(ln Salest−4 − ln Salest−5) + εt. Adding the seasonal difference ln 
Salest−4 − ln Salest−5 is an attempt to remove a consistent quarterly pattern 
in the data and could also eliminate a seasonal nonstationarity if one existed. 
The estimates of this equation appear in Exhibit 27.
11 In this example, we restrict the start of the sample period to the beginning of 2001, and we do not use 
prior observations for the lags. Accordingly, the number of observations decreases with an increase in the 
number of lags. In Exhibit 26, the first observation is for the third quarter of 2001 because we use up to two 
lags. In Exhibit 27, the first observation is for the second quarter of 2002 because we use up to five lags. Time-Series Analysis 44
 
Exhibit 27: Log Differenced Sales: AR(1) Model with Seasonal Lag—
Starbucks, Quarterly Observations, 2005–2019
 
 
Regression Statistics
R20.7032
Standard error 0.0373
Observations 69
Durbin–Watson 2.0392
 
 
  Coefficient Standard Error t-Statistic
Intercept 0.0107 0.0059 1.8136
ln Salest−1 − 
ln Salest−2−0.1540 0.0729 −2.1125
ln Salest−4 − 
ln Salest−50.7549 0.0720 10.4847
 
 
Autocorrelations of the Residual
Lag Autocorrelation Standard Error t-Statistic
1 0.0135 0.1204 0.1121
2 −0.0171 0.1204 −0.1420
3 0.1589 0.1204 1.3198
4 −0.1498 0.1204 −1.2442
 
Source: Compustat.
Note the autocorrelations of the residual shown at the bottom of Exhibit 27. 
None of the t-statistics on the first four autocorrelations is now significant. 
Because the overall regression is highly significant (an F-test, not shown in 
the exhibit, is significant at the 0.01 level), we can take an AR(1) model with 
a seasonal lag as a reasonable working model for Starbucks sales. (A model 
having only a seasonal lag term was investigated and not found to improve 
on this model.)
How can we interpret the coefficients in this model? To predict the cur -
rent quarter’s sales growth at Starbucks, we need to know two things: sales 
growth in the previous quarter and sales growth four quarters ago. If sales 
remained constant in each of those two quarters, the model in Exhibit 27 
would predict that sales will grow by 0.0107 (1.07%) in the current quarter. 
If sales grew by 1% last quarter and by 2% four quarters ago, then the model 
would predict that sales growth this quarter will be 0.0107 − 0.0154(0.01) + 
0.7549(0.02) = 0.0256, or 2.56%. Note that all of these growth rates are expo-
nential growth rates. Notice also that the R2 in the model with the seasonal 
lag (0.7032 in Exhibit 27) was more than three times higher than the R2 in 
the model without the seasonal lag (0.2044 in Exhibit 26). Again, the season-
al lag model does a much better job of explaining the data.Seasonality in Time-Series Models 45
EXAMPLE 16
(Historical Example)
Retail Sales Growth
We want to predict the growth in monthly retail sales of Canadian furniture 
and home furnishing stores so that we can decide whether to recommend the 
shares of these stores. We decide to use non-seasonally adjusted data on retail 
sales. To begin with, we estimate an AR(1) model with observations on the 
annualized monthly growth in retail sales from January 1995 to December 2012. 
We estimate the following equation: Sales growtht = b0 + b1(Sales growtht−1) + 
εt. Exhibit 28 shows the results from this model.
The autocorrelations of the residuals from this model, shown at the bottom of 
Exhibit 28, indicate that seasonality is extremely significant in this model. With 
216 observations and two parameters, this model has 214 degrees of freedom. 
At the 0.05 significance level, the critical value for a t -statistic is about 1.97. The 
12th-lag autocorrelation (the seasonal autocorrelation, because we are using 
monthly data) has a value of 0.7620 and a t -statistic of 11.21. The t -statistic on 
this autocorrelation is larger than the critical value (1.97), implying that we can 
reject the null hypothesis that the 12th autocorrelation is 0. Note also that many 
of the other t-statistics for autocorrelations shown in the table differ significantly 
from 0. Consequently, the model shown in Exhibit 28 is misspecified, so we 
cannot rely on it to forecast sales growth.
Suppose we add the seasonal lag of sales growth (the 12th lag) to the AR(1) 
model to estimate the equation Sales growtht = b0 + b1(Sales growtht−1) + 
b2(Sales growtht−12) + εt. In this example, although we state that the sample 
period begins in 1995, we use prior observations for the lags. This results in 
the same number of observations irrespective of the number of lags. Exhibit 
29 presents the results of estimating this equation. The estimated value of the 
seasonal autocorrelation (the 12th autocorrelation) has fallen to −0.1168. None 
of the first 12 autocorrelations has a t -statistic with an absolute value greater 
than the critical value of 1.97 at the 0.05 significance level. We can conclude that 
there is no significant serial correlation in the residuals from this model. Because 
we can reasonably believe that the model is correctly specified, we can use it to 
predict retail sales growth. Note that the R2 in Exhibit 29 is 0.6724, much larger 
than the R2 in Exhibit 28 (computed by the model without the seasonal lag).
 
Exhibit 28: Monthly Retail Sales Growth of Canadian Furniture and 
Home Furnishing Stores: AR(1) Model, January 1995–December 
2012
 
 
Regression Statistics
R20.0509
Standard error 1.8198 Time-Series Analysis 46
Regression Statistics
Observations 216
Durbin–Watson 2.0956
 
 
  Coefficient Standard Error t-Statistic
Intercept 1.0518 0.1365 7.7055
Sales 
growtht−1−0.2252 0.0665 −3.3865
 
 
Autocorrelations of the Residual
Lag Autocorrelation Standard Error t-Statistic
1 −0.0109 0.0680 −0.1603
2 −0.1949 0.0680 −2.8662
3 0.1173 0.0680 1.7250
4 −0.0756 0.0680 −1.1118
5 −0.1270 0.0680 −1.8676
6 −0.1384 0.0680 −2.0353
7 −0.1374 0.0680 −2.0206
8 −0.0325 0.0680 −0.4779
9 0.1207 0.0680 1.7750
10 −0.2197 0.0680 −3.2309
11 −0.0342 0.0680 −0.5029
12 0.7620 0.0680 11.2059
 
Source: Statistics Canada (Government of Canada).
How can we interpret the coefficients in the model? To predict growth in retail 
sales in this month, we need to know last month’s retail sales growth and retail 
sales growth 12 months ago. If retail sales remained constant both last month 
and 12 months ago, the model in Exhibit 29 would predict that retail sales will 
grow at an annual rate of about 23.7% this month. If retail sales grew at an annual 
rate of 10% last month and at an annual rate of 5% 12 months ago, the model in 
Exhibit 29 would predict that retail sales will grow in the current month at an 
annual rate of 0.2371 − 0.0792(0.10) + 0.7798(0.05) = 0.2682, or 26.8%.
 
Exhibit 29: Monthly Retail Sales Growth of Canadian Furniture and 
Home Furnishing Stores: AR(1) Model with Seasonal Lag, January 
1995–December 2012
 
 
Regression Statistics
R20.6724
Standard error 1.0717AR Moving-Average Models and ARCH Models 47
Regression Statistics
Observations 216
Durbin–Watson 2.1784
 
 
  Coefficient Standard Error t-Statistic
Intercept 0.2371 0.0900 2.6344
Sales 
growtht−1−0.0792 0.0398 −1.9899
Sales 
growtht−120.7798 0.0388 20.0979
 
 
Autocorrelations of the Residual
Lag Autocorrelation Standard Error t-Statistic
1 −0.0770 0.0680 −1.1324
2 −0.0374 0.0680 −0.5500
3 0.0292 0.0680 0.4294
4 −0.0358 0.0680 −0.5265
5 −0.0399 0.0680 −0.5868
6 0.0227 0.0680 0.3338
7 −0.0967 0.0680 −1.4221
8 0.1241 0.0680 1.8250
9 0.0499 0.0680 0.7338
10 −0.0631 0.0680 −0.9279
11 0.0231 0.0680 0.3397
12 −0.1168 0.0680 −1.7176
 
Source: Statistics Canada (Government of Canada).
AR MOVING-AVERAGE MODELS AND ARCH MODELS
explain autoregressive conditional heteroskedasticity (ARCH) and 
describe how ARCH models can be applied to predict the variance of 
a time series
So far, we have presented autoregressive and moving-average models as alternatives 
for modeling a time series. The time series we have considered in examples have usu-
ally been explained quite well with a simple autoregressive model (with or without 
seasonal lags).12 Some statisticians, however, have advocated using a more general 
model, the autoregressive moving-average (ARMA) model. The advocates of ARMA 
models argue that these models may fit the data better and provide better forecasts 
12 For the returns on the S&P BSE 100 (see Example 14), we chose a moving-average model over an 
autoregressive model.14 Time-Series Analysis 48
than do plain autoregressive (AR) models. However, as we discuss later in this section, 
there are severe limitations to estimating and using these models. Because you may 
encounter ARMA models, we next provide a brief overview.
An ARMA model combines both autoregressive lags of the dependent variable and 
moving-average errors. The equation for such a model with p  autoregressive terms 
and q moving-average terms, denoted ARMA(p , q), is
    x  t   =  b  0   +  b  1    x  t−1   + ⋯ +  b  p    x  t−p   +  ε  t   +  θ  1    ε  t−1   + ⋯ +  θ  q    ε  t−q  ,
      E   (   ε  t   )     = 0,  E   (   ε  t  2  )     =  σ   2 ,  cov   (   ε  t  ,  ε  s   )     = E   (   ε  t    ε  s   )     = 0 for t  ≠ s,   (16)
where b1, b2, . . . , bp are the autoregressive parameters and θ1, θ2, . . . , θq are the 
moving-average parameters.
Estimating and using ARMA models has several limitations. First, the parameters 
in ARMA models can be very unstable. In particular, slight changes in the data sample 
or the initial guesses for the values of the ARMA parameters can result in very different 
final estimates of the ARMA parameters. Second, choosing the right ARMA model 
is more of an art than a science. The criteria for deciding on p  and q  for a particular 
time series are far from perfect. Moreover, even after a model is selected, that model 
may not forecast well.
To reiterate, ARMA models can be very unstable, depending on the data sample 
used and the particular ARMA model estimated. Therefore, you should be skeptical 
of claims that a particular ARMA model provides much better forecasts of a time 
series than any other ARMA model. In fact, in most cases, you can use an AR model 
to produce forecasts that are just as accurate as those from ARMA models without 
nearly as much complexity. Even some of the strongest advocates of ARMA models 
admit that these models should not be used with fewer than 80 observations, and 
they do not recommend using ARMA models for predicting quarterly sales or gross 
margins for a company using even 15 years of quarterly data.
Autoregressive Conditional Heteroskedasticity Models
Up to now, we have ignored any issues of heteroskedasticity in time-series models and 
have assumed homoskedasticity. Heteroskedasticity is the dependence of the error 
term variance on the independent variable; homoskedasticity is the independence 
of the error term variance from the independent variable. We have assumed that the 
error term’s variance is constant and does not depend on the value of the time series 
itself or on the size of previous errors. At times, however, this assumption is violated 
and the variance of the error term is not constant. In such a situation, the standard 
errors of the regression coefficients in AR, MA, or ARMA models will be incorrect, 
and our hypothesis tests would be invalid. Consequently, we can make poor investment 
decisions based on those tests.
For example, suppose you are building an autoregressive model of a company’s sales. 
If heteroskedasticity is present, then the standard errors of the regression coefficients 
of your model will be incorrect. It is likely that because of heteroskedasticity, one or 
more of the lagged sales terms may appear statistically significant when in fact they 
are not. Therefore, if you use this model for your decision making, you may make 
some suboptimal decisions.
In work responsible in part for his shared 2003 Nobel Prize in Economics, Robert 
F. Engle in 1982 first suggested a way of testing whether the variance of the error in 
a particular time-series model in one period depends on the variance of the error in 
previous periods. He called this type of heteroskedasticity “autoregressive conditional 
heteroskedasticity” (ARCH).
As an example, consider the ARCH(1) model
 εt ~  N   (  0,  a  0   +  a  1    ε  t−1  2   )    ,  (17)AR Moving-Average Models and ARCH Models 49
where the distribution of εt, conditional on its value in the previous period, εt−1, 
is normal, with mean 0 and variance   a  0   +  a  1    ε  t−1  2   . If a1 = 0, the variance of the error 
in every period is just a0. The variance is constant over time and does not depend on 
past errors. Now suppose that a1 > 0. Then the variance of the error in one period 
depends on how large the squared error was in the previous period. If a large error 
occurs in one period, the variance of the error in the next period will be even larger.
Engle showed that we can test whether a time series is ARCH(1) by regressing 
the squared residuals from a previously estimated time-series model (AR, MA, or 
ARMA) on a constant and one lag of the squared residuals. We can estimate the 
linear regression equation
     ˆ ε    t  2  =  a  0   +  a  1      ˆ ε    t−1  2   +  u  t  ,  (18)
where ut is an error term. If the estimate of a1 is statistically significantly different 
from zero, we conclude that the time series is ARCH(1). If a time-series model has 
ARCH(1) errors, then the variance of the errors in period t  + 1 can be predicted in 
period t using the formula     ˆ σ    t+1  2   =    ˆ a    0   +    ˆ a    1      ˆ ε    t  2  .
EXAMPLE 17
Testing for ARCH(1) in Monthly Inflation
Analyst Lisette Miller wants to test whether monthly data on CPI inflation con -
tain autoregressive conditional heteroskedasticity. She could estimate Equation 
18 using the residuals from the time-series model. Based on the analyses in 
Examples 6 through 9 , she has concluded that if she modeled monthly CPI 
inflation from 1995 to 2018, there would not be much difference in the perfor -
mance of AR(1) and AR(2) models in forecasting inflation. The analyst looked 
at the AR(1) model for 2008–2018, found it sufficient, and decided to further 
explore the AR(1) model for the entire period, 1995–2018. She decides to further 
explore the AR(1) model for the entire period 1995 to 2018. Exhibit 30 shows 
the results of testing whether the errors in that model are ARCH(1). Because 
the test involves the first lag of residuals of the estimated time-series model, the 
number of observations in the test is one less than that in the model.
The t-statistic for the coefficient on the previous period’s squared residu-
als is greater than 4.8. Therefore, Miller easily rejects the null hypothesis that 
the variance of the error does not depend on the variance of previous errors. 
Consequently, the test statistics she computed in Exhibits 6 through 9 are not 
valid, and she should not use them in deciding her investment strategy.
 
Exhibit 30: Test for ARCH(1) in an AR(1) Model: Residuals from 
Monthly CPI Inflation at an Annual Rate, March 1995–December 
2018
 
 
Regression Statistics
R20.0759
Standard error 23.7841 Time-Series Analysis 50
Regression Statistics
Observations 286
Durbin–Watson 2.0569
 
 
  Coefficient Standard Error t-Statistic
Intercept 6.3626 1.4928 4.2622
    ˆ ε    t−1  2   0.2754 0.0570 4.8316
 
Source: US Bureau of Labor Statistics.
It is possible Miller’s conclusion—that the AR(1) model for monthly inflation has 
ARCH in the errors—may have been due to the sample period used (1995–2018). 
In Example 9, she used a shorter sample period, 2008–2018, and concluded that 
monthly CPI inflation follows an AR(1) process. (These results were shown in 
Exhibit 16.) Exhibit 30 shows that errors for a time-series model of inflation for 
the entire sample (1995–2018) have ARCH errors. Do the errors estimated with 
a shorter sample period (2008–2018) also display ARCH? For the shorter sample 
period, Miller estimated an AR(1) model using monthly inflation data. Now 
she tests to see whether the errors display ARCH. Exhibit 31 shows the results.
In this sample, the coefficient on the previous period’s squared residual has 
a t-statistic of 4.0229. Consequently, Miller rejects the null hypothesis that the 
errors in this regression have no autoregressive conditional heteroskedasticity. 
The error variance appears to be heteroskedastic, and Miller cannot rely on the 
t-statistics.
 
Exhibit 31: Test for ARCH(1) in an AR(1) Model: Monthly CPI 
Inflation at an Annual Rate, February 2008–December 2018
 
 
Regression Statistics
R20.1113
Standard error 24.64
Observations 131
Durbin–Watson 2.0385
 
 
  Coefficient Standard Error t-Statistic
Intercept 6.2082 2.2873 2.7142
    ˆ ε    t−1  2   0.3336 0.0830 4.0229
 
Source: US Bureau of Labor Statistics.
 
Suppose a model contains ARCH(1) errors. What are the consequences of that fact? 
First, if ARCH exists, the standard errors for the regression parameters will not be 
correct. We will need to use generalized least squares13 or other methods that correct 
for heteroskedasticity to correctly estimate the standard error of the parameters in the 
time-series model. Second, if ARCH exists and we have it modeled—for example, as 
ARCH(1)—we can predict the variance of the errors. Suppose, for instance, that we 
want to predict the variance of the error in inflation using the estimated parameters 
13 See Greene (2018).Regressions with More Than One Time Series 51
from Exhibit 30:     ˆ σ    t  2  = 6.3626 + 0.2754    ˆ ε    t−1  2   . If the error in one period were 0%, the 
predicted variance of the error in the next period would be 6.3626 + 0.2754(0) = 
6.3626. If the error in one period were 1%, the predicted variance of the error in the 
next period would be 6.3626 + 0.2754(12) = 6.6380.
Engle and other researchers have suggested many generalizations of the ARCH(1) 
model, including ARCH( p) and generalized autoregressive conditional heteroskedas -
ticity (GARCH) models. In an ARCH(p ) model, the variance of the error term in the 
current period depends linearly on the squared errors from the previous p  periods:   
σ  t  2  =  a  0   +  a  1    ε  t−1  2   + ⋯ +  a  p    ε  t−p  2   . GARCH models are similar to ARMA models of 
the error variance in a time series. Just like ARMA models, GARCH models can be 
finicky and unstable: Their results can depend greatly on the sample period and the 
initial guesses of the parameters in the GARCH model. Financial analysts who use 
GARCH models should be well aware of how delicate these models can be, and they 
should examine whether GARCH estimates are robust to changes in the sample and 
the initial guesses about the parameters.14
REGRESSIONS WITH MORE THAN ONE TIME SERIES
explain how time-series variables should be analyzed for 
nonstationarity and/or cointegration before use in a linear regression
Up to now, we have discussed time-series models only for one time series. Although in 
the readings on correlation and regression and on multiple regression we used linear 
regression to analyze the relationship among different time series, in those readings we 
completely ignored unit roots. A time series that contains a unit root is not covariance 
stationary. If any time series in a linear regression contains a unit root, ordinary least 
squares estimates of regression test statistics may be invalid.
To determine whether we can use linear regression to model more than one time 
series, let us start with a single independent variable; that is, there are two time series, 
one corresponding to the dependent variable and one corresponding to the indepen-
dent variable. We will then extend our discussion to multiple independent variables.
We first use a unit root test, such as the Dickey–Fuller test, for each of the two 
time series to determine whether either of them has a unit root.15 There are several 
possible scenarios related to the outcome of these tests. One possible scenario is that 
we find that neither of the time series has a unit root. Then we can safely use linear 
regression to test the relations between the two time series. Otherwise, we may have 
to use additional tests, as we discuss later in this section.
EXAMPLE 18
Unit Roots and the Fisher Effect
Researchers at an asset management firm examined the Fisher effect by estimating 
the regression relation between expected inflation and US Treasury bill (T-bill) 
returns. They used 181 quarterly observations on expected inflation rates and 
T-bill returns from the sample period extending from the fourth quarter of 1968 
through the fourth quarter of 2013. They used linear regression to analyze the 
14 For more on ARCH, GARCH, and other models of time-series variance, see Hamilton (1994).
15 For theoretical details of unit root tests, see Greene (2018) or Tsay (2010). Unit root tests are available 
in some econometric software packages, such as EViews.15 Time-Series Analysis 52
relationship between the two time series. The results of this regression would 
be valid if both time series are covariance stationary; that is, neither of the two 
time series has a unit root. So, if they compute the Dickey–Fuller t -test statistic 
of the hypothesis of a unit root separately for each time series and find that they 
can reject the null hypothesis that the T-bill return series has a unit root and the 
null hypothesis that the expected inflation time series has a unit root, then they 
can use linear regression to analyze the relation between the two series. In that 
case, the results of their analysis of the Fisher effect would be valid.
A second possible scenario is that we reject the hypothesis of a unit root for the 
independent variable but fail to reject the hypothesis of a unit root for the dependent 
variable. In this case, the error term in the regression would not be covariance station -
ary. Therefore, one or more of the following linear regression assumptions would be 
violated: (1) that the expected value of the error term is 0, (2) that the variance of the 
error term is constant for all observations, and (3) that the error term is uncorrelated 
across observations. Consequently, the estimated regression coefficients and standard 
errors would be inconsistent. The regression coefficients might appear significant, but 
those results would be spurious.16 Thus we should not use linear regression to analyze 
the relation between the two time series in this scenario.
A third possible scenario is the reverse of the second scenario: We reject the 
hypothesis of a unit root for the dependent variable but fail to reject the hypothesis 
of a unit root for the independent variable. In this case also, like the second scenario, 
the error term in the regression would not be covariance stationary, and we cannot 
use linear regression to analyze the relation between the two time series.
EXAMPLE 19
(Historical Example)
Unit Roots and Predictability of Stock Market Returns by 
Price-to-Earnings Ratio
Johann de Vries is analyzing the performance of the South African stock market. 
He examines whether the percentage change in the Johannesburg Stock Exchange 
(JSE) All Share Index can be predicted by the price-to-earnings ratio (P/E) for 
the index. Using monthly data from January 1994 to December 2013, he runs 
a regression using (Pt − Pt−1)/Pt−1 as the dependent variable and Pt−1/Et−2 as 
the independent variable, where Pt is the value of the JSE index at time t  and 
Et is the earnings on the index. De Vries finds that the regression coefficient is 
negative and statistically significant and the value of the R2 for the regression 
is quite high. What additional analysis should he perform before accepting the 
regression as valid?
De Vries needs to perform unit root tests for each of the two time series. 
If one of the two time series has a unit root, implying that it is not stationary, 
the results of the linear regression are not meaningful and cannot be used to 
conclude that stock market returns are predictable by P/E.17
The next possibility is that both time series have a unit root. In this case, we 
need to establish whether the two time series are cointegrated before we can rely 
on regression analysis.18 Two time series are cointegrated if a long-term financial or 
16 The problem of spurious regression for nonstationary time series was first discussed by Granger and 
Newbold (1974).
17 Barr and Kantor (1999) contains evidence that the P/E time series is nonstationary.
18 Engle and Granger (1987) first discussed cointegration.Regressions with More Than One Time Series 53
economic relationship exists between them such that they do not diverge from each 
other without bound in the long run. For example, two time series are cointegrated 
if they share a common trend.
In the fourth scenario, both time series have a unit root but are not cointegrated. 
In this scenario, as in the second and third scenarios, the error term in the linear 
regression will not be covariance stationary, some regression assumptions will be 
violated, the regression coefficients and standard errors will not be consistent, and we 
cannot use them for hypothesis tests. Consequently, linear regression of one variable 
on the other would be meaningless.
Finally, the fifth possible scenario is that both time series have a unit root but they 
are cointegrated. In this case, the error term in the linear regression of one time series 
on the other will be covariance stationary. Accordingly, the regression coefficients and 
standard errors will be consistent, and we can use them for hypothesis tests. However, 
we should be very cautious in interpreting the results of a regression with cointegrated 
variables. The cointegrated regression estimates the long-term relation between the 
two series but may not be the best model of the short-term relation between the two 
series. Short-term models of cointegrated series (error correction models) are dis -
cussed in Engle and Granger (1987) and Tsay (2010), but these are specialist topics.
Now let us look at how we can test for cointegration between two time series 
that each have a unit root, as in the fourth and fifth scenarios.19 Engle and Granger 
suggested the following test. If yt and xt are both time series with a unit root, we 
should do the following:
1. Estimate the regression yt = b0 + b1xt + εt.
2. Test whether the error term from the regression in Step 1 has a unit root 
using a Dickey–Fuller test. Because the residuals are based on the estimated 
coefficients of the regression, we cannot use the standard critical values for 
the Dickey–Fuller test. Instead, we must use the critical values computed by 
Engle and Granger, which take into account the effect of uncertainty about 
the regression parameters on the distribution of the Dickey–Fuller test.
3. If the (Engle–Granger) Dickey–Fuller test fails to reject the null hypothesis 
that the error term has a unit root, then we conclude that the error term in 
the regression is not covariance stationary. Therefore, the two time series 
are not cointegrated. In this case, any regression relation between the two 
series is spurious.
4. If the (Engle–Granger) Dickey–Fuller test rejects the null hypothesis that 
the error term has a unit root, then we may assume that the error term 
in the regression is covariance stationary and that the two time series are 
cointegrated. The parameters and standard errors from linear regression will 
be consistent and will let us test hypotheses about the long-term relation 
between the two series.
19 Consider a time series, xt, that has a unit root. For many such financial and economic time series, the 
first difference of the series, xt − xt−1, is stationary. We say that such a series, whose first difference is sta-
tionary, has a single unit root. However, for some time series, even the first difference may not be stationary 
and further differencing may be needed to achieve stationarity. Such a time series is said to have multiple  
unit roots. In this section, we consider only the case in which each nonstationary series has a single unit 
root (which is quite common). Time-Series Analysis 54
EXAMPLE 20
Testing for Cointegration between Intel Sales and 
Nominal GDP
Suppose we want to test whether the natural log of Intel’s sales and the natural log 
of GDP are cointegrated (that is, whether there is a long-term relation between 
GDP and Intel sales). We want to test this hypothesis using quarterly data from 
the first quarter of 1995 through the fourth quarter of 2019. Here are the steps:
1. Test whether the two series each have a unit root. If we cannot reject 
the null hypothesis of a unit root for both series, implying that both 
series are nonstationary, we must then test whether the two series are 
cointegrated.
2. Having established that each series has a unit root, we estimate the 
regression ln Intel salest = b0 + b1(ln GDPt) + εt, then conduct the 
(Engle–Granger) Dickey–Fuller test of the hypothesis that there is a 
unit root in the error term of this regression using the residuals from 
the estimated regression. If we reject the null hypothesis of a unit root 
in the error term of the regression, we reject the null hypothesis of 
no cointegration. That is, the two series would be cointegrated. If the 
two series are cointegrated, we can use linear regression to estimate 
the long-term relation between the natural log of Intel sales and the 
natural log of GDP .
We have so far discussed models with a single independent variable. We now 
extend the discussion to a model with two or more independent variables, so that 
there are three or more time series. The simplest possibility is that none of the time 
series in the model has a unit root. Then, we can safely use multiple regression to test 
the relation among the time series.
EXAMPLE 21
Unit Roots and Returns to the Fidelity Select Technology 
Fund
In earlier coverage of multiple regression, we used a multiple linear regression 
model to examine whether returns to either the S&P 500 Growth Index or the 
S&P 500 Value Index explain returns to the Fidelity Select Technology Portfolio 
using monthly observations between October 2015 and August 2019. Of course, 
if any of the three time series has a unit root, then the results of our regression 
analysis may be invalid. Therefore, we could use a Dickey–Fuller test to deter -
mine whether any of these series has a unit root.
If we reject the hypothesis of unit roots for all three series, we can use linear 
regression to analyze the relation among the series. In that case, the results of 
our analysis of the factors affecting returns to the Fidelity Select Technology 
Portfolio would be valid.
If at least one time series (the dependent variable or one of the independent vari-
ables) has a unit root while at least one time series (the dependent variable or one 
of the independent variables) does not, the error term in the regression cannot be 
covariance stationary. Consequently, we should not use multiple linear regression to 
analyze the relation among the time series in this scenario.Other Issues in Time Series 55
Another possibility is that each time series, including the dependent variable 
and each of the independent variables, has a unit root. If this is the case, we need 
to establish whether the time series are cointegrated. To test for cointegration, the 
procedure is similar to that for a model with a single independent variable. First, 
estimate the regression yt = b0 + b1x1t + b2x2t + . . . + bkxkt + εt. Then conduct the 
(Engle–Granger) Dickey–Fuller test of the hypothesis that there is a unit root in the 
errors of this regression using the residuals from the estimated regression.
If we cannot reject the null hypothesis of a unit root in the error term of the regres -
sion, we cannot reject the null hypothesis of no cointegration. In this scenario, the 
error term in the multiple regression will not be covariance stationary, so we cannot 
use multiple regression to analyze the relationship among the time series.
If we can reject the null hypothesis of a unit root in the error term of the regression, 
we can reject the null hypothesis of no cointegration. However, modeling three or more 
time series that are cointegrated may be difficult. For example, an analyst may want 
to predict a retirement services company’s sales based on the country’s GDP and the 
total population over age 65. Although the company’s sales, GDP , and the population 
over 65 may each have a unit root and be cointegrated, modeling the cointegration 
of the three series may be difficult, and doing so is beyond the scope of this volume. 
Analysts who have not mastered all these complex issues should avoid forecasting 
models with multiple time series that have unit roots; the regression coefficients may 
be inconsistent and may produce incorrect forecasts.
OTHER ISSUES IN TIME SERIES
determine an appropriate time-series model to analyze a given 
investment problem and justify that choice
Time-series analysis is an extensive topic and includes many highly complex issues. 
Our objective in this reading has been to present those issues in time series that are 
the most important for financial analysts and can also be handled with relative ease. 
In this section, we briefly discuss some of the issues that we have not covered but 
could be useful for analysts.
In this reading, we have shown how to use time-series models to make forecasts. 
We have also introduced the RMSE as a criterion for comparing forecasting models. 
However, we have not discussed measuring the uncertainty associated with forecasts 
made using time-series models. The uncertainty of these forecasts can be very large, 
and should be taken into account when making investment decisions. Fortunately, the 
same techniques apply to evaluating the uncertainty of time-series forecasts as apply 
to evaluating the uncertainty about forecasts from linear regression models. To accu -
rately evaluate forecast uncertainty, we need to consider both the uncertainty about 
the error term and the uncertainty about the estimated parameters in the time-series 
model. Evaluating this uncertainty is fairly complicated when using regressions with 
more than one independent variable.
In this reading, we used the US CPI inflation series to illustrate some of the prac -
tical challenges analysts face in using time-series models. We used information on US 
Federal Reserve policy to explore the consequences of splitting the inflation series in 
two. In financial time-series work, we may suspect that a time series has more than 
one regime but lack the information to attempt to sort the data into different regimes. 16 Time-Series Analysis 56
If you face such a problem, you may want to investigate other methods, especially 
switching regression models, to identify multiple regimes using only the time series 
itself.
If you are interested in these and other advanced time-series topics, you can learn 
more from Diebold (2008) and Tsay (2010).
Suggested Steps in Time-Series Forecasting
The following is a step-by-step guide to building a model to predict a time series.
1. Understand the investment problem you have, and make an initial choice 
of model. One alternative is a regression model that predicts the future 
behavior of a variable based on hypothesized causal relationships with other 
variables. Another is a time-series model that attempts to predict the future 
behavior of a variable based on the past behavior of the same variable.
2. If you have decided to use a time-series model, compile the time series and 
plot it to see whether it looks covariance stationary. The plot might show 
important deviations from covariance stationarity, including the following:
 ●a linear trend,
 ●an exponential trend,
 ●seasonality, or
 ●a significant shift in the time series during the sample period (for exam-
ple, a change in mean or variance).
3. If you find no significant seasonality or shift in the time series, then perhaps 
either a linear trend or an exponential trend will be sufficient to model the 
time series. In that case, take the following steps:
 ●Determine whether a linear or exponential trend seems most reasonable 
(usually by plotting the series).
 ●Estimate the trend.
 ●Compute the residuals.
 ●Use the Durbin–Watson statistic to determine whether the residuals 
have significant serial correlation. If you find no significant serial correla-
tion in the residuals, then the trend model is sufficient to capture the 
dynamics of the time series and you can use that model for forecasting.
4. If you find significant serial correlation in the residuals from the trend 
model, use a more complex model, such as an autoregressive model. First, 
however, reexamine whether the time series is covariance stationary. The 
following is a list of violations of stationarity, along with potential methods 
to adjust the time series to make it covariance stationary:
 ●If the time series has a linear trend, first-difference the time series.
 ●If the time series has an exponential trend, take the natural log of the 
time series and then first-difference it.
 ●If the time series shifts significantly during the sample period, estimate 
different time-series models before and after the shift.
 ●If the time series has significant seasonality, include seasonal lags (dis -
cussed in Step 7).Other Issues in Time Series 57
5. After you have successfully transformed a raw time series into a covari-
ance-stationary time series, you can usually model the transformed series 
with a short autoregression.20 To decide which autoregressive model to use, 
take the following steps:
 ●Estimate an AR(1) model.
 ●Test to see whether the residuals from this model have significant serial 
correlation.
 ●If you find no significant serial correlation in the residuals, you can use 
the AR(1) model to forecast.
6. If you find significant serial correlation in the residuals, use an AR(2) model 
and test for significant serial correlation of the residuals of the AR(2) model.
 ●If you find no significant serial correlation, use the AR(2) model.
 ●If you find significant serial correlation of the residuals, keep increas -
ing the order of the AR model until the residual serial correlation is no 
longer significant.
7. Your next move is to check for seasonality. You can use one of two 
approaches:
 ●Graph the data and check for regular seasonal patterns.
 ●Examine the data to see whether the seasonal autocorrelations of the 
residuals from an AR model are significant (for example, the fourth auto-
correlation for quarterly data) and whether the autocorrelations before 
and after the seasonal autocorrelations are significant. To correct for 
seasonality, add seasonal lags to your AR model. For example, if you are 
using quarterly data, you might add the fourth lag of a time series as an 
additional variable in an AR(1) or an AR(2) model.
8. Next, test whether the residuals have autoregressive conditional heteroske-
dasticity. To test for ARCH(1), for example, do the following:
 ●Regress the squared residual from your time-series model on a lagged 
value of the squared residual.
 ●Test whether the coefficient on the squared lagged residual differs sig-
nificantly from 0.
 ●If the coefficient on the squared lagged residual does not differ signifi-
cantly from 0, the residuals do not display ARCH and you can rely on 
the standard errors from your time-series estimates.
 ●If the coefficient on the squared lagged residual does differ significantly 
from 0, use generalized least squares or other methods to correct for 
ARCH.
9. Finally, you may also want to perform tests of the model’s out-of-sample 
forecasting performance to see how the model’s out-of-sample performance 
compares to its in-sample performance.
20 Most financial time series can be modeled using an autoregressive process. For a few time series, a 
moving-average model may fit better. To see whether this is the case, examine the first five or six autocor -
relations of the time series. If the autocorrelations suddenly drop to 0 after the first q  autocorrelations, a 
moving-average model (of order q ) is appropriate. If the autocorrelations start large and decline gradually, 
an autoregressive model is appropriate. Time-Series Analysis 58
Using these steps in sequence, you can be reasonably sure that your model is 
correctly specified.
SUMMARY
 ■The predicted trend value of a time series in period t  is     ˆ b    0   +    ˆ b    1   t in a linear 
trend model; the predicted trend value of a time series in a log-linear trend 
model is   e      ˆ b    0  +   ˆ b    1  t  .
 ■Time series that tend to grow by a constant amount from period to period 
should be modeled by linear trend models, whereas time series that tend to 
grow at a constant rate should be modeled by log-linear trend models.
 ■Trend models often do not completely capture the behavior of a time series, 
as indicated by serial correlation of the error term. If the Durbin–Watson 
statistic from a trend model differs significantly from 2, indicating serial 
correlation, we need to build a different kind of model.
 ■An autoregressive model of order p, denoted AR(p ), uses p  lags of a time 
series to predict its current value: xt = b0 + b1xt−1 + b2xt−2 + . . . + bpxt–p + 
εt.
 ■A time series is covariance stationary if the following three conditions are 
satisfied: First, the expected value of the time series must be constant and 
finite in all periods. Second, the variance of the time series must be con-
stant and finite in all periods. Third, the covariance of the time series with 
itself for a fixed number of periods in the past or future must be constant 
and finite in all periods. Inspection of a nonstationary time-series plot may 
reveal an upward or downward trend (nonconstant mean) and/or noncon-
stant variance. The use of linear regression to estimate an autoregressive 
time-series model is not valid unless the time series is covariance stationary.
 ■For a specific autoregressive model to be a good fit to the data, the autocor -
relations of the error term should be 0 at all lags.
 ■A time series is mean reverting if it tends to fall when its level is above its 
long-run mean and rise when its level is below its long-run mean. If a time 
series is covariance stationary, then it will be mean reverting.
 ■The one-period-ahead forecast of a variable xt from an AR(1) model made 
in period t  for period t  + 1 is     ˆ x    t+1   =    ˆ b    0   +    ˆ b    1    x  t   . This forecast can be used 
to create the two-period-ahead forecast from the model made in period 
t,     ˆ x    t+2   =    ˆ b    0   +    ˆ b    1    x  t+1   . Similar results hold for AR(p ) models.
 ■In-sample forecasts are the in-sample predicted values from the estimated 
time-series model. Out-of-sample forecasts are the forecasts made from the 
estimated time-series model for a time period different from the one for 
which the model was estimated. Out-of-sample forecasts are usually more 
valuable in evaluating the forecasting performance of a time-series model 
than are in-sample forecasts. The root mean squared error (RMSE), defined 
as the square root of the average squared forecast error, is a criterion for 
comparing the forecast accuracy of different time-series models; a smaller 
RMSE implies greater forecast accuracy.
 ■Just as in regression models, the coefficients in time-series models are often 
unstable across different sample periods. In selecting a sample period for 
estimating a time-series model, we should seek to assure ourselves that the 
time series was stationary in the sample period.Other Issues in Time Series 59
 ■A random walk is a time series in which the value of the series in one period 
is the value of the series in the previous period plus an unpredictable ran-
dom error. If the time series is a random walk, it is not covariance station-
ary. A random walk with drift is a random walk with a nonzero intercept 
term. All random walks have unit roots. If a time series has a unit root, then 
it will not be covariance stationary.
 ■If a time series has a unit root, we can sometimes transform the time 
series into one that is covariance stationary by first-differencing the time 
series; we may then be able to estimate an autoregressive model for the 
first-differenced series.
 ■An n-period moving average of the current and past (n  − 1) values of a time 
series, xt, is calculated as [xt + xt−1 + . . . + xt−(n−1)]/n.
 ■A moving-average model of order q , denoted MA(q ), uses q  lags of a ran-
dom error term to predict its current value.
 ■The order q  of a moving-average model can be determined using the fact 
that if a time series is a moving-average time series of order q,  its first q 
autocorrelations are nonzero while autocorrelations beyond the first q  are 
zero.
 ■The autocorrelations of most autoregressive time series start large and 
decline gradually, whereas the autocorrelations of an MA(q ) time series sud-
denly drop to 0 after the first q  autocorrelations. This helps in distinguishing 
between autoregressive and moving-average time series.
 ■If the error term of a time-series model shows significant serial correlation 
at seasonal lags, the time series has significant seasonality. This season-
ality can often be modeled by including a seasonal lag in the model, such 
as adding a term lagged four quarters to an AR(1) model on quarterly 
observations.
 ■The forecast made in time t  for time t  + 1 using a quarterly AR(1) model 
with a seasonal lag would be   x  t+1   =    ˆ b    0   +    ˆ b    1    x  t   +    ˆ b    2    x  t−3   .
 ■ARMA models have several limitations: The parameters in ARMA models 
can be very unstable; determining the AR and MA order of the model can 
be difficult; and even with their additional complexity, ARMA models may 
not forecast well.
 ■The variance of the error in a time-series model sometimes depends on 
the variance of previous errors, representing autoregressive conditional 
heteroskedasticity (ARCH). Analysts can test for first-order ARCH in a 
time-series model by regressing the squared residual on the squared residual 
from the previous period. If the coefficient on the squared residual is statis -
tically significant, the time-series model has ARCH(1) errors.
 ■If a time-series model has ARCH(1) errors, then the variance of the 
errors in period t  + 1 can be predicted in period t  using the for -
mula     ˆ σ    t+1  2   =    ˆ a    0   +    ˆ a    1      ˆ ε    t  2  .
 ■If linear regression is used to model the relationship between two time 
series, a test should be performed to determine whether either time series 
has a unit root:
 ●If neither of the time series has a unit root, then we can safely use linear 
regression.
 ●If one of the two time series has a unit root, then we should not use 
linear regression. Time-Series Analysis 60
 ●If both time series have a unit root and the time series are cointegrated, 
we may safely use linear regression; however, if they are not cointegrated, 
we should not use linear regression. The (Engle–Granger) Dickey–Fuller 
test can be used to determine whether time series are cointegrated.References 61
REFERENCES
Barr, G. D. I., B. S. Kantor. 1999. “Price–Earnings Ratios on the Johannesburg Stock Exchange—
Are They a Good Value?” SA Journal of Accounting Research 13 (1): 1–23.
Dickey, David A., Wayne A. Fuller. 1979. “Distribution of the Estimators for Autoregressive 
Time Series with a Unit Root. ” Journal of the American Statistical Association 74 (366): 
427–31. 10.2307/2286348
Diebold, Francis X. 2008. Elements of Forecasting, 4th ed. Cincinnati: South-Western.
Engle, Robert F., Clive W. J. Granger. 1987. “Co-Integration and Error Correction: Representation, 
Estimation, and Testing. ” Econometrica 55 (2): 251–76. 10.2307/1913236
Garbade, Kenneth. 1982. Securities Markets. New York: McGraw-Hill.
Granger, Clive W. J., Paul Newbold. 1974. “Spurious Regressions in Econometrics. ” Journal of 
Econometrics 2 (2): 111–20. 10.1016/0304-4076(74)90034-7
Hamilton, James D. 1994. Time Series Analysis. Princeton, NJ: Princeton University Press.
Tsay, Ruey S. 2010. Analysis of Financial Time Series, 3rd ed. New York: Wiley. Time-Series Analysis 62
PRACTICE PROBLEMS
The following information relates to questions 
1-7
Angela Martinez, an energy sector analyst at an investment bank, is concerned 
about the future level of oil prices and how it might affect portfolio values. She is 
considering whether to recommend a hedge for the bank portfolio’s exposure to 
changes in oil prices. Martinez examines West Texas Intermediate (WTI) month-
ly crude oil price data, expressed in US dollars per barrel, for the 181-month 
period from August 2000 through August 2015. The end-of-month WTI oil price 
was $51.16 in July 2015 and $42.86 in August 2015 (Month 181).
After reviewing the time-series data, Martinez determines that the mean and 
variance of the time series of oil prices are not constant over time. She then runs 
the following four regressions using the WTI time-series data.
 ■Linear trend model: Oil pricet = b0 + b1t + et.
 ■Log-linear trend model: ln Oil pricet = b0 + b1t + et.
 ■AR(1) model: Oil pricet = b0 + b1Oil pricet−1 + et.
 ■AR(2) model: Oil pricet= b0 + b1Oil pricet−1 + b2Oil pricet−2 + et.
Exhibit 1 presents selected data from all four regressions, and Exhibit 2 presents 
selected autocorrelation data from the AR(1) models.
Exhibit 1: Crude Oil Price per Barrel, August 2000–August 2015 
 Regression Statistics  
(t-statistics for coefficients are reported in parentheses)
  Linear Log-Linear AR(1) AR(2)
R20.5703 0.6255 0.9583 0.9656
Standard error 18.6327 0.3034 5.7977 5.2799
Observations 181 181 180 179
Durbin–Watson 0.10 0.08 1.16 2.08
RMSE     2.0787 2.0530
Coefficients:        
Intercept 28.3278 3.3929 1.5948 2.0017
  (10.1846) (74.9091) (1.4610) (1.9957)
t (Trend) 0.4086 0.0075    
  (15.4148) (17.2898)    
Oil pricet−1     0.9767 1.3946
      (63.9535) (20.2999)
Oil pricet−2       −0.4249
        (−6.2064)Practice Problems 63
In Exhibit 1, at the 5% significance level, the lower critical value for the Durbin–
Watson test statistic is 1.75 for both the linear and log-linear regressions. 
Exhibit 2: Autocorrelations of the Residual from AR(1) 
Model
Lag Autocorrelation t-Statistic
1 0.4157 5.5768
2 0.2388 3.2045
3 0.0336 0.4512
4 −0.0426 −0.5712
Note: At the 5% significance level, the critical value for a t -statistic is 1.97. 
After reviewing the data and regression results, Martinez draws the following 
conclusions.
Conclusion 1 The time series for WTI oil prices is covariance stationary. 
Conclusion 2 Out-of-sample forecasting using the AR(1) model appears to 
be more accurate than that of the AR(2) model. 
1. Based on Exhibit 1, the predicted WTI oil price for October 2015 using the linear 
trend model is closest to:
A. $29.15.
B. $74.77.
C. $103.10.
2. Based on Exhibit 1, the predicted WTI oil price for September 2015 using the 
log-linear trend model is closest to:
A. $29.75.
B. $29.98.
C. $116.50.
3. Based on the regression output in Exhibit 1, there is evidence of positive serial 
correlation in the errors in:
A. the linear trend model but not the log-linear trend model.
B. both the linear trend model and the log-linear trend model.
C. neither the linear trend model nor the log-linear trend model.
4. Martinez’s Conclusion 1 is:
A. correct.
B. incorrect because the mean and variance of WTI oil prices are not constant 
over time. Time-Series Analysis 64
C. incorrect because the Durbin–Watson statistic of the AR(2) model is greater 
than 1.75.
5. Based on Exhibit 1, the forecasted oil price in September 2015 based on the 
AR(2) model is closest to:
A. $38.03.
B. $40.04.
C. $61.77.
6. Based on the data for the AR(1) model in Exhibits 1 and 2, Martinez can con-
clude that the:
A. residuals are not serially correlated.
B. autocorrelations do not differ significantly from zero.
C. standard error for each of the autocorrelations is 0.0745. 
7. Based on the mean-reverting level implied by the AR(1) model regression output 
in Exhibit 1, the forecasted oil price for September 2015 is most likely to be:
A. less than $42.86.
B. equal to $42.86.
C. greater than $42.86.
8. You have been assigned to analyze automobile manufacturers, and as a first step 
in your analysis, you decide to model monthly sales of lightweight vehicles to 
determine sales growth in that part of the industry. Exhibit 3 gives lightweight 
vehicle monthly sales (annualized) from January 1992 to December 2000.
Exhibit 1: Lightweight Vehicle Sales
11121314151617181920
1993 1994 1995 1996 1997 1998 1999 2000
YearMillions of Units (Annualized) 
1992
Monthly sales in the lightweight vehicle sector, Salest, have been increasing over 
time, but you suspect that the growth rate of monthly sales is relatively con-Practice Problems 65
stant. Write the simplest time-series model for Salest that is consistent with your 
perception.
The following information relates to questions 
9-10
The civilian unemployment rate (UER) is an important component of many eco-
nomic models. Exhibit 1 gives regression statistics from estimating a linear trend 
model of the unemployment rate: UERt = b0 + b1t + εt.
Exhibit 1: Estimating a Linear Trend in the Civilian Unemployment Rate: 
Monthly Observations, January 2013–August 2019
Regression Statistics
R20.9316  
Standard error 0.3227  
Observations 80  
Durbin–Watson 0.1878  
  Coefficient Standard Error t-Statistic
Intercept 7.2237 0.0728 99.1704
Trend −0.0510 0.0016 −32.6136
9. Using the regression output in the previous table, what is the model’s prediction 
of the unemployment rate for July 2013? 
10. How should we interpret the Durbin–Watson (DW) statistic for this regression? 
What does the value of the DW statistic say about the validity of a t-test on the 
coefficient estimates?
11. Exhibit 1 compares the predicted civilian unemployment rate (PRED) with the 
actual civilian unemployment rate (UER) from January 2013 to August 2019. The 
predicted results come from estimating the linear time trend model UERt = b0 + 
b1t + εt. 
What can we conclude about the appropriateness of this model? Time-Series Analysis 66
Exhibit 1: Predicted and Actual Civilian Unemployment Rates
9
8
7
6
5
4
3
2
1
0
Jan/13 Jan/14 Jan/15 Jan/16 Jan/17 Jan/18 Jan/19
12. Exhibit 1 shows a plot of first differences in the log of monthly lightweight vehicle 
sales over the same period as in Problem 11. Has differencing the data made the 
resulting series, Δln(Salest) = ln(Salest) − ln(Salest−1), covariance stationary?
Exhibit 1: Change in Natural Log of Lightweight Vehicle Sales
YearΔ ln (Millions of Units)
(Annualized) 
0.15
0.10
0.05
0.00
–0.05
–0.10
–0.15
–0.20
1992 1993 1994 1995 1996 1997 1998 1999 2000
The following information relates to questions 
13-14
Exhibit 1 shows a plot of the first differences in the civilian unemployment rate 
(UER) between January 2013 and August 2019, ΔUERt = UERt − UERt−1.Practice Problems 67
Exhibit 1: Change in Civilian Unemployment Rate
0.3
0.2
0.1
0
–0.1
–0.2
–0.3
–0.4
–0.5
–0.6
Jan/13 Jan/14 Jan/15 Jan/16 Jan/17 Jan/18 Jan/19
13. Has differencing the data made the new series, ΔUERt, covariance stationary? 
Explain your answer. 
14. Given the graph of the change in the unemployment rate shown in the figure, 
describe the steps we should take to determine the appropriate autoregressive 
time-series model specification for the series ΔUERt.
15. Exhibit 1 gives the regression output of an AR(1) model on first differences in 
the unemployment rate. Describe how to interpret the DW statistic for this 
regression.
Exhibit 1: Estimating an AR(1) Model of Changes in the Civilian 
Unemployment Rate: Monthly Observations, February 2013–August 2019
Regression Statistics
R20.0546  
Standard error 0.1309  
Observations 79  
Durbin–Watson 2.0756  
  Coefficient Standard Error t-Statistic
Intercept −0.0668 0.0158 −4.2278
ΔUERt–1 −0.2320 0.1100 −2.191 Time-Series Analysis 68
The following information relates to questions 
16-17
Using monthly data from January 1992 to December 2000, we estimate the 
following equation for lightweight vehicle sales: Δln(Salest) = 2.7108 + 0.3987Δl-
n(Salest−1) + εt. Exhibit 1 gives sample autocorrelations of the errors from this 
model.
Exhibit 1: Different Order Autocorrelations of Differences in the Logs of 
Vehicle Sales
Lag Autocorrelation Standard Error t-Statistic
1 0.9358 0.0962 9.7247
2 0.8565 0.0962 8.9005
3 0.8083 0.0962 8.4001
4 0.7723 0.0962 8.0257
5 0.7476 0.0962 7.7696
6 0.7326 0.0962 7.6137
7 0.6941 0.0962 7.2138
8 0.6353 0.0962 6.6025
9 0.5867 0.0962 6.0968
10 0.5378 0.0962 5.5892
11 0.4745 0.0962 4.9315
12 0.4217 0.0962 4.3827
16. Use the information in the table to assess the appropriateness of the specification 
given by the equation. 
17. If the residuals from the AR(1) model above violate a regression assumption, how 
would you modify the AR(1) specification?
18. Assume that changes in the civilian unemployment rate are covariance stationary 
and that an AR(1) model is a good description for the time series of changes in 
the unemployment rate. Specifically, we have ΔUERt = −0.0668 − 0.2320ΔUERt−1 
(using the coefficient estimates given in the previous problem). Given this equa-
tion, what is the mean-reverting level to which changes in the unemployment 
rate converge?
The following information relates to questions 
19-21
Suppose the following model describes changes in the civilian unemployment 
rate: ΔUERt = −0.0668 − 0.2320ΔUERt−1. The current change (first difference) 
in the unemployment rate is 0.0300. Assume that the mean-reverting level for 
changes in the unemployment rate is −0.0542.Practice Problems 69
19. What is the best prediction of the next change?
20. What is the prediction of the change following the next change?
21. Explain your answer to Part B in terms of equilibrium.
22. Exhibit 1 gives the actual sales, log of sales, and changes in the log of sales of 
Cisco Systems for the period 1Q 2019 to 4Q 2019.
Exhibit 1
DateActual Sales  
($ Millions) Log of SalesChanges in Log of Sales  
Δln(Salest)
1Q 2019 13,072 9.4782 0.0176
2Q 2019 12,446 9.4292 −0.0491
3Q 2019 12,958 9.4695 0.0403
4Q 2019 13,428 9.5051 0.0356
1Q 2020      
2Q 2020      
Forecast the first- and second-quarter sales of Cisco Systems for 2020 using the 
regression Δln(Salest) = 0.0068 + 0.2633Δln(Salest−1).
The following information relates to questions 
23-24
Exhibit 1 gives the actual change in the log of sales of Cisco Systems from 1Q 
2019 to 4Q 2019, along with the forecasts from the regression model Δln(Salest) 
= 0.0068 + 0.2633Δln(Salest−1) estimated using data from 1Q 2001 to 4Q 2018. 
(Note that the observations after the fourth quarter of 2018 are out of sample.)
Exhibit 1
DateActual Value of Changes in the Log 
of Sales Δln(Salest)Forecast Value of Changes in the 
Log of Sales Δln(Salest)
1Q 2019 0.0176 0.0147
2Q 2019 −0.0491 0.0107
3Q 2019 0.0403 0.0096
4Q 2019 0.0356 0.0093
23. Calculate the RMSE for the out-of-sample forecast errors. 
24. Compare the forecasting performance of the model given with that of another 
model having an out-of-sample RMSE of 2%. Time-Series Analysis 70
The following information relates to questions 
25-26
25. The AR(1) model for the civilian unemployment rate, ΔUERt = −0.0405 − 
0.4674ΔUERt−1, was developed with five years of data. What would be the draw -
back to using the AR(1) model to predict changes in the civilian unemployment 
rate 12 months or more ahead, as compared with 1 month ahead?
26. For purposes of estimating a predictive equation, what would be the drawback to 
using 30 years of civilian unemployment data rather than only 5 years?
The following information relates to questions 
27-35
Max Busse is an analyst in the research department of a large hedge fund. He was 
recently asked to develop a model to predict the future exchange rate between 
two currencies. Busse gathers monthly exchange rate data from the most re-
cent 10-year period and runs a regression based on the following AR(1) model 
specification:
Regression 1: xt = b0 + b1xt−1 + εt, where xt is the exchange rate at time t .
Based on his analysis of the time series and the regression results, Busse reaches 
the following conclusions:
Conclusion 1 The variance of xt increases over time.
Conclusion 2 The mean-reverting level is undefined.
Conclusion 3 b0 does not appear to be significantly different from 0.
Busse decides to do additional analysis by first-differencing the data and running 
a new regression.
Regression 2: yt = b0 + b1yt−1 + εt, where yt = xt − xt−1.
Exhibit 1 shows the regression results.
Exhibit 1: First-Differenced Exchange Rate AR(1) Model: Month-End 
Observations, Last 10 Y ears
Regression Statistics
R2  0.0017  
Standard error   7.3336  Practice Problems 71
Regression Statistics
Observations   118  
Durbin–Watson   1.9937  
  Coefficient Standard Error t-Statistic
Intercept −0.8803 0.6792 −1.2960
xt−1 − xt−2 0.0412 0.0915 0.4504
Autocorrelations of the Residual
Lag Autocorrelation Standard Error t-Statistic
1 0.0028 0.0921 0.0300
2 0.0205 0.0921 0.2223
3 0.0707 0.0921 0.7684
4 0.0485 0.0921 0.5271
Note: The critical t -statistic at the 5% significance level is 1.98. 
Busse decides that he will need to test the data for nonstationarity using a 
Dickey–Fuller test. To do so, he knows he must model a transformed version of 
Regression 1. 
Busse’s next assignment is to develop a model to predict future quarterly sales for 
PoweredUP , Inc., a major electronics retailer. He begins by running the following 
regression:
Regression 3: ln Salest − ln Salest−1 = b0 + b1(ln Salest−1 − ln Salest−2) + εt.
Exhibit 2 presents the results of this regression.
Exhibit 2: Log Differenced Sales AR(1) Model: PoweredUP , Inc., Last 10 Y ears 
of Quarterly Sales
Regression Statistics
R2  0.2011  
Standard error   0.0651  
Observations   38  
Durbin–Watson   1.9677  
  Coefficient Standard Error t-Statistic
Intercept 0.0408 0.0112 3.6406
ln Salest−1 − ln Salest−2 −0.4311 0.1432 −3.0099
Autocorrelations of the Residual
Lag Autocorrelation Standard Error t-Statistic
1 0.0146 0.1622 0.0903
2 −0.1317 0.1622 −0.8119
3 −0.1123 0.1622 −0.6922
4 0.6994 0.1622 4.3111 Time-Series Analysis 72
Note: The critical t -statistic at the 5% significance level is 2.02. 
Because the regression output from Exhibit 2 raises some concerns, Busse runs a 
different regression. These regression results, along with quarterly sales data for 
the past five quarters, are presented in Exhibits 3 and 4, respectively.
Exhibit 3: Log Differenced Sales AR(1) Model with Seasonal Lag: 
PoweredUP , Inc., Last 10 Y ears of Quarterly Sales
Regression Statistics
R2  0.6788  
Standard error   0.0424  
Observations   35  
Durbin–Watson   1.8799  
  Coefficient Standard Error t-Statistic
Intercept 0.0092 0.0087 1.0582
ln Salest−1 − ln Salest−2 −0.1279 0.1137 −1.1252
ln Salest−4 − ln Salest−5 0.7239 0.1093 6.6209
Autocorrelations of the Residual
Lag Autocorrelation Standard Error t-Statistic
1 0.0574 0.1690 0.3396
2 0.0440 0.1690 0.2604
3 0.1923 0.1690 1.1379
4 −0.1054 0.1690 −0.6237
Note: The critical t -statistic at the 5% significance level is 2.03. 
Exhibit 4: Most Recent Quarterly Sales Data (in billions)
Dec 2015 (Salest−1) $3.868
Sep 2015 (Salest−2) $3.780
June 2015 (Salest−3) $3.692
Mar 2015 (Salest−4) $3.836
Dec 2014 (Salest−5) $3.418
After completing his work on PoweredUP , Busse is asked to analyze the rela-
tionship of oil prices and the stock prices of three transportation companies. His 
firm wants to know whether the stock prices can be predicted by the price of oil. 
Exhibit 5 shows selected information from the results of his analysis.Practice Problems 73
Exhibit 5: Analysis Summary of Stock Prices for Three Transportation Stocks and the Price of Oil
 
Unit 
Root?Linear or 
Exponential 
Trend?Serial Correlation of 
Residuals in Trend 
Model? ARCH(1)? Comments
Company 1 Yes Exponential Yes Yes Not cointegrated with oil price
Company 2 Yes Linear Yes No Cointegrated with oil price
Company 3 No Exponential Yes No Not cointegrated with oil price
Oil Price Yes        
To assess the relationship between oil prices and stock prices, Busse runs three 
regressions using the time series of each company’s stock prices as the dependent 
variable and the time series of oil prices as the independent variable. 
27. Which of Busse’s conclusions regarding the exchange rate time series is con-
sistent with both the properties of a covariance-stationary time series and the 
properties of a random walk?
A. Conclusion 1
B. Conclusion 2
C. Conclusion 3
28. Based on the regression output in Exhibit 1, the first-differenced series used to 
run Regression 2 is consistent with:
A. a random walk.
B. covariance stationarity.
C. a random walk with drift.
29. Based on the regression results in Exhibit 1, the original time series of exchange 
rates:
A. has a unit root.
B. exhibits stationarity.
C. can be modeled using linear regression.
30. In order to perform the nonstationarity test, Busse should transform the Regres -
sion 1 equation by:
A. adding the second lag to the equation.
B. changing the regression’s independent variable.
C. subtracting the independent variable from both sides of the equation.
31. Based on the regression output in Exhibit 2, what should lead Busse to conclude 
that the Regression 3 equation is not correctly specified?
A. The Durbin–Watson statistic
B. The t-statistic for the slope coefficient Time-Series Analysis 74
C. The t-statistics for the autocorrelations of the residual
32. Based on the regression output in Exhibit 3 and sales data in Exhibit 4, the fore-
casted value of quarterly sales for March 2016 for PoweredUP is closest to: 
A. $4.193 billion.
B. $4.205 billion.
C. $4.231 billion.
33. Based on Exhibit 5, Busse should conclude that the variance of the error terms 
for Company 1:
A. is constant.
B. can be predicted.
C. is homoskedastic.
34. Based on Exhibit 5, for which company would the regression of stock prices on 
oil prices be expected to yield valid coefficients that could be used to estimate the 
long-term relationship between stock price and oil price?
A. Company 1
B. Company 2
C. Company 3
35. Based on Exhibit 5, which single time-series model would most likely be appro-
priate for Busse to use in predicting the future stock price of Company 3?
A. Log-linear trend model
B. First-differenced AR(2) model 
C. First-differenced log AR(1) model
The following information relates to questions 
36-37
Exhibit 1 shows monthly observations on the natural log of lightweight vehicle 
sales, ln(Salest), for January 1992 to December 2000.Practice Problems 75
Exhibit 1: Lightweight Vehicle Sales
1992 1993 1994 1995 1996 1997 1998 1999 2000ln (Millions of Units)
(Annualized) 
2.202.302.402.502.602.702.802.903.00
Year
36. Using the figure, comment on whether the specification ln(Salest) = b0 + b1[ln(-
Salest−1)] + εt is appropriate. 
37. State an appropriate transformation of the time series.
38. Suppose we want to predict the annualized return of the five-year T-bill using the 
annualized return of the three-month T-bill with monthly observations from Jan-
uary 1993 to December 2002. Our analysis produces the data shown in Exhibit 1.
Exhibit 1: Regression with Three-Month T-Bill as the Independent Variable 
and the Five-Y ear T-Bill as the Dependent Variable: Monthly Observations, 
January 1993–December 2002
Regression Statistics
R20.5829  
Standard error 0.6598  
Observations 120  
Durbin–Watson 0.1130  
  Coefficient Standard Error t-Statistic
Intercept 3.0530 0.2060 14.8181
Three-month 0.5722 0.0446 12.8408
Can we rely on the regression model in Exhibit 1 to produce meaningful predic -
tions? Specify what problem might be a concern with this regression.
39. Exhibit 1 shows the quarterly sales of Avon Products from 1Q 1992 to 2Q 2002.  Time-Series Analysis 76
Describe the salient features of the data shown.
Exhibit 1: Quarterly Sales at Avon
1993 1994 1995 1996 1997 1998 1999 2000 2001Millions of Dollars
Year1900
7009001100130015001700
1992 2002
The following information relates to questions 
40-41
Exhibit 1 shows the autocorrelations of the residuals from an AR(1) model fit to 
the changes in the gross profit margin (GPM) of the Home Depot, Inc.
Exhibit 1: Autocorrelations of the Residuals from 
Estimating the Regression ΔGPMt = 0.0006 − 
0.3330ΔGPMt–1 + εt, 1Q 1992–4Q 2001 (40 Observations) 
Lag Autocorrelation
1 −0.1106
2 −0.5981
3 −0.1525
4 0.8496
5 −0.1099
Exhibit 2 shows the output from a regression on changes in the GPM for Home 
Depot, where we have changed the specification of the AR regression.
Exhibit 2: Change in Gross Profit Margin for Home Depot, 1Q 1992–4Q 2001
Regression Statistics
R20.9155  
Standard error 0.0057  Practice Problems 77
Regression Statistics
Observations 40  
Durbin–Watson 2.6464  
  Coefficient Standard Error t-Statistic
Intercept −0.0001 0.0009 −0.0610
ΔGPMt−1 −0.0608 0.0687 −0.8850
ΔGPMt−4 0.8720 0.0678 12.8683
40. Identify the change that was made to the regression model. 
41. Discuss the rationale for changing the regression specification.
The following information relates to questions 
42-43
Suppose we decide to use an autoregressive model with a seasonal lag because of 
the seasonal autocorrelation in the previous problem. We are modeling quarterly 
data, so we estimate Equation 15: (ln Salest − ln Salest−1) = b0 + b1(ln Salest−1 − ln 
Salest−2) + b2(ln Salest−4 − ln Salest−5) + εt. Exhibit 1 shows the regression statis -
tics from this equation.
Exhibit 1: Log Differenced Sales: AR(1) Model with Seasonal Lag Johnson & 
Johnson Quarterly Observations, January 1985–December 2001
Regression Statistics
R20.4220  
Standard error 0.0318  
Observations 68  
Durbin–Watson 1.8784  
  Coefficient Standard Error t-Statistic
Intercept 0.0121 0.0053 2.3055
Lag 1 −0.0839 0.0958 −0.8757
Lag 4 0.6292 0.0958 6.5693
Autocorrelations of the Residual
Lag Autocorrelation Standard Error t-Statistic
1 0.0572 0.1213 0.4720
2 −0.0700 0.1213 −0.5771
3 0.0065 0.1213 −0.0532
4 −0.0368 0.1213 −0.3033
42. Using the information in Exhibit 1, determine whether the model is correctly  Time-Series Analysis 78
specified. 
43. If sales grew by 1% last quarter and by 2% four quarters ago, use the model to 
predict the sales growth for this quarter.
44. Describe how to test for autoregressive conditional heteroskedasticity (ARCH) in 
the residuals from the AR(1) regression on first differences in the civilian unem-
ployment rate, ΔUERt = b0 + b1ΔUERt−1 + εt.
The following information relates to questions 
45-47
Exhibit 1 shows the quarterly sales of Cisco Systems from 3Q 2001 to 2Q 2019.
Exhibit 1: Quarterly Sales at Cisco
0100020003000400050006000
1992 1993 1994 1995 1996 1997 1998 1999Millions of Dollars
7000
1991 2000
Year
Exhibit 2 gives the regression statistics from estimating the model Δln(Salest) = 
b0 + b1Δln(Salest−1) + εt.
Exhibit 2: Change in the Natural Log of Sales for Cisco Quarterly 
Observations, 3Q 1991–4Q 2000
Regression Statistics
R20.2899  
Standard error 0.0408  
Observations 38  
Durbin–Watson 1.5707  
  Coefficient Standard Error t-Statistic
Intercept 0.0661 0.0175 3.7840
Δln(Salest–1) 0.4698 0.1225 3.8339Practice Problems 79
45. Describe the salient features of the quarterly sales series. 
46. Describe the procedures we should use to determine whether the AR(1) specifi-
cation is correct.
47. Assuming the model is correctly specified, what is the long-run change in the log 
of sales toward which the series will tend to converge? Time-Series Analysis 80
SOLUTIONS
1. C is correct. The predicted value for period t from a linear trend is calculated 
as     ˆ y    t   =    ˆ b    0   +    ˆ b    1     (  t )     . 
October 2015 is the second month out of sample, or t = 183. So, the predicted 
value for October 2015 is calculated as 
     ˆ y    t    = 28.3278 + 0.4086(183) = $103.10. 
Therefore, the predicted WTI oil price for October 2015 based on the linear 
trend model is $103.10.
2. C is correct. The predicted value for period t from a log-linear trend is calculated 
as  ln    ˆ y    t   =    ˆ b    0   +    ˆ b    1     (  t )     . 
September 2015 is the first month out of sample, or t = 182. So, the predicted 
value for September 2015 is calculated as follows:
 ln     ˆ y    t    = 3.3929 + 0.0075(182).
 ln     ˆ y    t    = 4.7579.
     ˆ y    t    = e4.7579 = $116.50.
Therefore, the predicted WTI oil price for September 2015, based on the 
log-linear trend model, is $116.50.
3. B is correct. The Durbin–Watson statistic for the linear trend model is 0.10 and 
for the log-linear trend model is 0.08. Both of these values are below the critical 
value of 1.75. Therefore, we can reject the hypothesis of no positive serial cor -
relation in the regression errors in both the linear trend model and the log-linear 
trend model. 
4. B is correct. There are three requirements for a time series to be covariance sta-
tionary. First, the expected value of the time series must be constant and finite in 
all periods. Second, the variance of the time series must be constant and finite in 
all periods. Third, the covariance of the time series with itself for a fixed number 
of periods in the past or future must be constant and finite in all periods. Marti-
nez concludes that the mean and variance of the time series of WTI oil prices are 
not constant over time. Therefore, the time series is not covariance stationary. 
5. B is correct. The last two observations in the WTI time series are July and August 
2015, when the WTI oil price was $51.16 and $42.86, respectively. Therefore, 
September 2015 represents a one-period-ahead forecast. The one-period-ahead 
forecast from an AR(2) model is calculated as 
     ˆ x    t+1   =    ˆ b    0   +    ˆ b    1    x  t   +    ˆ b    2    x  t−1  . 
So, the one-period-ahead (September 2015) forecast is calculated as 
     ˆ x    t+1    = 2.0017 + 1.3946($42.86) − 0.4249($51.16) = $40.04. 
Therefore, the September 2015 forecast based on the AR(2) model is $40.04.
6. C is correct. The standard error of the autocorrelations is calculated as    1 _  √ _
 T     , where 
T represents the number of observations used in the regression. Therefore, the 
standard error for each of the autocorrelations is    1 _  √ _ 180      = 0.0745. Martinez can 
conclude that the residuals are serially correlated and are significantly different 
from zero because two of the four autocorrelations in Exhibit 2 have a t-statistic Solutions 81
in absolute value that is greater than the critical value of 1.97.
Choices A and B are incorrect because two of the four autocorrelations have a 
t-statistic in absolute value that is greater than the critical value of the t-statistic 
of 1.97. 
7. C is correct. The mean-reverting level from the AR(1) model is calculated as 
     ˆ x    t   =    b  0   _ 1 −  b  1     =   1.5948 _ 1 − 0.9767   = $68.45.  
Therefore, the mean-reverting WTI oil price from the AR(1) model is $68.45. The 
forecasted oil price in September 2015 will likely be greater than $42.86 because 
the model predicts that the price will rise in the next period from the August 
2015 price of $42.86. 
8. A log-linear model captures growth at a constant rate. The log-linear model 
ln(Salest) = b0 + b1t + εt would be the simplest model consistent with a constant 
growth rate for monthly sales. Note that we would need to confirm that the re-
gression assumptions are satisfied before accepting the model as valid.
9. The estimated forecasting equation is UERt = 5.5098 − 0.0294(t). The data begin 
in January 2013, and July 2013 is Period 7. Thus the linear trend model predicts 
the unemployment rate to be UER7 = 7.2237 − 0.0510(7) = 6.8667, or approxi-
mately 6.9%.
10. The DW statistic is designed to detect positive serial correlation of the errors of 
a regression equation. Under the null hypothesis of no positive serial correlation, 
the DW statistic is 2.0. Positive serial correlation will lead to a DW statistic that is 
less than 2.0. From the table in Problem 1, we see that the DW statistic is 0.1878. 
To see whether this result is significantly less than 2.0, refer to the Durbin–
Watson table in Appendix E at the end of this volume, in the column marked k = 
1 (one independent variable) and the row corresponding to 80 observations. We 
see that dl = 1.61. Because our DW statistic is clearly less than dl, we reject the 
null hypothesis of no serial correlation at the 0.05 significance level.
The presence of serial correlation in the error term violates one of the regression 
assumptions. The standard errors of the estimated coefficients will be biased 
downward, so we cannot conduct hypothesis testing on the coefficients.
11. The difference between UER and its forecast value, PRED, is the forecast error. In 
an appropriately specified regression model, the forecast errors are randomly dis -
tributed around the regression line and have a constant variance. We can see that 
the errors from this model specification are persistent. The errors tend first to be 
above the regression line, and then, starting in 2014, they tend to be below the 
regression line until 2017, when they again are persistently above the regression 
line. This persistence suggests that the errors are positively serially correlated. 
Therefore, we conclude that the model is not appropriate for making estimates.
12. The plot of the series Δln(Salest) appears to fluctuate around a constant mean; its 
volatility seems constant throughout the period. Differencing the data appears to 
have made the time series covariance stationary.
13. The plot of the series ΔUERt seems to fluctuate around a constant mean; its vol-
atility appears to be constant throughout the period. Our initial judgment is that 
the differenced series is covariance stationary.
14. The change in the unemployment rate seems covariance stationary, so we should 
first estimate an AR(1) model and test to see whether the residuals from this 
model have significant serial correlation. If the residuals do not display signifi- Time-Series Analysis 82
cant serial correlation, we should use the AR(1) model. If the residuals do display 
significant serial correlation, we should try an AR(2) model and test for serial cor -
relation of the residuals of the AR(2) model. We should continue this procedure 
until the errors from the final AR(p) model are serially uncorrelated.
15. The DW statistic cannot be appropriately used for a regression that has a lagged 
value of the dependent variable as one of the explanatory variables. To test for 
serial correlation, we need to examine the autocorrelations.
16. In a correctly specified regression, the residuals must be serially uncorrelated. We 
have 108 observations, so the standard error of the autocorrelation is  1 /  √ _
 T   , or 
in this case  1 /  √ _ 108    = 0.0962. The t-statistic for each lag is significant at the 0.01 
level. We would have to modify the model specification before continuing with 
the analysis.
17. Because the residuals from the AR(1) specification display significant serial cor -
relation, we should estimate an AR(2) model and test for serial correlation of the 
residuals of the AR(2) model. If the residuals from the AR(2) model are serially 
uncorrelated, we should then test for seasonality and ARCH behavior. If any 
serial correlation remains in the residuals, we should estimate an AR(3) process 
and test the residuals from that specification for serial correlation. We should 
continue this procedure until the errors from the final AR(p) model are serially 
uncorrelated. When serial correlation is eliminated, we should test for seasonality 
and ARCH behavior.
18. When a covariance-stationary series is at its mean-reverting level, the series will 
tend not to change until it receives a shock (εt). So, if the series ΔUERt is at the 
mean-reverting level, ΔUERt = ΔUERt−1. This implies that ΔUERt = −0.0668 − 
0.2320ΔUERt, so that (1 + 0.2320)ΔUERt = −0.0668 and ΔUERt = −0.0668/(1 + 
0.2320) = −0.0542. The mean-reverting level is −0.0542. In an AR(1) model, the 
general expression for the mean-reverting level is b0/(1 − b1).
19. The predicted change in the unemployment rate for next period is −7.38%, found 
by substituting 0.0300 into the forecasting model: −0.0668 − 0.2320(0.03) = 
−0.0738.
20. If we substitute our one-period-ahead forecast of −0.0738 into the model (using 
the chain rule of forecasting), we get a two-period-ahead forecast of −0.0497, or 
−4.97%.
21. The answer to Part B is quite close to the mean-reverting level of −0.0542. A 
stationary time series may need many periods to return to its equilibrium, 
mean-reverting level.
22. The forecast of sales is $13,647 million for the first quarter of 2020 and $13,800 
million for the second quarter of 2002, as the following table shows.
DateSales  
($ Millions) Log of SalesActual Value of Changes 
in the Log of Sales 
Δln(Salest)Forecast Value of Changes 
in the Log of Sales 
Δln(Salest)
1Q 2019 13,072 9.4782 0.0176  
2Q 2019 12,446 9.4292 −0.0491  
3Q 2019 12,958 9.4695 0.0403  
4Q 2019 13,428 9.5051 0.0356  Solutions 83
DateSales  
($ Millions) Log of SalesActual Value of Changes 
in the Log of Sales 
Δln(Salest)Forecast Value of Changes 
in the Log of Sales 
Δln(Salest)
1Q 2020 13,647 9.5213   0.0162
2Q 2020 13,800 9.5324   0.0111
We find the forecasted change in the log of sales for the first quarter of 2020 by 
inputting the value for the change in the log of sales from the previous quarter 
into the equation Δln(Salest) = 0.0068 + 0.2633Δln(Salest−1). Specifically, Δln(Sal-
est) = 0.0068 + 0.2633(0.0356) = 0.0162, which means that we forecast the log of 
sales in the first quarter of 2020 to be 9.5051 + 0.0162 = 9.5213.
Next, we forecast the change in the log of sales for the second quarter of 2020 
as Δln(Salest) = 0.0068 + 0.2633(0.0162) = 0.0111. Note that we have to use our 
first-quarter 2020 estimated value of the change in the log of sales as our input 
for Δln(Salest−1) because we are forecasting past the period for which we have 
actual data.
With a forecasted change of 0.0111, we forecast the log of sales in the second 
quarter of 2020 to be 9.5213 + 0.0111 = 9.5324.
We have forecasted the log of sales in the first and second quarters of 2020 to be 
9.5213 and 9.5324, respectively. Finally, we take the antilog of our estimates of the 
log of sales in the first and second quarters of 2020 to get our estimates of the lev -
el of sales: e9.5213 = 13,647 and e9.5324 = 13,800, respectively, for sales of $13,647 
million and $13,800 million.
23. The RMSE of the out-of-sample forecast errors is approximately 3.6%. 
Out-of-sample error refers to the difference between the realized value and the 
forecasted value of Δln(Salest) for dates beyond the estimation period. In this 
case, the out-of-sample period is 1Q 2019 to 4Q 2019. These are the four quarters 
for which we have data that we did not use to obtain the estimated model Δln(-
Salest) = 0.0068 + 0.2633Δln(Salest−1).
The steps to calculate RMSE are as follows:
i. Take the difference between the actual and the forecast values. This is the 
error. 
ii. Square the error.
iii. Sum the squared errors.
iv. Divide by the number of forecasts.
v. Take the square root of the average.
We show the calculations for RMSE in the following table.
Actual Values of 
Changes in the Log of 
Sales  
Δln(Salest)Forecast Values of 
Changes in the Log 
of Sales  
Δln(Salest)Error  
(Column 1  
− Column 2)Squared Error  
(Column 3 Squared)
0.0176 0.0147 0.0029 0.0000
−0.0491 0.0107 −0.0598 0.0036
0.0403 0.0096 0.0307 0.0009
0.0356 0.0093 0.0263 0.0007
    Sum 0.0052 Time-Series Analysis 84
Actual Values of 
Changes in the Log of 
Sales  
Δln(Salest)Forecast Values of 
Changes in the Log 
of Sales  
Δln(Salest)Error  
(Column 1  
− Column 2)Squared Error  
(Column 3 Squared)
    Mean 0.0013
    RMSE 0.036
24. The lower the RMSE, the more accurate the forecasts of a model in forecasting. 
Therefore, the model with the RMSE of 2% has greater accuracy in forecasting 
than the model in Part A, which has an RMSE of 3.6%. 
25. Predictions too far ahead can be nonsensical. For example, the AR(1) model we 
have been examining, ΔUERt = −0.0405 − 0.4674ΔUERt−1, taken at face value, 
predicts declining civilian unemployment into the indefinite future. Because the 
civilian unemployment rate will probably not go below 3% frictional unemploy -
ment and cannot go below 0% unemployment, this model’s long-range forecasts 
are implausible. The model is designed for short-term forecasting, as are many 
time-series models.
26. Using more years of data for estimation may lead to nonstationarity even in the 
series of first differences in the civilian unemployment rate. As we go further 
back in time, we increase the risk that the underlying civilian unemployment rate 
series has more than one regime (or true model). If the series has more than one 
regime, fitting one model to the entire period would not be correct. Note that 
when we have good reason to believe that a time series is stationary, a longer 
series of data is generally desirable.
27. C is correct. A random walk can be described by the equation xt = b0 + b1xt−1 + 
εt, where b0 = 0 and b1 = 1. So b0 = 0 is a characteristic of a random walk time se-
ries. A covariance-stationary series must satisfy the following three requirements:
1. The expected value of the time series must be constant and finite in all 
periods.
2. The variance of the time series must be constant and finite in all periods.
3. The covariance of the time series with itself for a fixed number of periods in 
the past or future must be constant and finite in all periods.
b0 = 0 does not violate any of these three requirements and is thus consistent 
with the properties of a covariance-stationary time series. 
28. B is correct. The critical t-statistic at a 5% confidence level is 1.98. As a result, 
neither the intercept nor the coefficient on the first lag of the first-differenced 
exchange rate in Regression 2 differs significantly from zero. Also, the residual 
autocorrelations do not differ significantly from zero. As a result, Regression 2 
can be reduced to yt = εt, with a mean-reverting level of b0/(1 − b1) = 0/1 = 0. 
Therefore, the variance of yt in each period is var(εt) = σ2. The fact that the resid-
uals are not autocorrelated is consistent with the covariance of the times series 
with itself being constant and finite at different lags. Because the variance and the 
mean of ytare constant and finite in each period, we can also conclude that ytis 
covariance stationary.
29. A is correct. If the exchange rate series is a random walk, then the 
first-differenced series will yield b0 = 0 and b1 = 0 and the error terms will not 
be serially correlated. The data in Exhibit 1 show that this is the case: Neither 
the intercept nor the coefficient on the first lag of the first-differenced exchange Solutions 85
rate in Regression 2 differs significantly from zero because the t-statistics of both 
coefficients are less than the critical t-statistic of 1.98. Also, the residual auto-
correlations do not differ significantly from zero because the t-statistics of all 
autocorrelations are less than the critical t-statistic of 1.98. Therefore, because all 
random walks have unit roots, the exchange rate time series used to run Regres -
sion 1 has a unit root.
30. C is correct. To conduct the Dickey–Fuller test, one must subtract the inde-
pendent variable, xt−1,from both sides of the original AR(1) model. This results 
in a change of the dependent variable (from xt to xt − xt−1) and a change in the 
regression’s slope coefficient (from b1 to b1 − 1) but not a change in the indepen-
dent variable.
31. C is correct. The regression output in Exhibit 2 suggests there is serial correlation 
in the residual errors. The fourth autocorrelation of the residual has a value of 
0.6994 and a t-statistic of 4.3111, which is greater than the t- statistic critical value 
of 2.02. Therefore, the null hypothesis that the fourth autocorrelation is equal to 
zero can be rejected. This indicates strong and significant seasonal autocorrela-
tion, which means the Regression 3 equation is misspecified.
32. C is correct. The quarterly sales for March 2016 are calculated as follows:
 ln Salest − ln Salest−1 = b0 + b1(ln Salest−1 − ln Salest−2) + b2(ln Salest−4 − ln 
Salest−5).
 ln Salest − ln 3.868 = 0.0092 − 0.1279(ln 3.868 − ln 3.780) + 0.7239(ln 3.836 − 
ln 3.418).
 ln Salest − 1.35274 = 0.0092 − 0.1279(1.35274 − 1.32972) + 0.7239(1.34443 
− 1.22906).
 ln Salest = 1.35274 + 0.0092 − 0.1279(0.02301) + 0.7239(0.11538).
 ln Salest = 1.44251.
 Salest = e1.44251 = 4.231.
33. B is correct. Exhibit 3 shows that the time series of the stock prices of Compa-
ny 1 exhibits heteroskedasticity, as evidenced by the fact that the time series is 
ARCH(1). If a time series is ARCH(1), then the variance of the error in one peri-
od depends on the variance of the error in previous periods. Therefore, the vari-
ance of the errors in period t + 1 can be predicted in period t using the formula 
     ˆ σ    t+1  2   =    ˆ a    0   +    ˆ a    1      ˆ ε    t  2 . 
34. B is correct. When two time series have a unit root but are cointegrated, the error 
term in the linear regression of one time series on the other will be covariance 
stationary. Exhibit 5 shows that the series of stock prices of Company 2 and the 
oil prices both contain a unit root and the two time series are cointegrated. As 
a result, the regression coefficients and standard errors are consistent and can 
be used for hypothesis tests. Although the cointegrated regression estimates the 
long-term relation between the two series, it may not be the best model of the 
short-term relationship. 
35. C is correct. As a result of the exponential trend in the time series of stock prices 
for Company 3, Busse would want to take the natural log of the series and then 
first-difference it. Because the time series also has serial correlation in the resid-
uals from the trend model, Busse should use a more complex model, such as an  Time-Series Analysis 86
autoregressive (AR) model. 
36. The graph of ln(Salest) appears to trend upward over time. A series that trends 
upward or downward over time often has a unit root and is thus not covariance 
stationary. Therefore, using an AR(1) regression on the undifferenced series is 
probably not correct. In practice, we need to examine regression statistics to 
confirm such visual impressions.
37. The most common way to transform a time series with a unit root into a 
covariance-stationary time series is to difference the data—that is, to create a 
new series: Δln(Salest) = ln(Salest) − ln(Salest−1).
38. To determine whether we can use linear regression to model more than one time 
series, we should first determine whether any of the time series has a unit root. If 
none of the time series has a unit root, then we can safely use linear regression to 
test the relations between the two time series. Note that if one of the two vari-
ables has a unit root, then our analysis would not provide valid results; if both 
of the variables have unit roots, then we would need to evaluate whether the 
variables are cointegrated.
39. The quarterly sales of Avon show an upward trend and a clear seasonal pattern, 
as indicated by the repeated regular cycle.
40. A second explanatory variable, the change in the gross profit margin lagged four 
quarters, ΔGPMt−4, was added.
41. The model was augmented to account for seasonality in the time series (with 
quarterly data, significant autocorrelation at the fourth lag indicates seasonali-
ty). The standard error of the autocorrelation coefficient equals 1 divided by the 
square root of the number of observations:  1 /  √ _ 40   , or 0.1581. The autocorrelation 
at the fourth lag (0.8496) is significant: t = 0.8496/0.1581 = 5.37. This indicates 
seasonality, and accordingly we added ΔGPMt−4. Note that in the augment -
ed regression, the coefficient on ΔGPMt−4 is highly significant. (Although the 
autocorrelation at second lag is also significant, the fourth lag is more important 
because of the rationale of seasonality. Once the fourth lag is introduced as an 
independent variable, we might expect that the second lag in the residuals would 
not be significant.)
42. In order to determine whether this model is correctly specified, we need to test 
for serial correlation among the residuals. We want to test whether we can reject 
the null hypothesis that the value of each autocorrelation is 0 against the alterna-
tive hypothesis that each is not equal to 0. At the 0.05 significance level, with 68 
observations and three parameters, this model has 65 degrees of freedom. The 
critical value of the t-statistic needed to reject the null hypothesis is thus about 
2.0. The absolute value of the t-statistic for each autocorrelation is below 0.60 
(less than 2.0), so we cannot reject the null hypothesis that each autocorrelation 
is not significantly different from 0. We have determined that the model is cor -
rectly specified.
43. If sales grew by 1% last quarter and by 2% four quarters ago, then the mod-
el predicts that sales growth this quarter will be 0.0121 − 0.0839[ln(1.01)] + 
0.6292[ln(1.02)] = e0.02372 − 1 = 2.40%.
44. We should estimate the regression ΔUERt = b0 + b1ΔUERt−1 + εt and save the re-
siduals from the regression. Then we should create a new variable,     ˆ ε    t  2  , by squaring 
the residuals. Finally, we should estimate     ˆ ε    t  2  =  a  0   +  a  1      ˆ ε    t−1  2   +  u  t    and test to see 
whether a1 is statistically different from 0.Solutions 87
45. The series has a steady upward trend of growth, suggesting an exponential 
growth rate. This finding suggests transforming the series by taking the natural 
log and differencing the data.
46. First, we should determine whether the residuals from the AR(1) specification 
are serially uncorrelated. If the residuals are serially correlated, then we should 
try an AR(2) specification and then test the residuals from the AR(2) model for 
serial correlation. We should continue in this fashion until the residuals are se-
rially uncorrelated and then look for seasonality in the residuals. If seasonality is 
present, we should add a seasonal lag. If no seasonality is present, we should test 
for ARCH. If ARCH is not present, we can conclude that the model is correctly 
specified.
47. If the model Δln(Salest) = b0 + b1[Δln(Salest−1)] + εt is correctly specified, 
then the series Δln(Salest) is covariance stationary. So, this series tends to its 
mean-reverting level, which is b0/(1 − b1), or 0.0661/(1 − 0.4698) = 0.1247.