{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2425b71f-c608-4940-84ab-9584c6e29a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af0cab7e-afad-47c7-bc74-be9430ea3cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key = 'your_access_key'\n",
    "secret_key = 'your_secret_key'\n",
    "bucket_name = 'my-cfa-pdfs'\n",
    "folder_key = 'pypdf/'\n",
    "output_folder = './metadata_output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "820f99f6-614b-4ca1-b483-840379727a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an S3 client\n",
    "# s3 = boto3.client('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key)\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e282ca51-1b2b-42bc-b984-7e32e2f01e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving metadata to individual files:\n",
      "Metadata saved to ./metadata_output/pypdf__metadata.txt\n",
      "Metadata saved to ./metadata_output/pypdf_2024-l1-topics-combined-2.txt_metadata.txt\n",
      "Metadata saved to ./metadata_output/pypdf_2024-l1-topics-combined-2_extracted_text.txt_metadata.txt\n",
      "Metadata saved to ./metadata_output/pypdf_2024-l2-topics-combined-2.txt_metadata.txt\n",
      "Metadata saved to ./metadata_output/pypdf_2024-l2-topics-combined-2_extracted_text.txt_metadata.txt\n",
      "Metadata saved to ./metadata_output/pypdf_2024-l3-topics-combined-2.txt_metadata.txt\n",
      "Metadata saved to ./metadata_output/pypdf_2024-l3-topics-combined-2_extracted_text.txt_metadata.txt\n",
      "Metadata saved to ./metadata_output/pypdf_coveo_links.txt_metadata.txt\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=folder_key)\n",
    "    objects = response.get('Contents', [])\n",
    "\n",
    "    if not objects:\n",
    "        print(\"No files found in the specified folder.\")\n",
    "    else:\n",
    "        # Create output folder if it doesn't exist\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        # Save metadata to individual files\n",
    "        print(\"Saving metadata to individual files:\")\n",
    "        for obj in objects:\n",
    "            file_key = obj['Key']\n",
    "            try:\n",
    "                # Retrieve file metadata\n",
    "                file_metadata = s3.head_object(Bucket=bucket_name, Key=file_key)\n",
    "                \n",
    "                # Extract metadata values\n",
    "                last_modified = file_metadata['LastModified']\n",
    "                etag = file_metadata['ETag']\n",
    "                content_type = file_metadata.get('ContentType', 'N/A')\n",
    "                content_length = file_metadata['ContentLength']\n",
    "                s3_link = f\"s3://{bucket_name}/{file_key}\"\n",
    "\n",
    "                # Create output file path\n",
    "                output_file_path = os.path.join(output_folder, f\"{file_key.replace('/', '_')}_metadata.txt\")\n",
    "\n",
    "                # Save metadata to file\n",
    "                with open(output_file_path, 'w') as output_file:\n",
    "                    output_file.write(f\"Metadata for file: {file_key}\\n\")\n",
    "                    output_file.write(f\"  Last Modified: {last_modified}\\n\")\n",
    "                    output_file.write(f\"  ETag: {etag}\\n\")\n",
    "                    output_file.write(f\"  Content Type: {content_type}\\n\")\n",
    "                    output_file.write(f\"  Content Length: {content_length} bytes\\n\")\n",
    "                    output_file.write(f\"  S3 Link: {s3_link}\\n\")\n",
    "\n",
    "                print(f\"Metadata saved to {output_file_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error retrieving metadata for {file_key}: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error listing objects: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beea0249-22fb-481d-8841-7dfe74fa4aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error retrieving metadata for pypdf/: list index out of range\n",
      "Error listing objects: 'datetime.datetime' object is not subscriptable\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "# Replace 'your_access_key', 'your_secret_key', 'your_bucket_name', 'your_folder', and 'output_csv_file' with your AWS credentials and details\n",
    "access_key = 'your_access_key'\n",
    "secret_key = 'your_secret_key'\n",
    "bucket_name = 'my-cfa-pdfs'\n",
    "folder_key = 'pypdf/'  # Specify the folder path\n",
    "output_csv_file = '../sample_output/metadata_output.csv'  # Specify the output CSV file path\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# List objects in the folder\n",
    "try:\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=folder_key)\n",
    "    objects = response.get('Contents', [])\n",
    "\n",
    "    if not objects:\n",
    "        print(\"No files found in the specified folder.\")\n",
    "    else:\n",
    "        # Organize metadata into a dictionary\n",
    "        metadata_dict = {}\n",
    "\n",
    "        # Iterate over objects and extract metadata\n",
    "        for obj in objects:\n",
    "            file_key = obj['Key']\n",
    "            try:\n",
    "                # Retrieve file metadata\n",
    "                file_metadata = s3.head_object(Bucket=bucket_name, Key=file_key)\n",
    "\n",
    "                # Extract common substring (e.g., '2024_l1_combined' or '2024_l2_combined')\n",
    "                common_substring = file_key.split('_RR_')[1].split('_combined')[0]\n",
    "\n",
    "                # If common substring not in dictionary, create an entry\n",
    "                if common_substring not in metadata_dict:\n",
    "                    metadata_dict[common_substring] = {}\n",
    "\n",
    "                # Extract metadata values\n",
    "                metadata_dict[common_substring][file_key] = {\n",
    "                    'Last Modified': file_metadata['LastModified'],\n",
    "                    'ETag': file_metadata['ETag'],\n",
    "                    'Content Type': file_metadata.get('ContentType', 'N/A'),\n",
    "                    'Content Length': file_metadata['ContentLength'],\n",
    "                    'S3 Link': f\"s3://{bucket_name}/{file_key}\"\n",
    "                }\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error retrieving metadata for {file_key}: {e}\")\n",
    "\n",
    "        # Save metadata to CSV file\n",
    "        with open(output_csv_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "            csv_writer = csv.writer(csv_file)\n",
    "\n",
    "            # Write header row\n",
    "            header_row = ['Common Substring', 'File Type']\n",
    "            for file_key, metadata in metadata_dict.items():\n",
    "                for subfile_key, subfile_metadata in metadata.items():\n",
    "                    for metadata_key in subfile_metadata.keys():\n",
    "                        header_row.append(f\"{file_key}_{metadata_key}\")\n",
    "            csv_writer.writerow(header_row)\n",
    "\n",
    "            # Write data rows\n",
    "            for common_substring, metadata in metadata_dict.items():\n",
    "                for subfile_key, subfile_metadata in metadata.items():\n",
    "                    data_row = [common_substring, subfile_key.split('_RR_')[0]]\n",
    "                    for metadata_key, value in subfile_metadata.items():\n",
    "                        data_row.extend([value['Last Modified'], value['ETag'], value['Content Type'], value['Content Length'], value['S3 Link']])\n",
    "                    csv_writer.writerow(data_row)\n",
    "\n",
    "        print(f\"Metadata saved to {output_csv_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error listing objects: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc3349f2-8a7f-4496-9ba2-7dc04549b90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Key': 'pypdf/', 'LastModified': datetime.datetime(2024, 2, 11, 17, 33, 15, tzinfo=tzutc()), 'ETag': '\"d41d8cd98f00b204e9800998ecf8427e\"', 'Size': 0, 'StorageClass': 'STANDARD'}, {'Key': 'pypdf/Grobid_RR_2024_l1_combined.txt', 'LastModified': datetime.datetime(2024, 2, 13, 22, 21, 50, tzinfo=tzutc()), 'ETag': '\"20923b87e7442a82db6c2901d9157931\"', 'Size': 52416, 'StorageClass': 'STANDARD'}, {'Key': 'pypdf/Grobid_RR_2024_l2_combined.txt', 'LastModified': datetime.datetime(2024, 2, 13, 22, 21, 50, tzinfo=tzutc()), 'ETag': '\"cd30b300f521c7ca1af3b379b4d63023\"', 'Size': 50368, 'StorageClass': 'STANDARD'}, {'Key': 'pypdf/Grobid_RR_2024_l3_combined.txt', 'LastModified': datetime.datetime(2024, 2, 13, 22, 21, 50, tzinfo=tzutc()), 'ETag': '\"97a9b17d373ba89ae1be0b47a33e9a5b\"', 'Size': 32699, 'StorageClass': 'STANDARD'}, {'Key': 'pypdf/PyPDF_RR_2024_l1_combined.txt', 'LastModified': datetime.datetime(2024, 2, 13, 22, 21, 37, tzinfo=tzutc()), 'ETag': '\"fd57ef7f8805d0191c7109f21d11837e\"', 'Size': 46586, 'StorageClass': 'STANDARD'}, {'Key': 'pypdf/PyPDF_RR_2024_l2_combined.txt', 'LastModified': datetime.datetime(2024, 2, 13, 22, 21, 37, tzinfo=tzutc()), 'ETag': '\"7737dc9531500ffbed9f4655057ad8c1\"', 'Size': 47620, 'StorageClass': 'STANDARD'}, {'Key': 'pypdf/PyPDF_RR_2024_l3_combined.txt', 'LastModified': datetime.datetime(2024, 2, 13, 22, 21, 38, tzinfo=tzutc()), 'ETag': '\"7fa45441467d7b82130dfd0ac44c9d66\"', 'Size': 31945, 'StorageClass': 'STANDARD'}]\n",
      "hello\n",
      "Error retrieving metadata for file key l1: An error occurred (404) when calling the HeadObject operation: Not Found\n",
      "Error retrieving metadata for file key l2: An error occurred (404) when calling the HeadObject operation: Not Found\n",
      "Error retrieving metadata for file key l3: An error occurred (404) when calling the HeadObject operation: Not Found\n",
      "Metadata saved to ../sample_output/metadata_output.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "# Replace 'your_access_key', 'your_secret_key', 'your_bucket_name', 'your_folder', and 'output_csv_file' with your AWS credentials and details\n",
    "access_key = 'your_access_key'\n",
    "secret_key = 'your_secret_key'\n",
    "bucket_name = 'my-cfa-pdfs'\n",
    "folder_key = 'pypdf/'  # Specify the folder path\n",
    "output_csv_file = '../sample_output/metadata_output.csv'  # Specify the output CSV file path\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# List objects in the folder\n",
    "try:\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=folder_key)\n",
    "    objects = response.get('Contents', [])\n",
    "\n",
    "    if not objects:\n",
    "        print(\"No files found in the specified folder.\")\n",
    "    else:\n",
    "        # Save metadata to CSV file\n",
    "        with open(output_csv_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "            csv_writer = csv.writer(csv_file)\n",
    "            \n",
    "            # Write header row\n",
    "            csv_writer.writerow(['File Key', 'Last Modified', 'ETag', 'Content Type', 'Content Length', 'S3 Link'])\n",
    "            print(objects)\n",
    "            # Extract unique file keys (e.g., '2024_l1_combined', '2024_l2_combined', etc.)\n",
    "            unique_file_keys = set(file['Key'].split('_')[3] for file in objects if file['Key'] != 'pypdf/')\n",
    "            print(\"hello\")\n",
    "            # Write metadata for each unique file key\n",
    "            for file_key_suffix in sorted(unique_file_keys):\n",
    "                pypdf_key = f'PyPDF_RR_2024_l{file_key_suffix}_combined.txt'\n",
    "                grobid_key = f'Grobid_RR_2024_l{file_key_suffix}_combined.txt'\n",
    "\n",
    "                try:\n",
    "                    # Retrieve metadata for PyPDF file\n",
    "                    pypdf_metadata = s3.head_object(Bucket=bucket_name, Key=pypdf_key)\n",
    "                    pypdf_last_modified = pypdf_metadata['LastModified']\n",
    "                    pypdf_etag = pypdf_metadata['ETag']\n",
    "                    pypdf_content_type = pypdf_metadata.get('ContentType', 'N/A')\n",
    "                    pypdf_content_length = pypdf_metadata['ContentLength']\n",
    "                    pypdf_s3_link = f\"s3://{bucket_name}/{pypdf_key}\"\n",
    "\n",
    "                    # Retrieve metadata for Grobid file\n",
    "                    grobid_metadata = s3.head_object(Bucket=bucket_name, Key=grobid_key)\n",
    "                    grobid_last_modified = grobid_metadata['LastModified']\n",
    "                    grobid_etag = grobid_metadata['ETag']\n",
    "                    grobid_content_type = grobid_metadata.get('ContentType', 'N/A')\n",
    "                    grobid_content_length = grobid_metadata['ContentLength']\n",
    "                    grobid_s3_link = f\"s3://{bucket_name}/{grobid_key}\"\n",
    "                    print([f'RR_2024_l{file_key_suffix}_combined',\n",
    "                                         pypdf_last_modified, pypdf_etag, pypdf_content_type, pypdf_content_length, pypdf_s3_link,\n",
    "                                         grobid_last_modified, grobid_etag, grobid_content_type, grobid_content_length, grobid_s3_link])\n",
    "                    # Write metadata to CSV file\n",
    "                    csv_writer.writerow([f'RR_2024_l{file_key_suffix}_combined',\n",
    "                                         pypdf_last_modified, pypdf_etag, pypdf_content_type, pypdf_content_length, pypdf_s3_link,\n",
    "                                         grobid_last_modified, grobid_etag, grobid_content_type, grobid_content_length, grobid_s3_link])\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error retrieving metadata for file key {file_key_suffix}: {e}\")\n",
    "\n",
    "        print(f\"Metadata saved to {output_csv_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error listing objects: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae21583d-322f-49e5-9419-74be350fdfc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd7b1a0-8f82-4a66-b5f8-93abc6a53269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b58055d8-8c83-484e-94a9-95f975995bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved to ../sample_output/metadata_output.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "import boto3\n",
    "\n",
    "# Replace 'your_access_key', 'your_secret_key', 'your_bucket_name', 'your_folder', and 'output_csv_file' with your AWS credentials and details\n",
    "access_key = 'your_access_key'\n",
    "secret_key = 'your_secret_key'\n",
    "bucket_name = 'my-cfa-pdfs'\n",
    "folder_key = 'pypdf/'  # Specify the folder path\n",
    "output_csv_file = '../sample_output/metadata_output.csv'  # Specify the output CSV file path\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# List objects in the folder\n",
    "try:\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=folder_key)\n",
    "    objects = response.get('Contents', [])\n",
    "\n",
    "    if not objects:\n",
    "        print(\"No files found in the specified folder.\")\n",
    "    else:\n",
    "        # Create dictionaries to store metadata for PyPDF and Grobid separately\n",
    "        pypdf_metadata = defaultdict(dict)\n",
    "        grobid_metadata = defaultdict(dict)\n",
    "\n",
    "        # Process each object and store metadata\n",
    "        for obj in objects:\n",
    "            file_key = obj['Key']\n",
    "            if 'Grobid' in file_key:\n",
    "                row_name = file_key.split('_l')[1].split('_combined')[0]  # Extract RR_2024_l1 from the key\n",
    "                grobid_metadata[row_name]['File Key'] = file_key\n",
    "                grobid_metadata[row_name]['Last Modified'] = obj['LastModified']\n",
    "                grobid_metadata[row_name]['ETag'] = obj['ETag']\n",
    "                grobid_metadata[row_name]['Size'] = obj['Size']\n",
    "            elif 'PyPDF' in file_key:\n",
    "                row_name = file_key.split('_l')[1].split('_combined')[0]  # Extract RR_2024_l1 from the key\n",
    "                pypdf_metadata[row_name]['File Key'] = file_key\n",
    "                pypdf_metadata[row_name]['Last Modified'] = obj['LastModified']\n",
    "                pypdf_metadata[row_name]['ETag'] = obj['ETag']\n",
    "                pypdf_metadata[row_name]['Size'] = obj['Size']\n",
    "\n",
    "        # Combine PyPDF and Grobid metadata into rows\n",
    "        rows = []\n",
    "        for row_name, grobid_info in grobid_metadata.items():\n",
    "            row = grobid_info\n",
    "            row.update(pypdf_metadata.get(row_name, {}))\n",
    "            rows.append(row)\n",
    "\n",
    "        # Save metadata to CSV file\n",
    "        with open(output_csv_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "            fieldnames = ['File Key', 'Last Modified', 'ETag', 'Size']  # Define the field names\n",
    "            csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "            # Write header row\n",
    "            csv_writer.writeheader()\n",
    "\n",
    "            # Write metadata for each row\n",
    "            for row in rows:\n",
    "                csv_writer.writerow(row)\n",
    "\n",
    "        print(f\"Metadata saved to {output_csv_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error listing objects: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2484168-0ecd-4016-9091-537eabf19782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved to ../sample_output/metadata_output.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "import boto3\n",
    "\n",
    "# Replace 'your_access_key', 'your_secret_key', 'your_bucket_name', 'your_folder', and 'output_csv_file' with your AWS credentials and details\n",
    "access_key = 'your_access_key'\n",
    "secret_key = 'your_secret_key'\n",
    "bucket_name = 'my-cfa-pdfs'\n",
    "folder_key = 'pypdf/'  # Specify the folder path\n",
    "output_csv_file = '../sample_output/metadata_output.csv'  # Specify the output CSV file path\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# List objects in the folder\n",
    "try:\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=folder_key)\n",
    "    objects = response.get('Contents', [])\n",
    "\n",
    "    if not objects:\n",
    "        print(\"No files found in the specified folder.\")\n",
    "    else:\n",
    "        # Create dictionaries to store metadata for PyPDF and Grobid separately\n",
    "        pypdf_metadata = defaultdict(dict)\n",
    "        grobid_metadata = defaultdict(dict)\n",
    "\n",
    "        # Process each object and store metadata\n",
    "        for obj in objects:\n",
    "            file_key = obj['Key']\n",
    "            if 'Grobid' in file_key:\n",
    "                row_name = file_key.split('_l')[1].split('_combined')[0]  # Extract RR_2024_l1 from the key\n",
    "                grobid_metadata[row_name]['File Key'] = file_key\n",
    "                grobid_metadata[row_name]['Last Modified'] = obj['LastModified']\n",
    "                grobid_metadata[row_name]['ETag'] = obj['ETag']\n",
    "                grobid_metadata[row_name]['Size'] = obj['Size']\n",
    "            elif 'PyPDF' in file_key:\n",
    "                row_name = file_key.split('_l')[1].split('_combined')[0]  # Extract RR_2024_l1 from the key\n",
    "                pypdf_metadata[row_name]['File Key'] = file_key\n",
    "                pypdf_metadata[row_name]['Last Modified'] = obj['LastModified']\n",
    "                pypdf_metadata[row_name]['ETag'] = obj['ETag']\n",
    "                pypdf_metadata[row_name]['Size'] = obj['Size']\n",
    "\n",
    "        # Combine PyPDF and Grobid metadata into rows\n",
    "        rows = []\n",
    "        for row_name, grobid_info in grobid_metadata.items():\n",
    "            row = {}\n",
    "            row.update(grobid_info)\n",
    "            row.update(pypdf_metadata.get(row_name, {}))\n",
    "            rows.append(row)\n",
    "\n",
    "        # Save metadata to CSV file\n",
    "        with open(output_csv_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "            fieldnames = ['File Key', 'Last Modified (Grobid)', 'ETag (Grobid)', 'Size (Grobid)',\n",
    "                          'Last Modified (PyPDF)', 'ETag (PyPDF)', 'Size (PyPDF)']  # Define the field names\n",
    "            csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "            # Write header row\n",
    "            csv_writer.writeheader()\n",
    "\n",
    "            # Write metadata for each row\n",
    "            for row in rows:\n",
    "                csv_writer.writerow({\n",
    "                    'File Key': row.get('File Key', ''),\n",
    "                    'Last Modified (Grobid)': row.get('Last Modified', ''),\n",
    "                    'ETag (Grobid)': row.get('ETag', ''),\n",
    "                    'Size (Grobid)': row.get('Size', ''),\n",
    "                    'Last Modified (PyPDF)': row.get('Last Modified', ''),\n",
    "                    'ETag (PyPDF)': row.get('ETag', ''),\n",
    "                    'Size (PyPDF)': row.get('Size', ''),\n",
    "                })\n",
    "\n",
    "        print(f\"Metadata saved to {output_csv_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error listing objects: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c94b09d-a322-4817-ba9a-920ef010c786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved to ../sample_output/metadata_output.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "import boto3\n",
    "\n",
    "# Replace 'your_access_key', 'your_secret_key', 'your_bucket_name', 'your_folder', and 'output_csv_file' with your AWS credentials and details\n",
    "access_key = 'your_access_key'\n",
    "secret_key = 'your_secret_key'\n",
    "bucket_name = 'my-cfa-pdfs'\n",
    "folder_key = 'pypdf/'  # Specify the folder path\n",
    "output_csv_file = '../sample_output/metadata_output.csv'  # Specify the output CSV file path\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# List objects in the folder\n",
    "try:\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=folder_key)\n",
    "    objects = response.get('Contents', [])\n",
    "\n",
    "    if not objects:\n",
    "        print(\"No files found in the specified folder.\")\n",
    "    else:\n",
    "        # Create dictionaries to store metadata for PyPDF and Grobid separately\n",
    "        pypdf_metadata = defaultdict(dict)\n",
    "        grobid_metadata = defaultdict(dict)\n",
    "\n",
    "        # Process each object and store metadata\n",
    "        for obj in objects:\n",
    "            file_key = obj['Key']\n",
    "            if 'Grobid' in file_key:\n",
    "                row_name = file_key.split('_l')[1].split('_combined')[0]  # Extract RR_2024_l1 from the key\n",
    "                grobid_metadata[row_name]['File Key'] = row_name\n",
    "                grobid_metadata[row_name]['Last Modified'] = obj['LastModified']\n",
    "                grobid_metadata[row_name]['ETag'] = obj['ETag']\n",
    "                grobid_metadata[row_name]['Size'] = obj['Size']\n",
    "            elif 'PyPDF' in file_key:\n",
    "                row_name = file_key.split('_l')[1].split('_combined')[0]  # Extract RR_2024_l1 from the key\n",
    "                pypdf_metadata[row_name]['File Key'] = row_name\n",
    "                pypdf_metadata[row_name]['Last Modified'] = obj['LastModified']\n",
    "                pypdf_metadata[row_name]['ETag'] = obj['ETag']\n",
    "                pypdf_metadata[row_name]['Size'] = obj['Size']\n",
    "\n",
    "        # Combine PyPDF and Grobid metadata into rows\n",
    "        rows = []\n",
    "        for row_name, grobid_info in grobid_metadata.items():\n",
    "            row = {}\n",
    "            row.update(grobid_info)\n",
    "            row.update(pypdf_metadata.get(row_name, {}))\n",
    "            rows.append(row)\n",
    "\n",
    "        # Save metadata to CSV file\n",
    "        with open(output_csv_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "            fieldnames = ['File Key', 'Last Modified (Grobid)', 'ETag (Grobid)', 'Size (Grobid)',\n",
    "                          'Last Modified (PyPDF)', 'ETag (PyPDF)', 'Size (PyPDF)']  # Define the field names\n",
    "            csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "            # Write header row\n",
    "            csv_writer.writeheader()\n",
    "\n",
    "            # Write metadata for each row\n",
    "            for row in rows:\n",
    "                csv_writer.writerow({\n",
    "                    'File Key': row.get('File Key', ''),\n",
    "                    'Last Modified (Grobid)': row.get('Last Modified', ''),\n",
    "                    'ETag (Grobid)': row.get('ETag', ''),\n",
    "                    'Size (Grobid)': row.get('Size', ''),\n",
    "                    'Last Modified (PyPDF)': row.get('Last Modified', ''),\n",
    "                    'ETag (PyPDF)': row.get('ETag', ''),\n",
    "                    'Size (PyPDF)': row.get('Size', ''),\n",
    "                })\n",
    "\n",
    "        print(f\"Metadata saved to {output_csv_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error listing objects: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8876f6cc-bbfe-4a50-9eec-245045ff4e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved to ../sample_output/metadata_output.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "import boto3\n",
    "\n",
    "# Replace 'your_access_key', 'your_secret_key', 'your_bucket_name', 'your_folder', and 'output_csv_file' with your AWS credentials and details\n",
    "access_key = 'your_access_key'\n",
    "secret_key = 'your_secret_key'\n",
    "bucket_name = 'my-cfa-pdfs'\n",
    "folder_key = 'pypdf/'  # Specify the folder path\n",
    "output_csv_file = '../sample_output/metadata_output.csv'  # Specify the output CSV file path\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# List objects in the folder\n",
    "try:\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=folder_key)\n",
    "    objects = response.get('Contents', [])\n",
    "\n",
    "    if not objects:\n",
    "        print(\"No files found in the specified folder.\")\n",
    "    else:\n",
    "        # Create dictionaries to store metadata for PyPDF and Grobid separately\n",
    "        pypdf_metadata = defaultdict(dict)\n",
    "        grobid_metadata = defaultdict(dict)\n",
    "\n",
    "        # Process each object and store metadata\n",
    "        for obj in objects:\n",
    "            file_key = obj['Key']\n",
    "            if 'Grobid' in file_key:\n",
    "                row_name = file_key.split('_', 1)[1]  # Extract RR_2024_l1 from the key\n",
    "                grobid_metadata[row_name]['File Key'] = row_name  # Extract only RR_2024_l1_combined.txt\n",
    "                grobid_metadata[row_name]['Last Modified'] = obj['LastModified']\n",
    "                grobid_metadata[row_name]['ETag'] = obj['ETag']\n",
    "                grobid_metadata[row_name]['Size'] = obj['Size']\n",
    "            elif 'PyPDF' in file_key:\n",
    "                row_name = file_key.split('_', 1)[1]  # Extract RR_2024_l1 from the key\n",
    "                pypdf_metadata[row_name]['File Key'] = row_name  # Extract only RR_2024_l1_combined.txt\n",
    "                pypdf_metadata[row_name]['Last Modified'] = obj['LastModified']\n",
    "                pypdf_metadata[row_name]['ETag'] = obj['ETag']\n",
    "                pypdf_metadata[row_name]['Size'] = obj['Size']\n",
    "\n",
    "        # Combine PyPDF and Grobid metadata into rows\n",
    "        rows = []\n",
    "        for row_name, grobid_info in grobid_metadata.items():\n",
    "            row = {}\n",
    "            row.update(grobid_info)\n",
    "            row.update(pypdf_metadata.get(row_name, {}))\n",
    "            rows.append(row)\n",
    "\n",
    "        # Save metadata to CSV file\n",
    "        with open(output_csv_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "            fieldnames = ['File Key', 'Last Modified (Grobid)', 'ETag (Grobid)', 'Size (Grobid)',\n",
    "                          'Last Modified (PyPDF)', 'ETag (PyPDF)', 'Size (PyPDF)']  # Define the field names\n",
    "            csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "            # Write header row\n",
    "            csv_writer.writeheader()\n",
    "\n",
    "            # Write metadata for each row\n",
    "            for row in rows:\n",
    "                csv_writer.writerow({\n",
    "                    'File Key': row.get('File Key', ''),\n",
    "                    'Last Modified (Grobid)': row.get('Last Modified', ''),\n",
    "                    'ETag (Grobid)': row.get('ETag', ''),\n",
    "                    'Size (Grobid)': row.get('Size', ''),\n",
    "                    'Last Modified (PyPDF)': row.get('Last Modified', ''),\n",
    "                    'ETag (PyPDF)': row.get('ETag', ''),\n",
    "                    'Size (PyPDF)': row.get('Size', ''),\n",
    "                })\n",
    "\n",
    "        print(f\"Metadata saved to {output_csv_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error listing objects: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ea5245-b826-4c3a-9f3d-d24213129672",
   "metadata": {},
   "source": [
    "## Metadata Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c1b8233-a409-4199-8b77-9213d3d42e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved to ../sample_output/metadata_output.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "import boto3\n",
    "\n",
    "# Replace 'your_access_key', 'your_secret_key', 'your_bucket_name', 'your_folder', and 'output_csv_file' with your AWS credentials and details\n",
    "access_key = 'your_access_key'\n",
    "secret_key = 'your_secret_key'\n",
    "bucket_name = 'my-cfa-pdfs'\n",
    "folder_key = 'pypdf/'  # Specify the folder path\n",
    "output_csv_file = '../sample_output/metadata_output.csv'  # Specify the output CSV file path\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# List objects in the folder\n",
    "try:\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=folder_key)\n",
    "    objects = response.get('Contents', [])\n",
    "\n",
    "    if not objects:\n",
    "        print(\"No files found in the specified folder.\")\n",
    "    else:\n",
    "        # Create dictionaries to store metadata for PyPDF and Grobid separately\n",
    "        pypdf_metadata = defaultdict(dict)\n",
    "        grobid_metadata = defaultdict(dict)\n",
    "\n",
    "        # Process each object and store metadata\n",
    "        for obj in objects:\n",
    "            file_key = obj['Key']\n",
    "            if 'Grobid' in file_key:\n",
    "                row_name = file_key.split('_', 1)[1]  # Extract RR_2024_l1 from the key\n",
    "                grobid_metadata[row_name]['File Key'] = row_name  # Extract only RR_2024_l1_combined.txt\n",
    "                grobid_metadata[row_name]['Last Modified'] = obj['LastModified']\n",
    "                grobid_metadata[row_name]['ETag'] = obj['ETag']\n",
    "                grobid_metadata[row_name]['Size'] = obj['Size']\n",
    "                grobid_metadata[row_name]['S3 Link'] = f\"https://{bucket_name}.s3.amazonaws.com/{file_key}\"\n",
    "                grobid_metadata[row_name]['File Type'] = file_key.split('.')[-1].lower()\n",
    "            elif 'PyPDF' in file_key:\n",
    "                row_name = file_key.split('_', 1)[1]  # Extract RR_2024_l1 from the key\n",
    "                pypdf_metadata[row_name]['File Key'] = row_name  # Extract only RR_2024_l1_combined.txt\n",
    "                pypdf_metadata[row_name]['Last Modified'] = obj['LastModified']\n",
    "                pypdf_metadata[row_name]['ETag'] = obj['ETag']\n",
    "                pypdf_metadata[row_name]['Size'] = obj['Size']\n",
    "                pypdf_metadata[row_name]['S3 Link'] = f\"https://{bucket_name}.s3.amazonaws.com/{file_key}\"\n",
    "                pypdf_metadata[row_name]['File Type'] = file_key.split('.')[-1].lower()\n",
    "\n",
    "        # Combine PyPDF and Grobid metadata into rows\n",
    "        rows = []\n",
    "        for row_name, grobid_info in grobid_metadata.items():\n",
    "            row = {}\n",
    "            row.update(grobid_info)\n",
    "            row.update(pypdf_metadata.get(row_name, {}))\n",
    "            rows.append(row)\n",
    "\n",
    "        # Save metadata to CSV file\n",
    "        with open(output_csv_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "            fieldnames = ['File Key', 'Last Modified (Grobid)', 'ETag (Grobid)', 'Size (Grobid)', 'S3 Link (Grobid)', 'File Type (Grobid)',\n",
    "                          'Last Modified (PyPDF)', 'ETag (PyPDF)', 'Size (PyPDF)', 'S3 Link (PyPDF)', 'File Type (PyPDF)']  # Define the field names\n",
    "            csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "            # Write header row\n",
    "            csv_writer.writeheader()\n",
    "\n",
    "            # Write metadata for each row\n",
    "            for row in rows:\n",
    "                csv_writer.writerow({\n",
    "                    'File Key': row.get('File Key', ''),\n",
    "                    'Last Modified (Grobid)': row.get('Last Modified', ''),\n",
    "                    'ETag (Grobid)': row.get('ETag', ''),\n",
    "                    'Size (Grobid)': row.get('Size', ''),\n",
    "                    'S3 Link (Grobid)': row.get('S3 Link', ''),\n",
    "                    'File Type (Grobid)': row.get('File Type', ''),\n",
    "                    'Last Modified (PyPDF)': row.get('Last Modified', ''),\n",
    "                    'ETag (PyPDF)': row.get('ETag', ''),\n",
    "                    'Size (PyPDF)': row.get('Size', ''),\n",
    "                    'S3 Link (PyPDF)': row.get('S3 Link', ''),\n",
    "                    'File Type (PyPDF)': row.get('File Type', ''),\n",
    "                })\n",
    "\n",
    "        print(f\"Metadata saved to {output_csv_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error listing objects: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b06e610-3ae8-41ed-8d57-81b3b295ab07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da95f57-1c02-4b84-a27c-42f3d2300e39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e934d80-1872-4cde-a09f-d5e7f88ad413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved to ../sample_output/metadata_output.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "import boto3\n",
    "\n",
    "# Replace 'your_access_key', 'your_secret_key', 'your_bucket_name', 'output_csv_file' with your AWS credentials and details\n",
    "access_key = 'your_access_key'\n",
    "secret_key = 'your_secret_key'\n",
    "bucket_name = 'my-cfa-pdfs'\n",
    "output_csv_file = '../sample_output/metadata_output.csv'\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# List objects for pypdf folder\n",
    "pypdf_folder_key = 'pypdf/'\n",
    "try:\n",
    "    response_pypdf = s3.list_objects_v2(Bucket=bucket_name, Prefix=pypdf_folder_key)\n",
    "    pypdf_objects = response_pypdf.get('Contents', [])\n",
    "except Exception as e:\n",
    "    print(f\"Error listing pypdf objects: {e}\")\n",
    "    pypdf_objects = []\n",
    "\n",
    "# List objects for grobid folder\n",
    "grobid_folder_key = 'grobid/'\n",
    "try:\n",
    "    response_grobid = s3.list_objects_v2(Bucket=bucket_name, Prefix=grobid_folder_key)\n",
    "    grobid_objects = response_grobid.get('Contents', [])\n",
    "except Exception as e:\n",
    "    print(f\"Error listing grobid objects: {e}\")\n",
    "    grobid_objects = []\n",
    "\n",
    "# Combine objects from both folders\n",
    "objects = pypdf_objects + grobid_objects\n",
    "\n",
    "# Process objects and store metadata\n",
    "pypdf_metadata = defaultdict(dict)\n",
    "grobid_metadata = defaultdict(dict)\n",
    "\n",
    "for obj in objects:\n",
    "    file_key = obj['Key']\n",
    "    if 'Grobid' in file_key:\n",
    "        row_name = file_key.split('_', 1)[1]\n",
    "        grobid_metadata[row_name]['File Key'] = row_name\n",
    "        grobid_metadata[row_name]['Last Modified'] = obj['LastModified']\n",
    "        grobid_metadata[row_name]['ETag'] = obj['ETag']\n",
    "        grobid_metadata[row_name]['Size'] = obj['Size']\n",
    "        grobid_metadata[row_name]['S3 Link'] = f\"https://{bucket_name}.s3.amazonaws.com/{file_key}\"\n",
    "        grobid_metadata[row_name]['File Type'] = file_key.split('.')[-1].lower()\n",
    "    elif 'PyPDF' in file_key:\n",
    "        row_name = file_key.split('_', 1)[1]\n",
    "        pypdf_metadata[row_name]['File Key'] = row_name\n",
    "        pypdf_metadata[row_name]['Last Modified'] = obj['LastModified']\n",
    "        pypdf_metadata[row_name]['ETag'] = obj['ETag']\n",
    "        pypdf_metadata[row_name]['Size'] = obj['Size']\n",
    "        pypdf_metadata[row_name]['S3 Link'] = f\"https://{bucket_name}.s3.amazonaws.com/{file_key}\"\n",
    "        pypdf_metadata[row_name]['File Type'] = file_key.split('.')[-1].lower()\n",
    "\n",
    "# Combine PyPDF and Grobid metadata into rows\n",
    "rows = []\n",
    "for row_name, grobid_info in grobid_metadata.items():\n",
    "    row = {}\n",
    "    row.update(grobid_info)\n",
    "    row.update(pypdf_metadata.get(row_name, {}))\n",
    "    rows.append(row)\n",
    "\n",
    "# Save metadata to CSV file\n",
    "with open(output_csv_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    fieldnames = ['File Key', 'Last Modified (Grobid)', 'ETag (Grobid)', 'Size (Grobid)', 'S3 Link (Grobid)', 'File Type (Grobid)',\n",
    "                  'Last Modified (PyPDF)', 'ETag (PyPDF)', 'Size (PyPDF)', 'S3 Link (PyPDF)', 'File Type (PyPDF)']\n",
    "    csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "    # Write header row\n",
    "    csv_writer.writeheader()\n",
    "\n",
    "    # Write metadata for each row\n",
    "    for row in rows:\n",
    "        csv_writer.writerow({\n",
    "            'File Key': row.get('File Key', ''),\n",
    "            'Last Modified (Grobid)': row.get('Last Modified', ''),\n",
    "            'ETag (Grobid)': row.get('ETag', ''),\n",
    "            'Size (Grobid)': row.get('Size', ''),\n",
    "            'S3 Link (Grobid)': row.get('S3 Link', ''),\n",
    "            'File Type (Grobid)': row.get('File Type', ''),\n",
    "            'Last Modified (PyPDF)': row.get('Last Modified', ''),\n",
    "            'ETag (PyPDF)': row.get('ETag', ''),\n",
    "            'Size (PyPDF)': row.get('Size', ''),\n",
    "            'S3 Link (PyPDF)': row.get('S3 Link', ''),\n",
    "            'File Type (PyPDF)': row.get('File Type', ''),\n",
    "        })\n",
    "\n",
    "print(f\"Metadata saved to {output_csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa5eac5-6191-449f-bc24-0c44d65970a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
