{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "842cf2ff-3400-4e3e-9588-0df620edb284",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:22:51.761827Z",
     "iopub.status.busy": "2024-02-06T21:22:51.761451Z",
     "iopub.status.idle": "2024-02-06T21:22:51.764640Z",
     "shell.execute_reply": "2024-02-06T21:22:51.764159Z",
     "shell.execute_reply.started": "2024-02-06T21:22:51.761776Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests_html in /Users/torch-it/miniconda3/envs/torch/lib/python3.9/site-packages (0.10.0)\n",
      "Requirement already satisfied: pyppeteer>=0.0.14 in /Users/torch-it/miniconda3/envs/torch/lib/python3.9/site-packages (from requests_html) (1.0.2)\n",
      "Requirement already satisfied: w3lib in /Users/torch-it/miniconda3/envs/torch/lib/python3.9/site-packages (from requests_html) (2.1.1)\n",
      "Requirement already satisfied: bs4 in /Users/torch-it/miniconda3/envs/torch/lib/python3.9/site-packages (from requests_html) (0.0.2)\n",
      "Requirement already satisfied: pyquery in /Users/torch-it/miniconda3/envs/torch/lib/python3.9/site-packages (from requests_html) (2.0.0)\n",
      "Requirement already satisfied: fake-useragent in /Users/torch-it/miniconda3/envs/torch/lib/python3.9/site-packages (from requests_html) (1.4.0)\n",
      "Requirement already satisfied: parse in /Users/torch-it/miniconda3/envs/torch/lib/python3.9/site-packages (from requests_html) (1.20.1)\n",
      "Requirement already satisfied: requests in /Users/torch-it/miniconda3/envs/torch/lib/python3.9/site-packages (from requests_html) (2.28.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /Users/torch-it/miniconda3/envs/torch/lib/python3.9/site-packages (from pyppeteer>=0.0.14->requests_html) (4.64.1)\n",
      "Requirement already satisfied: pyee<9.0.0,>=8.1.0 in /Users/torch-it/miniconda3/envs/torch/lib/python3.9/site-packages (from pyppeteer>=0.0.14->requests_html) (8.1.0)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /Users/torch-it/miniconda3/envs/torch/lib/python3.9/site-packages (from pyppeteer>=0.0.14->requests_html) (1.4.4)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /Users/torch-it/miniconda3/envs/torch/lib/python3.9/site-packages (from pyppeteer>=0.0.14->requests_html) (4.11.3)\n",
      "Requirement already satisfied: certifi>=2021 in /Users/torch-it/miniconda3/envs/torch/lib/python3.9/site-packages (from pyppeteer>=0.0.14->requests_html) (2024.2.2)\n",
      "Requirement already satisfied: websockets<11.0,>=10.0 in /Users/torch-it/miniconda3/envs/torch/lib/python3.9/site-packages (from pyppeteer>=0.0.14->requests_html) (10.4)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /Users/torch-it/miniconda3/envs/torch/lib/python3.9/site-packages (from pyppeteer>=0.0.14->requests_html) (1.26.14)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/torch-it/miniconda3/envs/torch/lib/python3.9/site-packages (from bs4->requests_html) (4.11.1)\n",
      "Requirement already satisfied: importlib-resources>=5.0 in /Users/torch-it/miniconda3/envs/torch/lib/python3.9/site-packages (from fake-useragent->requests_html) (6.1.1)\n",
      "Requirement already satisfied: cssselect>=1.2.0 in /Users/torch-it/miniconda3/envs/torch/lib/python3.9/site-packages (from pyquery->requests_html) (1.2.0)\n",
      "Requirement already satisfied: lxml>=2.1 in /Users/torch-it/miniconda3/envs/torch/lib/python3.9/site-packages (from pyquery->requests_html) (4.9.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/torch-it/miniconda3/envs/torch/lib/python3.9/site-packages (from requests->requests_html) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/torch-it/miniconda3/envs/torch/lib/python3.9/site-packages (from requests->requests_html) (3.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/torch-it/miniconda3/envs/torch/lib/python3.9/site-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests_html) (3.11.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/torch-it/miniconda3/envs/torch/lib/python3.9/site-packages (from beautifulsoup4->bs4->requests_html) (2.3.2.post1)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d29c0db-cf44-410d-929c-7aba7e9f14a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T21:22:52.124583Z",
     "iopub.status.busy": "2024-02-06T21:22:52.124094Z",
     "iopub.status.idle": "2024-02-06T21:22:53.487961Z",
     "shell.execute_reply": "2024-02-06T21:22:53.487459Z",
     "shell.execute_reply.started": "2024-02-06T21:22:52.124556Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in /Users/torch-it/miniconda3/envs/torch/lib/python3.9/site-packages (1.5.6)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nest_asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed0a2c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://www.cfainstitute.org/membership/professional-development/refresher-readings/time-series-analysis',\n",
    " 'https://www.cfainstitute.org/membership/professional-development/refresher-readings/credit-analysis-models',\n",
    " 'https://www.cfainstitute.org/membership/professional-development/refresher-readings/introduction-alternative-investments',\n",
    " 'https://www.cfainstitute.org/membership/professional-development/refresher-readings/credit-default-swaps',\n",
    " 'https://www.cfainstitute.org/membership/professional-development/refresher-readings/valuation-contingent-claims',\n",
    " 'https://www.cfainstitute.org/membership/professional-development/refresher-readings/introduction-commodities-commodity-derivatives',\n",
    " 'https://www.cfainstitute.org/membership/professional-development/refresher-readings/understanding-income-statements',\n",
    " 'https://www.cfainstitute.org/membership/professional-development/refresher-readings/pricing-and-valuation-of-forward-commitments',\n",
    " 'https://www.cfainstitute.org/membership/professional-development/refresher-readings/private-equity-investments',\n",
    " 'https://www.cfainstitute.org/membership/professional-development/refresher-readings/valuation-analysis-bonds-embedded-options']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79d175ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# URL of the webpage you want to scrape\n",
    "def getContent(url):\n",
    "    # Send a GET request to the webpage\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Function to clean up text by removing extra spaces, tabs, and newlines\n",
    "    def clean_text(text):\n",
    "        # Remove extra spaces, tabs, and newlines using regular expressions\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', text)\n",
    "        return cleaned_text.strip()\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the content of the page with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Function to find and print section based on its title\n",
    "        def print_section(title):\n",
    "            # Find the section by its title\n",
    "            section = soup.find('h2', text=title)\n",
    "            if section:\n",
    "                content = []\n",
    "                next_node = section.find_next_sibling()\n",
    "                while next_node and next_node.name != 'h2':\n",
    "                    text = next_node.get_text(\" \", strip=True)\n",
    "                    cleaned_text = clean_text(text)\n",
    "                    if cleaned_text:  # Only add non-empty strings\n",
    "                        content.append(cleaned_text)\n",
    "                    next_node = next_node.find_next_sibling()\n",
    "                return \"\\n\".join(content)\n",
    "            else:\n",
    "                return f\"{title} section not found.\"\n",
    "\n",
    "        # Titles of the sections to extract\n",
    "        text_final = []\n",
    "        titles = ['Introduction', 'Learning Outcomes', 'Summary']\n",
    "\n",
    "        # Loop through the titles and print each section\n",
    "        for title in titles:\n",
    "            text_final.append(print_section(title))\n",
    "        return text_final[0], text_final[1],text_final[2]\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4df08a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_information(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the content of the page with BeautifulSoup\\\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    else:\n",
    "        print(\"error getting webpage\")\n",
    "    # Initialize variables to store extracted information\n",
    "    pdf_link = None\n",
    "    topic_name = None\n",
    "    year = None\n",
    "    level = None\n",
    "    categories = []\n",
    "    text_final = None\n",
    "    parent_topic = None\n",
    "\n",
    "    # Extract the PDF download link\n",
    "    ## PDF link extraction\n",
    "    for a_tag in soup.find_all('a'):\n",
    "        if 'Download the full reading (PDF)' in a_tag.text:\n",
    "            # pdf_link = \"https://cfainstitute.org/\" + a_tag.get('href')\n",
    "            pdf_link = \"https://www.cfainstitute.org\" + a_tag.get('href')\n",
    "            break\n",
    "\n",
    "    # Extract the topic name\n",
    "    title_tag = soup.find('title')\n",
    "    if title_tag:\n",
    "        topic_name = title_tag.text\n",
    "    def clean_text(text):\n",
    "        # Remove extra spaces, tabs, and newlines using regular expressions\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', text)\n",
    "        return cleaned_text.strip()\n",
    "\n",
    "    # Extract the year and level from the content utility section\n",
    "    content_utility = soup.find('div', class_='content-utility')\n",
    "    if content_utility:\n",
    "        text = content_utility.get_text()\n",
    "        year, level, parent_topic = extract_info(clean_text(text))\n",
    "#     print(\"------>\", topic_name )\n",
    "    \n",
    "    card_title = soup.find('p', class_='card-title', text='Categories')\n",
    "    if card_title:\n",
    "        # Find all <p> tags following the 'Categories' card title\n",
    "        for sibling in card_title.find_next_siblings('p'):\n",
    "            a_tag = sibling.find('a')\n",
    "            if a_tag and a_tag.text.strip():\n",
    "                categories.append(a_tag.text.strip())\n",
    "                \n",
    "    Intro, LearningOutcome, Summary  = getContent(url)\n",
    "    \n",
    "    return pdf_link, topic_name, year, level, Intro, LearningOutcome, Summary , categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0f21b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_info(topic):\n",
    "    parent_topics = [\n",
    "        \"Portfolio Management and Wealth Planning\",\n",
    "        \"Fixed Income\",\n",
    "        \"Corporate Finance\",\n",
    "        \"Equity Investments\",\n",
    "        \"Financial Reporting and Analysis\",\n",
    "        \"Quantitative Methods\",\n",
    "        \"Derivatives\",\n",
    "        \"Alternative Investments\",\n",
    "        \"Economics\",\n",
    "        \"Ethical and Professional Standards\"\n",
    "    ]\n",
    "    # Regex pattern for year, level, and parent topic\n",
    "    pattern = r'(?P<year>20[0-2]\\d) Curriculum CFA Program (?P<level>Level [I]{1,3}) (?P<topic>.+)'\n",
    "    match = re.match(pattern, topic)\n",
    "\n",
    "    if match:\n",
    "        year = match.group('year')\n",
    "        level = match.group('level')\n",
    "        topic = match.group('topic')\n",
    "\n",
    "        # Check if the extracted topic is in the predefined parent topics list\n",
    "        for parent_topic in parent_topics:\n",
    "            if parent_topic in topic:\n",
    "                return year, level, parent_topic\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def write_to_csv(file_path, header, content):\n",
    "    # Check if the CSV file already exists\n",
    "    file_exists = os.path.isfile(file_path)\n",
    "    \n",
    "    # Open the file in append mode if it exists, or write mode if it doesn't\n",
    "    with open(file_path, mode='a' if file_exists else 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # If the file didn't exist, write the header first\n",
    "        if not file_exists:\n",
    "            writer.writerow(header)\n",
    "        \n",
    "        # Write the content to the CSV file\n",
    "        writer.writerow(content)\n",
    "\n",
    "# Example usage\n",
    "# file_path = 'example.csv'\n",
    "# header = ['Column1', 'Column2', 'Column3']  # Define your header columns\n",
    "# content = ['Data1', 'Data2', 'Data3']  # This should be a list representing a row of data\n",
    "\n",
    "# write_to_csv(file_path, header, content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea7377e",
   "metadata": {},
   "source": [
    "## Main function to call "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9a01179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topic_name(link):\n",
    "    # Use regex to extract the topic name from the URL\n",
    "    match = re.search(r'([^/]+)$', link)\n",
    "    if match:\n",
    "        topic_name = match.group(1)\n",
    "        # Replace hyphens with spaces and convert to title case\n",
    "        topic_name = topic_name.replace('-', ' ').title()\n",
    "        return topic_name\n",
    "    else:\n",
    "        return \"Topic name not found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6311cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in urls:\n",
    "    write_to_csv('FinanceHub.csv', ['pdf_link', 'Parent_topic', 'year', 'level', 'Introduction', 'LearningOutcome', 'Summary' , 'categories', 'url', \"topicName\"],[*extract_information(url),url,extract_topic_name(url)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24b23296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Series Analysis\n",
      "Credit Analysis Models\n",
      "Introduction Alternative Investments\n",
      "Credit Default Swaps\n",
      "Valuation Contingent Claims\n",
      "Introduction Commodities Commodity Derivatives\n",
      "Understanding Income Statements\n",
      "Pricing And Valuation Of Forward Commitments\n",
      "Private Equity Investments\n",
      "Valuation Analysis Bonds Embedded Options\n"
     ]
    }
   ],
   "source": [
    "for i in urls:\n",
    "    print(extract_topic_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f33246f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
