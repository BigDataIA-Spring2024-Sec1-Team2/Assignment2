{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "026070aa-9b56-41af-b035-fb501800577f",
   "metadata": {},
   "source": [
    "# DAMG 7245 : Assignment 2.1 \n",
    "## Web Scraping and Dataset Creation\n",
    "\n",
    "### Scraping 224 URL's from Finance Institute's Webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db41171-d0f3-47ac-81ec-75269648e847",
   "metadata": {},
   "source": [
    "### Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "718265ba-d258-4bdb-921b-d0af7c4b298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d0663b",
   "metadata": {},
   "source": [
    "#### Loading environment Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4105e256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv('../config/.env',override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bebef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadenv():\n",
    "    csv_filename = os.getenv(\"CSV_CFA_WEB\")\n",
    "    folderpath = os.getenv(\"DIR_CFA_WEB\")\n",
    "    txt_filename = os.getenv(\"TXT_CFA_LINKS\")\n",
    "    \n",
    "    return csv_filename, folderpath, txt_filename\n",
    "\n",
    "csv_filename, folderpath, txt_filename =loadenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d087ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('FinanceHub.csv', '../sample_output/', '224_links.txt')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_filename, folderpath, txt_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d554de1-8cff-4898-bced-874c09db9ca4",
   "metadata": {},
   "source": [
    "### Scrape function to extract links which comes as part of `CoveoResultLink` class under anchor tag in the given webpage and save those to a .txt file\n",
    "\n",
    "- Strategy used involves edge browser webdriver to make the beutiful soup scraper wait for a certain amount of time before scraping as the links which needs to be extracted loads dynamically over the page after a particular Javascript snippet gets successfully executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "249b86ae-acb9-48de-a0b1-9114eed113aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_coveo_links(url):\n",
    "    # Set up the WebDriver with the command line flag for Edge\n",
    "    edge_options = webdriver.EdgeOptions()\n",
    "    edge_options.add_argument('--enable-chrome-browser-cloud-management')\n",
    "\n",
    "    driver = webdriver.Edge(options=edge_options)\n",
    "\n",
    "    try:\n",
    "        # Make a request using Selenium\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the dynamic content to load (you may need to adjust the sleep duration)\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Get the page source after dynamic content has loaded\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Find all the links with class 'coveo'\n",
    "        coveo_links = soup.find_all('a', class_='CoveoResultLink')\n",
    "\n",
    "        # Extract and write the href attribute of each coveo link to a file\n",
    "        with open(folderpath+txt_filename, \"a\") as file:\n",
    "            for link in coveo_links:\n",
    "                href = link.get('href')\n",
    "                if href:\n",
    "                    file.write(href + '\\n')\n",
    "\n",
    "        # Print the total number of coveo links\n",
    "        print(f\"Total number of Coveo class links: {len(coveo_links)}\")\n",
    "        print(\"Coveo class links saved to '224_links.txt'\")\n",
    "\n",
    "    finally:\n",
    "        # Close the WebDriver in a 'finally' block to ensure it is closed even if an exception occurs\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ef8bea-e6f8-43cd-a3e6-b5ecbb475148",
   "metadata": {},
   "source": [
    "### Looping to scrape links page by page taking `pagination` into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22b98bfb-1f98-45aa-b941-7f691e79bbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file '../sample_output/224_links.txt' has been emptied.\n",
      "Total number of Coveo class links: 24\n",
      "Coveo class links saved to '224_links.txt'\n",
      "Total number of Coveo class links: 100\n",
      "Coveo class links saved to '224_links.txt'\n",
      "Total number of Coveo class links: 100\n",
      "Coveo class links saved to '224_links.txt'\n"
     ]
    }
   ],
   "source": [
    "count = 2\n",
    "\n",
    "file_path = folderpath+txt_filename  # Replace with the actual file path\n",
    "## Create/overwrite the file to empty it\n",
    "try:\n",
    "    # Open the file in write mode ('w') or truncate mode ('w+')\n",
    "    with open(file_path, 'w+', encoding='utf-8'):\n",
    "        pass  # The 'pass' statement does nothing, effectively emptying the file\n",
    "\n",
    "    print(f\"The file '{file_path}' has been emptied.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' does not exist.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "while(count>=0):\n",
    "    url = f\"https://www.cfainstitute.org/en/membership/professional-development/refresher-readings#first={count*100}&sort=%40refreadingcurriculumyear%20descending&numberOfResults=100\"\n",
    "    scrape_coveo_links(url)\n",
    "    count = count - 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37c891a",
   "metadata": {},
   "source": [
    "### reading all of url saved in `../sample_output/224_links.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94f19310-9f7f-42ff-b1d3-9082ace1ec66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.cfainstitute.org/membership/professional-development/refresher-readings/Organizational-Forms-Corporate-Issuer-Features-and-Ownership',\n",
       " 'https://www.cfainstitute.org/membership/professional-development/refresher-readings/Hedge-funds-L1',\n",
       " 'https://www.cfainstitute.org/membership/professional-development/refresher-readings/Introduction-to-Digital-Assets']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Open the file and read its contents\n",
    "with open(file_path, 'r') as file:\n",
    "    urls = [url.strip() for url in file.readlines()]\n",
    "    \n",
    "urls[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc2b829",
   "metadata": {},
   "source": [
    "### Defining utility function like \n",
    "\n",
    "* clearing extra spaces, tabs, and newlines\n",
    "* Identify heading tag \n",
    "* extract content from from `Introduction`, `Learning Outcomes`, `Summary` heading tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d35d1529",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to clean up text by removing extra spaces, tabs, and newlines\n",
    "def clean_text(text):\n",
    "    # Remove extra spaces, tabs, and newlines using regular expressions\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', text)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "# Function to find and print section based on its title\n",
    "def print_section(soup,title):\n",
    "    # Find the section by its title\n",
    "    section = soup.find('h2', text=title)\n",
    "    try:\n",
    "        if section:\n",
    "            content = []\n",
    "            next_node = section.find_next_sibling()\n",
    "            while next_node and next_node.name != 'h2':\n",
    "                text = next_node.get_text(\" \", strip=True)\n",
    "                cleaned_text = clean_text(text)\n",
    "                if cleaned_text:  # Only add non-empty strings\n",
    "                    content.append(cleaned_text)\n",
    "                next_node = next_node.find_next_sibling()\n",
    "            return \"\\n\".join(content)\n",
    "        else:\n",
    "            return f\"{title} section is missing.\"\n",
    "    except:\n",
    "        return f\"{title} section is missing.\"\n",
    "\n",
    "    \n",
    "# URL of the webpage to scrape\n",
    "def getContent(soup):\n",
    "    # Parse the content of the page with BeautifulSoup\\\n",
    "    # Titles of the sections to extract\n",
    "    try:\n",
    "        text_final = []\n",
    "        titles = ['Introduction', 'Learning Outcomes', 'Summary']\n",
    "        # Loop through the titles and print each section\n",
    "        for title in titles:\n",
    "            text_final.append(print_section(soup,title))\n",
    "        return text_final[0], text_final[1],text_final[2]\n",
    "    except:\n",
    "        return \"\",\"\",\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a04e06",
   "metadata": {},
   "source": [
    "### Identifying other relevant information using following methods\n",
    "\n",
    "* **Topic Name** : Retriving the text in `<title>` tag which was earlier extracted from URL\n",
    "* **Year, Level, Parent Topic** : Preprossing the text from content utility class using function `extract_info`\n",
    "* **Categories** : Exploring `<p>` tag with `class='card-title'` and `text='Categories'`\n",
    "* **Pdf_link** : Filtering a get which has text saying `Download the full reading (PDF)` and retriving the href associated with it \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e2c78ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_information(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the content of the page with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Parse the content of the page with BeautifulSoup\n",
    "        # Initialize variables to store extracted information\n",
    "        pdf_link = None\n",
    "        topic_name = None\n",
    "        year = None\n",
    "        level = None\n",
    "        categories = []\n",
    "        text_final = None\n",
    "        parent_topic = None\n",
    "\n",
    "        # Extract the PDF download link\n",
    "        ## PDF link extraction\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            if 'Download the full reading (PDF)' in a_tag.text:\n",
    "                pdf_link = \"https://cfainstitute.org\" + a_tag.get('href')\n",
    "                break\n",
    "\n",
    "        # Extract the topic name\n",
    "        title_tag = soup.find('title')\n",
    "        if title_tag:\n",
    "            topic_name = title_tag.text\n",
    "\n",
    "        # Extract the year and level from the content utility section\n",
    "        content_utility = soup.find('div', class_='content-utility')\n",
    "        if content_utility:\n",
    "            text = content_utility.get_text()\n",
    "            year, level, parent_topic = extract_info_year_level_topic(clean_text(text))    \n",
    "        card_title = soup.find('p', class_='card-title', text='Categories')\n",
    "        if card_title:\n",
    "            # Find all <p> tags following the 'Categories' card title\n",
    "            for sibling in card_title.find_next_siblings('p'):\n",
    "                a_tag = sibling.find('a')\n",
    "                if a_tag and a_tag.text.strip():\n",
    "                    categories.append(a_tag.text.strip())\n",
    "\n",
    "        Intro, LearningOutcome, Summary  = getContent(soup)\n",
    "\n",
    "        return pdf_link, parent_topic, year, level, Intro, LearningOutcome, Summary , categories, topic_name\n",
    "    else:\n",
    "        return \"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\n",
    "\n",
    "\n",
    "def extract_info_year_level_topic(topic):\n",
    "    parent_topics = [\n",
    "        \"Portfolio Management and Wealth Planning\",\n",
    "        \"Fixed Income\",\n",
    "        \"Corporate Finance\",\n",
    "        \"Equity Investments\",\n",
    "        \"Financial Reporting and Analysis\",\n",
    "        \"Quantitative Methods\",\n",
    "        \"Derivatives\",\n",
    "        \"Alternative Investments\",\n",
    "        \"Economics\",\n",
    "        \"Ethical and Professional Standards\"\n",
    "    ]\n",
    "    # Regex pattern for year, level, and parent topic\n",
    "    pattern = r'(?P<year>20[0-2]\\d) Curriculum CFA Program (?P<level>Level [I]{1,3}) (?P<topic>.+)'\n",
    "    match = re.match(pattern, topic)\n",
    "\n",
    "    if match:\n",
    "        year = match.group('year')\n",
    "        level = match.group('level')\n",
    "        topic = match.group('topic')\n",
    "\n",
    "        # Check if the extracted topic is in the predefined parent topics list\n",
    "        for parent_topic in parent_topics:\n",
    "            if parent_topic in topic:\n",
    "                return str(year), str(level), str(parent_topic)\n",
    "    return \"\",\"\",\"\"\n",
    "\n",
    "\n",
    "def write_to_csv(file_path, header, content):\n",
    "    # Check if the CSV file already exists\n",
    "    file_exists = os.path.isfile(file_path)\n",
    "    \n",
    "    # Open the file in append mode if it exists, or write mode if it doesn't\n",
    "    with open(file_path, mode='a' if file_exists else 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # If the file didn't exist, write the header first\n",
    "        if not file_exists:\n",
    "            writer.writerow(header)\n",
    "        \n",
    "        # Write the content to the CSV file\n",
    "        writer.writerow(content)\n",
    "\n",
    "\n",
    "# Define the path for the CSV file\n",
    "\n",
    "def delCSV(csv_file_path):\n",
    "    # Check if the CSV file exists\n",
    "    if os.path.exists(csv_file_path):\n",
    "        # If the file exists, remove it\n",
    "        os.remove(csv_file_path)\n",
    "        message = \"CSV file removed successfully.\"\n",
    "    else:\n",
    "        message = \"CSV file does not exist.\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161f760d",
   "metadata": {},
   "source": [
    "### Calling function to iterate over all the web pages to extract the information\n",
    "\n",
    "* Looking for existing file\n",
    "* deleting an existing file\n",
    "* creating a new CSV \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8815ac8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 224/224 [01:44<00:00,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.9 s, sys: 603 ms, total: 15.5 s\n",
      "Wall time: 1min 44s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# deleting old files \n",
    "delCSV(folderpath+csv_filename)\n",
    "\n",
    "# write to CSV \n",
    "for url in tqdm(urls[:]):\n",
    "    write_to_csv(folderpath+csv_filename, ['pdf_link', 'Parent_topic', 'year', 'level', 'Introduction', 'LearningOutcome', 'Summary' , 'categories', 'topicName','url'],[*extract_information(url),url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92cbdb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.cfainstitute.org/membership/professional-development/refresher-readings/Organizational-Forms-Corporate-Issuer-Features-and-Ownership',\n",
       " 'https://www.cfainstitute.org/membership/professional-development/refresher-readings/Hedge-funds-L1',\n",
       " 'https://www.cfainstitute.org/membership/professional-development/refresher-readings/Introduction-to-Digital-Assets']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55714e11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
