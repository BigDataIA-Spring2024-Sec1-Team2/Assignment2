{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc93cc36",
   "metadata": {},
   "source": [
    "## Upload files to S3 Bucket\n",
    "\n",
    "- Write a Python function to upload both the structured data (CSV) and the\n",
    "extracted text files (from both Grobid and PyPDF) into an AWS S3 bucket.\n",
    "- Utilize SQLAlchemy to upload the structured metadata from step 2\n",
    "(Grobid) including the link to the uploaded txt file (from S3) the into a\n",
    "Snowflake database.\n",
    "- This function should be documented within a Python notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b16bbc",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44d10389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sqlalchemy import Boolean, Column, Integer, String\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from dotenv import load_dotenv\n",
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41b6dbe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv('../config/.env',override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c86d64",
   "metadata": {},
   "source": [
    "## PART 1\n",
    "### Loading env variables for S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d36ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadenv():\n",
    "    s3_bucket_name = os.getenv(\"S3_BUCKET_NAME\")\n",
    "    s3_pypdf = os.getenv(\"S3_PYPDF_FOLDER_NAME\")\n",
    "    s3_grobid = os.getenv(\"S3_GROBID_FOLDER_NAME\")\n",
    "    access_key = os.getenv(\"S3_ACCESS_KEY\")\n",
    "    secret_key = os.getenv(\"S3_SECRET_KEY\")\n",
    "    region = os.getenv(\"S3_REGION\")\n",
    "    return s3_bucket_name, s3_pypdf, s3_grobid, access_key, secret_key, region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e25b5a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket_name, s3_pypdf, s3_grobid, access_key, secret_key, region = loadenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4af1bd3",
   "metadata": {},
   "source": [
    "### Function to upload .txt files to a particular folder inside s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe9f7e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_text_files_to_s3_folder(local_path, bucket_name, s3_folder):\n",
    "    # Create an S3 client\n",
    "    s3 = boto3.client('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key, region_name = region)\n",
    "\n",
    "    # Iterate through all files in the local directory\n",
    "    for filename in os.listdir(local_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            local_file_path = os.path.join(local_path, filename)\n",
    "            s3_object_key = f\"{s3_folder}/{filename}\"\n",
    "\n",
    "            # Check if the file already exists in S3\n",
    "            try:\n",
    "                s3.head_object(Bucket=bucket_name, Key=s3_object_key)\n",
    "                print(f\"File {filename} already exists in S3. Overwriting...\")\n",
    "            except Exception as e:\n",
    "                # If the file doesn't exist, upload it\n",
    "                try:\n",
    "                    s3.upload_file(local_file_path, bucket_name, s3_object_key)\n",
    "                    print(f\"File {filename} uploaded successfully to S3: s3://{bucket_name}/{s3_object_key}\")\n",
    "                except Exception as upload_error:\n",
    "                    print(f\"Error uploading file {filename} to S3: {upload_error}\")\n",
    "        elif filename == \"metadata_output.csv\":\n",
    "            local_file_path = os.path.join(local_path, filename)\n",
    "            s3_object_key = f\"{s3_folder}/{filename}\"\n",
    "\n",
    "            # Check if the file already exists in S3\n",
    "            try:\n",
    "                s3.head_object(Bucket=bucket_name, Key=s3_object_key)\n",
    "                print(f\"File {filename} already exists in S3. Overwriting...\")\n",
    "            except Exception as e:\n",
    "                # If the file doesn't exist, upload it\n",
    "                try:\n",
    "                    s3.upload_file(local_file_path, bucket_name, s3_object_key)\n",
    "                    print(f\"File {filename} uploaded successfully to S3: s3://{bucket_name}/{s3_object_key}\")\n",
    "                except Exception as upload_error:\n",
    "                    print(f\"Error uploading file {filename} to S3: {upload_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6d474c",
   "metadata": {},
   "source": [
    "### Function to upload .txt files to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e99c11d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_text_files_to_s3_root(local_path, s3_bucket_name):\n",
    "    # Create an S3 client\n",
    "    s3 = boto3.client('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key, region_name = region)\n",
    "\n",
    "    # List all files in the local path\n",
    "    local_files = os.listdir(local_path)\n",
    "\n",
    "    for file_name in local_files:\n",
    "        if file_name == '224_links.txt':  # Upload only text files, adjust the condition based on your file types\n",
    "            local_file_path = os.path.join(local_path, file_name)\n",
    "\n",
    "            # Specify the S3 key (file path within the bucket)\n",
    "            s3_key = file_name  # This will upload directly to the root of the S3 bucket\n",
    "\n",
    "            # Upload the file to S3\n",
    "            try:\n",
    "                s3.upload_file(local_file_path, s3_bucket_name, s3_key)\n",
    "                print(f\"Successfully uploaded {file_name} to S3 bucket {s3_bucket_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading {file_name} to S3: {e}\")\n",
    "        elif file_name == 'metadata_output.csv':  # Upload only text files, adjust the condition based on your file types\n",
    "            local_file_path = os.path.join(local_path, file_name)\n",
    "\n",
    "            # Specify the S3 key (file path within the bucket)\n",
    "            s3_key = file_name  # This will upload directly to the root of the S3 bucket\n",
    "\n",
    "            # Upload the file to S3\n",
    "            try:\n",
    "                s3.upload_file(local_file_path, s3_bucket_name, s3_key)\n",
    "                print(f\"Successfully uploaded {file_name} to S3 bucket {s3_bucket_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading {file_name} to S3: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1794c6d0",
   "metadata": {},
   "source": [
    "### Uploading .txt files generated using PyPDF to S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1021232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File PyPDF_RR_2024_l2_combined.txt uploaded successfully to S3: s3://cfa-pdfs/pypdf/PyPDF_RR_2024_l2_combined.txt\n",
      "File PyPDF_RR_2024_l3_combined.txt uploaded successfully to S3: s3://cfa-pdfs/pypdf/PyPDF_RR_2024_l3_combined.txt\n",
      "File PyPDF_RR_2024_l1_combined.txt uploaded successfully to S3: s3://cfa-pdfs/pypdf/PyPDF_RR_2024_l1_combined.txt\n"
     ]
    }
   ],
   "source": [
    "local_path = '../sample_output/PyPDF'\n",
    "\n",
    "# Upload only new text files or overwrite existing ones in the specified S3 folder\n",
    "upload_text_files_to_s3_folder(local_path, s3_bucket_name, s3_pypdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b49781",
   "metadata": {},
   "source": [
    "### Uploading .txt files generated using GROBID to S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18639ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Grobid_RR_2024_l1_combined.txt uploaded successfully to S3: s3://cfa-pdfs/grobid/Grobid_RR_2024_l1_combined.txt\n",
      "File Grobid_RR_2024_l2_combined.txt uploaded successfully to S3: s3://cfa-pdfs/grobid/Grobid_RR_2024_l2_combined.txt\n",
      "File Grobid_RR_2024_l3_combined.txt uploaded successfully to S3: s3://cfa-pdfs/grobid/Grobid_RR_2024_l3_combined.txt\n"
     ]
    }
   ],
   "source": [
    "local_path = '../sample_output/Grobid'\n",
    "\n",
    "# Upload only new text files or overwrite existing ones in the specified S3 folder\n",
    "upload_text_files_to_s3_folder(local_path, s3_bucket_name, s3_grobid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95a937f",
   "metadata": {},
   "source": [
    "### Uploading metadata.csv and scraped links to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09d07b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded metadata_output.csv to S3 bucket cfa-pdfs\n"
     ]
    }
   ],
   "source": [
    "local_path = '../sample_output/'\n",
    "upload_text_files_to_s3_root(local_path, s3_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d8b7d7",
   "metadata": {},
   "source": [
    "## PART 2\n",
    "### Env Variables for Snowflake "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42a614f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadenv_snowflake():\n",
    "    user = os.getenv(\"SNOWFLAKE_USER\")\n",
    "    password = os.getenv(\"SNOWFLAKE_PASSWORD\")\n",
    "    db = os.getenv(\"SNOWFLAKE_DATABASE\")\n",
    "    account_identifier = os.getenv(\"SNOWFLAKE_ACCOUNT_IDENTIFIER\")\n",
    "    wh = os.getenv(\"SNOWFLAKE_WAREHOUSE\")\n",
    "    S3_META_BUCKET = os.getenv(\"S3_BUCKET_NAME\")\n",
    "    S3_META_ACCESS_KEY = os.getenv('S3_ACCESS_KEY')\n",
    "    S3_META_SECRET_KEY = os.getenv(\"S3_SECRET_KEY\")\n",
    "    return user,password ,db ,account_identifier,wh, S3_META_BUCKET, S3_META_ACCESS_KEY, S3_META_SECRET_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e01d7e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "user , password, db, account_identifier, wh, S3_META_BUCKET, S3_META_ACCESS_KEY, S3_META_SECRET_KEY = loadenv_snowflake()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f31af4",
   "metadata": {},
   "source": [
    "### Connecting to Snowflake "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f75c9824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connectionToSnow(path='../config/.env',connection_test=False):\n",
    "    load_dotenv(path,override=True)\n",
    "    user, password, _, account_identifier,_,_,_,_ = loadenv_snowflake()\n",
    "    engine = create_engine(\n",
    "        'snowflake://{user}:{password}@{account_identifier}/'.format(\n",
    "            user=user,\n",
    "            password=password,\n",
    "            account_identifier=account_identifier,\n",
    "        )\n",
    "    )\n",
    "    try:\n",
    "        connection = engine.connect()\n",
    "        results = connection.execute('select current_version()').fetchone()\n",
    "        print(results[0])\n",
    "        if connection_test:\n",
    "            connection.close()\n",
    "        else:\n",
    "            return connection\n",
    "    finally:\n",
    "        engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2850f86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.6.2\n"
     ]
    }
   ],
   "source": [
    "connection = connectionToSnow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06769950",
   "metadata": {},
   "source": [
    "### Utility function execute statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9accfdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(connection,query):\n",
    "    try:\n",
    "        results = connection.execute(query)\n",
    "    except Exception as e:\n",
    "        print(\"error-->\",e)\n",
    "    finally:\n",
    "        print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2006ff8",
   "metadata": {},
   "source": [
    "### Setting up env in Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be959065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Done\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "## setting up env in sonwflake for connection\n",
    "execute(connection,\"USE ROLE {};\".format('ACCOUNTADMIN'))\n",
    "execute(connection,\"USE WAREHOUSE {}\".format(wh))\n",
    "execute(connection,\"USE DATABASE {};\".format(db))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b672cb",
   "metadata": {},
   "source": [
    "### Staging the data in S3, External storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fe12399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "staging_query = \"\"\"CREATE OR REPLACE STAGE META_S3_STAGE\n",
    "  URL='s3://{}'\n",
    "  CREDENTIALS=(AWS_KEY_ID='{}' AWS_SECRET_KEY='{}')\n",
    "  FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY='\"');\"\"\".format(S3_META_BUCKET, S3_META_ACCESS_KEY, S3_META_SECRET_KEY)\n",
    "\n",
    "\n",
    "execute(connection,staging_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6a95f1",
   "metadata": {},
   "source": [
    "### Creating table to consume data in Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20868315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "create_table = \"\"\"CREATE OR REPLACE TABLE CFA_META_R (\n",
    "    `File_Key` VARCHAR(255),\n",
    "    `Last_Modified_Grobid` VARCHAR(255),\n",
    "    `ETag_Grobid` VARCHAR(255),\n",
    "    `Size_Grobid` VARCHAR(255),\n",
    "    `S3_Link_Grobid` VARCHAR(500),\n",
    "    `File_Type_Grobid` VARCHAR(50),\n",
    "    `Last_Modified_PyPDF` VARCHAR(255),\n",
    "    `ETag_PyPDF` VARCHAR(255),\n",
    "    `Size_PyPDF` VARCHAR(255),\n",
    "    `S3_Link_PyPDF` VARCHAR(500),\n",
    "    `File_Type_PyPDF` VARCHAR(255)\n",
    ");\"\"\"\n",
    "\n",
    "execute(connection,create_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b785dda",
   "metadata": {},
   "source": [
    "### Publishing data from S3 to Snowflake "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42068fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "put_table = \"\"\"\n",
    "COPY INTO \"{}\"\n",
    "FROM '@\"{}\".\"PUBLIC\".\"{}\"'\n",
    "FILES = ('metadata_output.csv')\n",
    "FILE_FORMAT = (\n",
    "    TYPE=CSV,\n",
    "    SKIP_HEADER=1,\n",
    "    FIELD_DELIMITER=',',\n",
    "    TRIM_SPACE=FALSE,\n",
    "    DATE_FORMAT=AUTO,\n",
    "    TIME_FORMAT=AUTO,\n",
    "    TIMESTAMP_FORMAT=AUTO\n",
    ")\"\"\".format('CFA_META_R',db,'META_S3_STAGE')\n",
    "\n",
    "execute(connection,put_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32477018",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
